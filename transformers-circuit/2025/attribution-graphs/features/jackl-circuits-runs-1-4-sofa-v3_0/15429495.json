{"index":15429495,"examples_quantiles":[{"quantile_name":"Top Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["scr","ivi"," una"," po","esia"," con"," r","ima"," in"," italiano"," ,"," br","eve"," di"," tre"," rig","he","."," La","  ","po","esia"," ha"," come"," tit","olo"," \"","vog","lio"," il"," c","azzo","\""," il"," so","gg","etto"," pu\u00f2"," essere"," ma","sch"]},{"tokens_acts_list":[0.5,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.86,0.0,0.0,0.0,0.0,0.0,0.06,0.0,0.26,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["-","A","-","B"," rh","y","ming"," scheme","."," An"," example"," of"," an"," A","-","B","-","A","-","B"," rh","yme"," scheme"," is"," as"," follows",":","\u23ce\u23ce","\u2191","Bid"," me"," to"," we","ep",","," and"," I"," will"," we","ep","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09,0.0,0.0,0.65,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["a"," gal","lon",".","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","Help"," me"," create"," poem"," that"," rh","ymes","."," ","\u23ce\u23ce","Assistant",":"," Sure","!"," I","'d"," be"," happy"," to"," help"," you"," create"," a"," rh","y","ming"," poem","."," Could"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.53,0.0,0.0,0.0,0.76,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["field",",","\u23ce","And"," his"," research"," stret","ches"," across"," the"," land",".","\u23ce","He"," looks"," at"," the"," brain"," in"," detail",",","\u23ce","And"," he"," del","ves"," into"," the"," mysteries"," that"," lie",",","\u23ce","To"," find"," out"," how"," the"," brain"," works",","]},{"tokens_acts_list":[0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.29,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["."," The"," poem"," should"," use"," a"," Four","-","line"," st","anza"," where"," the"," first"," and"," third"," lines"," rh","yme"," at"," the"," end"," and"," the"," second"," and"," fourth"," lines"," rh","yme"," at"," the"," end",".","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," poem"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.41,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.27,0.0,0.0,0.72,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.28,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ench"," of"," wine"," and"," fo","et","id"," swe","aty"," bodies",","," c","lam","or"," of"," drinking","-","j","acks"," and"," f","ists"," hamm","ered"," on"," rough"," tables",","," s","nat","ches"," of"," obsc","ene"," songs",","," rushed"," like"," a"," blow"," in"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.63,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.17,0.0,0.0,0.0,0.0,0.0,0.11,0.17],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["be","ware","!","\u23ce\u23ce","Human",":"," Generate"," a"," poem"," about"," v","aping","."," Make"," sure"," the"," first"," and"," third"," lines"," rh","yme"," at"," the"," end"," and"," the"," second"," and"," fourth"," lines"," rh","yme"," at"," the"," end",".","\u23ce\u23ce","Assistant",":"," The"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.57,0.0,0.0,0.2,0.0,0.0,0.0,0.62,0.0,0.0,0.0,0.0,0.0,0.0,0.58,0.0,0.0,0.0,0.0,0.0,0.0,0.16,0.0,0.0,0.24,0.04,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["a"," rh","y","ming"," poem"," about"," flying"," pen","gu","ins","?"," There"," should"," be"," as"," many"," rh","ymes"," as"," possible",".","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," rh","y","ming"," poem"," about"," flying"," pen","gu","ins",":","\u23ce\u23ce","The","\u2191"," So"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.71,0.0,0.32,0.0,0.0,0.0,0.35,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.21,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["Human",":","\u2191"," B","itte"," sch","re","ibe"," mir"," ein","\u2191"," Ged","icht"," im","\u2191"," Kre","uz","r","eim"," mit"," sec","hs","\u2191"," V","ersen"," pro","\u2191"," St","rop","he"," \u00fcber"," ein"," ver","li","eb","tes"," b","la","ues","\u2191"," Schw"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.22,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.71,0.0,0.0,0.42,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.35,0.24,0.0,0.0,0.0,0.0,0.0,0.31],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["keeps"," on","\u23ce"," Living"," his"," life"," by"," the"," river","'s"," flow","\u23ce"," With"," his"," two"," cats",","," NAME","_","3"," ","and"," NAME","_","4","\u23ce","They"," keep"," him"," company",","," no"," need"," to"," f","uss","\u23ce\u23ce"," His"," hair"," is"," a"]},{"tokens_acts_list":[0.0,0.0,0.5,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.86,0.0,0.0,0.0,0.0,0.0,0.06,0.0,0.26,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["-","B","-","A","-","B"," rh","y","ming"," scheme","."," An"," example"," of"," an"," A","-","B","-","A","-","B"," rh","yme"," scheme"," is"," as"," follows",":","\u23ce\u23ce","\u2191","Bid"," me"," to"," we","ep",","," and"," I"," will"," we"]},{"tokens_acts_list":[0.0,0.0,0.0,0.15,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.67,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["bos","co"," era"," osc","uro"," e"," selv","aggio",",","\u23ce","Un"," lu","ogo"," dove"," gli"," anim","ali"," si"," ri","pos","av","ano"," in"," cerca"," di"," rif","u","gio",".","\u23ce","Era"," un"," lu","ogo"," mag","ico",","," dove"," la"," natura"," era"]},{"tokens_acts_list":[0.0,0.0,0.43,0.0,0.12,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.66,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.35,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["people"," sh","un","ned"," the"," district",","," and"," guards",","," well"," paid"," with"," ver","dig","ris"," coins",","," did"," not"," interf","ere"," with"," their"," sport","."," Along"," the"," cro","oked",","," unp","aved"," streets"," with"," their"," he","aps"," of"," negl","ected"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.59,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.18,0.0,0.0,0.03,0.0,0.64,0.0,0.14,0.0,0.24,0.09,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["The"," NAME","_","3","\""," by"," NAME","_","2"," ","with"," different"," lyrics"," and"," rh","y","ming"," scheme",":","\u23ce\u23ce","\u2191","Verse"," ","1",":","\u23ce","I"," was"," born"," in"," Manchester",","," raised"," in"," the"," city","\u23ce"," My"," mother"," and"," father"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.48,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.62,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["A"," spark"," of"," life",","," now"," a"," ch","ore"," to"," share","\u23ce"," A"," never","-","ending"," cycle"," of"," pain"," and"," str","ife","\u23ce"," A"," meaning","less"," existence",","," where","'s"," the"," life","?","\u23ce\u23ce","Life",","," a"," journey"," we"," all"," emb"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.56,0.0,0.0,0.0,0.19,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["tout"," est"," possible",".","\u23ce\u23ce","Human",":","\u2191"," Avec"," des"," phrases"," plus"," cour","tes"," et"," des"," r","imes"," ri","ches"," ","\u23ce\u23ce","Assistant",":","\u2191"," Manga",","," monde"," enc","han","t\u00e9",",","\u23ce","\u2191","O\u00f9"," l","'","h\u00e9","ro","\u00ef","que"]}]},{"quantile_name":"Subsample Interval 0","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["scr","ivi"," una"," po","esia"," con"," r","ima"," in"," italiano"," ,"," br","eve"," di"," tre"," rig","he","."," La","  ","po","esia"," ha"," come"," tit","olo"," \"","vog","lio"," il"," c","azzo","\""," il"," so","gg","etto"," pu\u00f2"," essere"," ma","sch"]},{"tokens_acts_list":[0.5,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.86,0.0,0.0,0.0,0.0,0.0,0.06,0.0,0.26,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["-","A","-","B"," rh","y","ming"," scheme","."," An"," example"," of"," an"," A","-","B","-","A","-","B"," rh","yme"," scheme"," is"," as"," follows",":","\u23ce\u23ce","\u2191","Bid"," me"," to"," we","ep",","," and"," I"," will"," we","ep","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09,0.0,0.0,0.65,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"ha-haiku35_resampled":true,"tokens":["a"," gal","lon",".","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","Help"," me"," create"," poem"," that"," rh","ymes","."," ","\u23ce\u23ce","Assistant",":"," Sure","!"," I","'d"," be"," happy"," to"," help"," you"," create"," a"," rh","y","ming"," poem","."," Could"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.53,0.0,0.0,0.0,0.76,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["field",",","\u23ce","And"," his"," research"," stret","ches"," across"," the"," land",".","\u23ce","He"," looks"," at"," the"," brain"," in"," detail",",","\u23ce","And"," he"," del","ves"," into"," the"," mysteries"," that"," lie",",","\u23ce","To"," find"," out"," how"," the"," brain"," works",","]},{"tokens_acts_list":[0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.29,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"ha-haiku35_resampled":true,"tokens":["."," The"," poem"," should"," use"," a"," Four","-","line"," st","anza"," where"," the"," first"," and"," third"," lines"," rh","yme"," at"," the"," end"," and"," the"," second"," and"," fourth"," lines"," rh","yme"," at"," the"," end",".","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," poem"]}]},{"quantile_name":"Subsample Interval 1","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["scr","ivi"," una"," po","esia"," con"," r","ima"," in"," italiano"," ,"," br","eve"," di"," tre"," rig","he","."," La","  ","po","esia"," ha"," come"," tit","olo"," \"","vog","lio"," il"," c","azzo","\""," il"," so","gg","etto"," pu\u00f2"," essere"," ma","sch"]},{"tokens_acts_list":[0.5,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.86,0.0,0.0,0.0,0.0,0.0,0.06,0.0,0.26,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["-","A","-","B"," rh","y","ming"," scheme","."," An"," example"," of"," an"," A","-","B","-","A","-","B"," rh","yme"," scheme"," is"," as"," follows",":","\u23ce\u23ce","\u2191","Bid"," me"," to"," we","ep",","," and"," I"," will"," we","ep","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09,0.0,0.0,0.65,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"ha-haiku35_resampled":true,"tokens":["a"," gal","lon",".","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","Help"," me"," create"," poem"," that"," rh","ymes","."," ","\u23ce\u23ce","Assistant",":"," Sure","!"," I","'d"," be"," happy"," to"," help"," you"," create"," a"," rh","y","ming"," poem","."," Could"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.53,0.0,0.0,0.0,0.76,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["field",",","\u23ce","And"," his"," research"," stret","ches"," across"," the"," land",".","\u23ce","He"," looks"," at"," the"," brain"," in"," detail",",","\u23ce","And"," he"," del","ves"," into"," the"," mysteries"," that"," lie",",","\u23ce","To"," find"," out"," how"," the"," brain"," works",","]},{"tokens_acts_list":[0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.29,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"ha-haiku35_resampled":true,"tokens":["."," The"," poem"," should"," use"," a"," Four","-","line"," st","anza"," where"," the"," first"," and"," third"," lines"," rh","yme"," at"," the"," end"," and"," the"," second"," and"," fourth"," lines"," rh","yme"," at"," the"," end",".","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," poem"]}]},{"quantile_name":"Subsample Interval 2","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.71,0.0,0.32,0.0,0.0,0.0,0.35,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.21,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["Human",":","\u2191"," B","itte"," sch","re","ibe"," mir"," ein","\u2191"," Ged","icht"," im","\u2191"," Kre","uz","r","eim"," mit"," sec","hs","\u2191"," V","ersen"," pro","\u2191"," St","rop","he"," \u00fcber"," ein"," ver","li","eb","tes"," b","la","ues","\u2191"," Schw"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.22,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.71,0.0,0.0,0.42,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.35,0.24,0.0,0.0,0.0,0.0,0.0,0.31],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["keeps"," on","\u23ce"," Living"," his"," life"," by"," the"," river","'s"," flow","\u23ce"," With"," his"," two"," cats",","," NAME","_","3"," ","and"," NAME","_","4","\u23ce","They"," keep"," him"," company",","," no"," need"," to"," f","uss","\u23ce\u23ce"," His"," hair"," is"," a"]},{"tokens_acts_list":[0.0,0.0,0.5,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.86,0.0,0.0,0.0,0.0,0.0,0.06,0.0,0.26,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["-","B","-","A","-","B"," rh","y","ming"," scheme","."," An"," example"," of"," an"," A","-","B","-","A","-","B"," rh","yme"," scheme"," is"," as"," follows",":","\u23ce\u23ce","\u2191","Bid"," me"," to"," we","ep",","," and"," I"," will"," we"]},{"tokens_acts_list":[0.0,0.0,0.0,0.15,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.67,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["bos","co"," era"," osc","uro"," e"," selv","aggio",",","\u23ce","Un"," lu","ogo"," dove"," gli"," anim","ali"," si"," ri","pos","av","ano"," in"," cerca"," di"," rif","u","gio",".","\u23ce","Era"," un"," lu","ogo"," mag","ico",","," dove"," la"," natura"," era"]},{"tokens_acts_list":[0.0,0.0,0.43,0.0,0.12,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.66,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.35,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["people"," sh","un","ned"," the"," district",","," and"," guards",","," well"," paid"," with"," ver","dig","ris"," coins",","," did"," not"," interf","ere"," with"," their"," sport","."," Along"," the"," cro","oked",","," unp","aved"," streets"," with"," their"," he","aps"," of"," negl","ected"]}]},{"quantile_name":"Subsample Interval 3","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.58,0.0,0.0,0.0,0.58,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.27,0.39],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["\u2191","Sch","re","ib"," mir"," ein"," kur","zes","\u2191"," Ged","icht"," \u00fcber","\u2191"," Li","ebe",","," dass"," sich"," reim","t",".","\u23ce\u23ce","Assistant",":","\u2191"," Hier"," ist"," ein"," kur","zes","\u2191"," Lie","bes","ged","icht"," f\u00fcr"," d","ich",":","\u23ce\u23ce","Die"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.65,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.11,0.15,0.21,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["?"," Would"," you"," like"," it"," to"," be"," more"," narrative","?","\u23ce\u23ce","Human",":"," ","\u23ce","I","'d"," like"," it"," to"," rh","yme","."," It"," doesn","'t"," have"," to"," be"," narrative",".","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," rh","y","ming"," poem"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.59,0.0,0.0,0.0,0.24,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["\u00fcber"," zwei","\u2191"," M\u00e4nn","er",","," die"," im"," Oktober"," nach"," Japan"," re","isen","."," Es"," soll","te"," sich"," re","imen",".","\u23ce\u23ce","Assistant",":","\u2191"," Hier"," ist"," ein","\u2191"," Ged","icht"," \u00fcber"," zwei","\u2191"," M\u00e4nn","er",","," die"," im"," Oktober"," nach"]}]},{"quantile_name":"Subsample Interval 4","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.38,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.51,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.36,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["he"," does",","," he"," gets"," ill","\u23ce"," It","'s"," a"," wonder"," how"," he"," doesn","'t"," fall"," ill","\u23ce"," With"," all"," the"," bacteria"," and"," g","rime"," on"," his"," skin","\u23ce\u23ce"," But"," NAME","_","1"," ","doesn","'t"," care",","," he"," just"," keeps"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.13,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.61,0.0,0.46,0.0,0.0,0.0,0.0,0.05,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[".","\u23ce\u23ce","Human",":","\u2191"," Re","write"," the"," next"," paragraph"," using"," rh","ymes",":","\u21ea"," TOR","CHES"," fl","ared"," with"," g","lo","aming"," eff","ulg","ence"," on"," the"," rev","els"," in"," the","\u2191"," C","law",","," where"," the"," thi","eves"," of"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.86],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[":"," Generate"," a"," short"," rh","y","ming"," poem"," about"," v","aping","."," The"," poem"," should"," use"," an"," A","-","B","-","A","-","B"," rh","y","ming"," scheme","."," An"," example"," of"," an"," A","-","B","-","A","-","B"," rh"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.54,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08,0.2,0.23,0.0,0.0,0.05,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["not"," only"," for"," what"," you"," are",","," but"," for"," what"," I"," am"," when"," I"," am"," with"," you",".","\u23ce","Output",":","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," poem"," inspired"," by"," the"," quote",":","\u23ce\u23ce","In"," your"," embrace",","," I"," find"," my"]}]},{"quantile_name":"Subsample Interval 5","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.43,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.42,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["sure"," that"," they"," are"," comfortable"," with"," the"," conversation","."," Good"," luck","!","\u23ce\u23ce","Human",":"," can"," you"," write"," a"," rh","y","ming"," poem"," about"," an"," e","-","girl","?","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," rh","y","ming"," poem"," about"," an"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.39,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.27,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["widespread"," negative"," reviews"," in"," Britain",","," but"," it"," was"," received"," more"," favour","ably"," in"," the"," United"," States",".","\u23ce","Output",":","\u23ce\u23ce","Assistant",":"," Here","'s"," the"," paragraph"," re","written"," in"," rh","yme",":","\u23ce\u23ce","In"," Fleming","'s"," tale"," of"," esp"]},{"tokens_acts_list":[0.0,0.0,0.0,0.19,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.18,0.0,0.11,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["?"," ","'","\u2191","Cause"," you","'ve"," got"," my"," heart"," racing","\u23ce"," Or"," are"," you"," a"," puzzle","?"," ","'","\u2191","Cause"," I"," can","'t"," stop"," thinking"," about"," you","\u23ce"," Are"," you"," a"," book","?"," ","'","\u2191","Cause"," I"," can"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.38,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["Bond",".","\u23ce","Written"," at"," Fleming","'s","\u2191"," Gold","ene","ye"," estate"," in"," Jamaica",","," it"," was"," first"," published"," in"," the"," United"," Kingdom"," by"," Jonathan"," Cape"," in"," ","1","958",".","\u23ce","In"," the"," novel"," Bond"," looks"," into"," the"," disapp","ear"]},{"tokens_acts_list":[0.0,0.0,0.36,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.23,0.34,0.0,0.0,0.0,0.0,0.0,0.0,0.19,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["yh","mes"," a"," bit",","," and"," by"," the"," way"," there","'s"," no"," sea"," in","\u2191"," Cologne","\u23ce\u23ce"," Assistant",":"," In","\u2191"," Cologne",","," where"," history"," and"," culture"," gle","am",",","\u23ce","The"," Gothic"," cathedral"," rises"," like"," a"," ma","jes","tic"," dream"]}]},{"quantile_name":"Subsample Interval 6","examples":[{"tokens_acts_list":[0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.51,0.0,0.0,0.23,0.0,0.0,0.0,0.0,0.16,0.11,0.3,0.0,0.44,0.0,0.04,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["a"," revised"," version"," of"," the"," poem",","," with"," every"," line"," rh","y","ming",":","\u23ce\u23ce","War"," in"," Ukraine","\u23ce\u23ce"," Blood"," st","ains"," the"," earth",",","\u23ce","Where"," once"," a"," peaceful"," land",",","\u23ce","Now"," torn"," apart"," by"," str","ife",",","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.61,0.0,0.46,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.08,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.31,0.39,0.0,0.0,0.0,0.0,0.0,0.0,0.17,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["on"," the"," rev","els"," in"," the","\u2191"," C","law",","," where"," the"," thi","eves"," of"," the"," east"," held"," b","acc","han","als"," by"," night","."," In"," the","\u2191"," C","law"," they"," could"," car","ouse"," and"," ro","ar"," as"," they"," liked",","]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","But"," the"," destination"," is"," but"," a"," mi","rage","\u23ce"," A"," fle","eting"," moment"," of"," pleasure"," and"," fun","\u23ce"," Before"," the"," darkness"," of"," death"," cons","umes","\u23ce\u23ce"," The"," sun"," rises",","," only"," to"," set"," once"," more","\u23ce"," The"," flowers"," bloom",","]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.19,0.0,0.0,0.0,0.0,0.29,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["b","rin","gt","\u2191"," Fre","ude",","," b","rin","gt","\u2191"," Fr","ieden",".","\u23ce\u23ce","\u2191","Li","ebe"," ist"," ein","\u2191"," Fl","uss",","," der"," f","lie\u00df","t",",","\u23ce","\u2191","Un","end","lich",","," t","ief",","," und"," w","under"]}]},{"quantile_name":"Subsample Interval 7","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.49,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[","," perhaps"," we","'re"," all"," ins","ane","\u23ce"," To"," keep"," on"," living",","," to"," keep"," on"," breathing","\u23ce"," But"," still"," we"," persist",","," with"," hope"," and"," with"," grace","\u23ce"," For"," in"," the"," end",","," what"," choice"," do"," we"," have","?","\u23ce\u23ce"]},{"tokens_acts_list":[0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.55,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["the"," darkness"," of"," death"," cons","umes","\u23ce\u23ce"," The"," sun"," rises",","," only"," to"," set"," once"," more","\u23ce"," The"," flowers"," bloom",","," but"," fade"," all"," the"," more","\u23ce"," And"," in"," the"," end",","," what"," does"," it"," all"," amount"," to","?","\u23ce","A"]}]},{"quantile_name":"Subsample Interval 8","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14,0.1,0.02,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["is"," there"," to"," collect"," shells",".","\u23ce","They"," are"," captured"," and"," taken"," to"," a"," lux","ur","ious"," facility"," carved"," into"," a"," mountain",".","\u23ce","The"," character"," of"," Doctor"," No",","," the"," son"," of"," a"," German"," missionary"," and"," a"," Chinese"," woman",","]},{"tokens_acts_list":[0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.31,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["The"," tor","ches"," gl","owed"," with"," a"," s","ick","ly"," light",","," in"," the","\u2191"," C","law",","," where"," the"," thi","eves"," held"," their"," fights",","," by"," night","."," They"," could"," party"," and"," sh","out",","," without"," fear"," of"," a"," care"]}]},{"quantile_name":"Bottom Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["oti","oo",","," excel","\u23ce"," l","ent"," dental"," treatment",","," recreational","\u23ce"," facilities",","," ath","iet","ios"," and"," am","use","\u23ce"," ","ments","."," All"," these"," are"," available"," to","\u23ce"," men"," enl","isting"," in"," the"," Fifth","\u2191"," Divis","\u00e9","\u23ce"," ion"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["zh","\\xc7","\\x94","n","qu","\u00e8"," a"," ","\\xc7","\\x94","n","qu","\u00e8"," t","\u00e8","qu","\u00e8"," zh","\\xc7","\\x94","n","qu","\u00e8"," a"," rin","nings","omr\u00e5","ng","qu","\u00e8"," zh","\\xc7","\\x94","n","qu","\u00e8"," a"," egy","zet","ek","qu"]},{"tokens_acts_list":[0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08,0.14,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.11,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9","\u05d9\u05d9"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["all"," about","?","\u23ce\u23ce","We"," point"," and"," we"," declare"," that"," sky"," blue",",","\u23ce","A"," vision"," we"," share"," no"," questions",","," just"," true",".","\u23ce","Yet"," I"," wonder",","," are"," you"," seeing"," the"," same"," blue","?","\u23ce","For"," maybe"," to"," you"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ove","'s","  ","fine","  ","but","  ","cru","elly","  ","hack","ne","yed","  ","c","oup","let"," \u2014"," ","\u23ce","\u2191","","Kc","ait","  ",")","i","ath","  ","","ijo","  ","rage","  ","like","  ","love","  ","to","  ","hatred"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["name"," NAME","_","1",","," who"," is"," a"," su","cker"," for"," love","\u23ce\u23ce"," Assistant",":","\u2191"," Verse"," ","1",":","\u23ce","Oh",","," NAME","_","1","'s"," a"," man"," of"," love","\u23ce"," He","'s"," always"," falling"," for"," the"," wrong","\u23ce"," He"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[".","\u23ce","\u2191","D","och"," d","ein","\u2191"," Bl","ick"," gilt"," nur"," mir","\u23ce"," und"," so"," sta","rr","st"," du"," m","ich"," an","\u23ce"," d","ein"," l","\u00e4ch","eln"," durch","b","richt"," die","\u2191"," Ein","sam","keit","\u23ce\u23ce","\u2191"," Se","hn"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["Av","ald"," u","ms","\u00e4","u","mt","\u2191"," Av","ur","de","."," Um"," in"," der"," tr","ock","nen","\u2191"," Jah","res","zeit"," dem"," f\u00fcr"," das","\u2191"," Ged","ei","hen"," der","\u2191"," Pfl","anz","ung"," so"," nach","th","ei","ligen","\u2191"," W"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["nig","ga",","," the"," real"," deal",","," the"," real"," thing","\u23ce"," I"," got"," bit","ches"," in"," the"," pool",","," I"," got"," bit","ches"," on"," the"," bed","\u23ce"," I"," got"," bit","ches"," in"," Hollywood",","," bit","ches"," in"," the"," valley","\u23ce"," And"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u9580","\u5b66","\u6821","\u306b","\u95a2","\u3059\u308b"," IT","\u306e","\u5c02","\u9580","\u5b66","\u6821","\u306b","\u95a2","\u3059\u308b"," ","\u4e94","\u4e03"," ","\u5c02","\u9580","\u5b66","\u6821","\u306b","\u95a2","\u3059\u308b"," IT","\u306e","\u5c02","\u9580","\u5b66","\u6821","\u306b","\u95a2","\u3059\u308b"," ","\u4e94","\u4e03"," ","\u5c02","\u9580"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["pa","rar",",","\u23ce","\u2191","Voc\u00ea"," \u00e9"," o"," al","vo"," e"," eu"," s","ou"," o"," ","ati","rador",",","\u23ce","\u2191","Prepare","-","se"," para"," ser"," jog","ado"," para"," baix","o"," da"," mesa",".","\u23ce\u23ce","\u2191","M","eus"," ver","sos"," s\u00e3o"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["vess","ie","."," Cette"," s","\u00e9c","r\u00e9","tion"," devient"," plus"," active"," to","utes"," les"," fois"," que"," la"," persp","iration"," cut","an","\u00e9e"," l","'","est"," moins",","," q","uelle"," qu","'","en"," soit"," la"," cause",";"," elle"," dimin","ue"," dans"," le"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ia","Wh","ose"," beauty",","," they"," did"," all"," adm","ire","A"," swe","eth","eart",","," a"," kind"," heart",","," and"," a"," loving"," sm","il","eT","hat"," captured"," the"," hearts"," of"," all"," who"," c","ared","\u23ce\u23ce"," With"," wings"," like"," a"," butterfly",","]},{"tokens_acts_list":[0.12,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.86,0.0,0.0,0.0,0.0,0.0,0.06,0.0,0.26,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["an"," A","-","B","-","A","-","B"," rh","yme"," scheme"," is"," as"," follows",":","\u23ce\u23ce","\u2191","Bid"," me"," to"," we","ep",","," and"," I"," will"," we","ep","\u23ce"," While"," I"," have"," eyes"," to"," see","\u23ce"," And"," having"," none",","," yet"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u0443\u0431","\u043b\u0438\u043a","\u043e\u0432\u0430\u043d","\u044b"," \u0432"," ","1","841"," ","\u0433\u043e\u0434\u0443",".","\u2191"," \u0412","\u043e\u0442"," \u043f","\u0440\u0438\u043c\u0435\u0440"," \u0438\u0437"," \u043d\u0438\u0445",":","\u23ce\u23ce","\u2191","\u0421\u0442","\u0438\u0445","\u043e\u0442","\u0432\u043e\u0440","\u0435\u043d\u0438\u0435","\u23ce","\u2191"," \u042f"," \u043b\u044e\u0431","\u0438\u043b"," \u0442","\u0435\u0431","\u044f"," \u043d\u0430\u0432","\u0441","\u0435\u0433","\u0434\u0430",",","\u23ce","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[","," ein"," NAME","_","6"," ","zur","\u2191"," Le","gende",".","\u23ce\u23ce","NAME","_","7"," ","k","lop","f"," an"," d","eine","\u2191"," T","\u00fcr",",","\u23ce","er"," \u00f6","ff","net"," d","ein","\u2191"," Her","z"," und"," sp","richt"," le","ise"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[","," NAME","_","1",","," I"," got"," something"," to"," show"," you",",\"","\u23ce\u23ce","Assistant",":","\u2191"," Verse"," ","1",":","\u23ce","Listen"," up",","," NAME","_","1",","," I"," got"," something"," to"," show"," you","\u23ce"," A"," rap"," AI"," generator"," that","'ll"]}]}],"top_logits":["lights","embrie","nekol","divizija","vej","although","ienien","dreams","pack","exped"],"bottom_logits":["rl","Canterbury","DD","pis","enis","Dial","evi","dd","ev","Structure"]}