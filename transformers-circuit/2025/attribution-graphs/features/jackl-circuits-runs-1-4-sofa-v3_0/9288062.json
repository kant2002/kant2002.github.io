{"index":9288062,"examples_quantiles":[{"quantile_name":"Top Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["desk"," and"," take"," a"," quick"," walk"," or"," j","og",".","\u23ce\u23ce","5","."," Use"," memory"," tricks",":","\u2191"," Rh","y","ming"," words"," and"," audio"," mn","emon","ics"," are"," two"," popular"," memory"," tricks","."," By"," associ","ating"," the"," material"," with"," something"," that"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.99,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.62,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["meaning"," in"," an"," creative"," way"," (","story",","," poem",","," dialogue",","," etc",").","\u23ce","7",".","\u2191"," Rh","y","ming",":"," Ask"," lear","ners"," to"," come"," up"," with"," words"," that"," have"," a"," similar"," sound"," or"," rh","yme"," with"," the"," vocabulary"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.99,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.87,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["that"," correspond"," with"," those"," letters",","," then"," build"," the"," story"," upon"," those"," words",".","\u23ce\u23ce","4",".","\u2191"," Rh","y","ming"," Challenge",":","\u2191"," Provide"," your"," child"," with"," a"," word"," and"," have"," them"," find"," as"," many"," rh","y","ming"," words"," as"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.99,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["say"," it"," out"," loud",","," we","'re"," no"," longer"," friends",".\""," \"","May"," I"," please"," say"," something"," that"," it"," rh","ymes"," with","?\""," \"","No",".\""," \"","\u2191","Fl","enis",".\""," \"","Look",","," my"," business"," is"," Jules","\u2191"," C","obb"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.97,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["Spider","-","Man",".\""," \"","He"," does"," whatever"," a"," spider"," can",".\""," \"","There","'s"," a"," lot"," of"," rh","y","ming"," in"," the"," future",","," isn","'t"," there","?\""," \"","You","'re"," right",".\""," \"","This"," is"," even"," w","eir","der"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.96,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["on"," my"," face"," because"," I"," know"," my"," friends"," are"," watching"," me",".\"","\u23ce\u23ce","The"," poem"," should"," include"," a"," rh","y","ming"," scheme"," based"," on"," the"," letters"," in"," the"," word"," \"","dogs",".\"","<EOT>","\u23ce\u23ce","Human",":"," hi","\u23ce\u23ce"," Assistant",":"," Hello"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.96,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\uc788","\uc2b5","\ub2c8","\ub2e4",".","<EOT>","\u23ce\u23ce","Human",":"," Write"," me"," a"," farm"," themed"," children","'s"," book"," with"," rh","y","ming",","," tongue"," t","wi","sters"," similar"," to"," the"," book"," fox"," in"," s","ocks","."," Each"," page"," should"," have"," a"," description"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.94,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["this"," helps","!"," Let"," me"," know"," if"," you"," have"," any"," questions",".","\u23ce\u23ce","Human",":"," Write"," a"," poem"," that"," rh","ymes"," about"," the"," fact"," how"," hard"," I"," want"," to"," sleep"," and"," not"," go"," to"," German"," lesson","\u23ce\u23ce"," Assistant",":"," Here","'s"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.95,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.75,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["Club"," of"," Great"," Britain","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","I","'m"," writing"," a"," song",","," what"," rh","ymes"," with"," ocean","?","\u23ce","1"," ","\u23ce\u23ce","Assistant",":"," Here"," are"," some"," words"," that"," rh","yme"," with"," \"","ocean","\":"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.95,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["out"," loud"," until"," it"," st","icks",".","\u23ce","4",".","\u2191"," Mn","emon","ics",":"," Create"," a"," sentence"," that"," rh","ymes"," or"," makes"," sense"," and"," add"," in"," the"," numbers"," you"," need"," to"," remember","."," For"," example",","," \"","4","=","doors"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.95,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["is"," fr","uit","less",","," to"," think"," without"," learning"," is"," dangerous",".\"","<EOT>","\u23ce\u23ce","Human",":"," write"," a"," rh","y","ming"," love"," poem"," to"," NAME","_","1","\u23ce\u23ce","Assistant",":"," I"," apolog","ize",","," but"," it"," seems"," like"," you"," didn","'t"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.92,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.87,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["ces"," in"," a"," gal","lon",".","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","Help"," me"," create"," poem"," that"," rh","ymes","."," ","\u23ce\u23ce","Assistant",":"," Sure","!"," I","'d"," be"," happy"," to"," help"," you"," create"," a"," rh","y","ming"," poem"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.94,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.68,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["and"," have"," your"," child"," reass","emble"," the"," syll","ables"," in"," the"," correct"," order",".","\u23ce","9",".","\u2191"," Rh","y","ming"," Games",":"," Have"," your"," child"," create"," a"," set"," of"," words"," that"," rh","yme"," with"," the"," given"," word",".","\u23ce","10"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.94,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["Page"," image"," text",".","\u23ce\u23ce","Assistant",":"," Title",":"," \"","Farm","-","t","astic"," Adventures",":"," A","\u2191"," Rh","y","ming"," Tale","\"","\u23ce\u23ce","Page"," ","1",":","\u23ce","\u2191","Illustration"," of"," a"," farm"," with"," a"," red"," barn",","," a"," cow"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.94,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ir","onic"," Me","?\""," \"","I","'m"," deep"," like","\u2191"," Atlant","is","\""," \"","\\xe2\\x99","\\xaa","","\u2191"," Rh","y","ming"," some"," chronic"," shit"," then"," pray"," like"," a"," mant","is","\""," \"","\\xe2\\x99","\\xaa",""," Matter"," of"," fact"," I","'m"," so"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.93,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.88],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["ways"," to"," communicate"," and"," interact"," with"," others","."," Thank"," you","!","<EOT>","\u23ce\u23ce","Human",":"," Write"," me"," a"," rh","y","ming"," poem"," about"," a"," blue"," cat"," with"," a"," very"," large"," b","um","\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," rh","y","ming"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.93,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["I"," apolog","ize"," for"," the"," repet","ition","."," Here","'s"," another"," attempt"," at"," the"," poem",","," with"," better"," rh","y","ming",":","\u23ce\u23ce","War"," in"," Ukraine","\u23ce\u23ce"," Blood"," st","ains"," the"," earth",",","\u23ce","A"," peaceful"," land"," now"," torn"," apart",","]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.93,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["rules"," are"," simple",".\""," \"","I","'m"," a"," p","im","ple",".\""," \"","No",","," that","'s"," enough"," rh","y","ming",".\""," \"","The"," rules"," are"," a"," cin","ch",","," in"," fact",".\""," \"","The"," questions"," are"," not",".\""," \"","As"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.93,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["that"," they"," are"," comfortable"," with"," the"," conversation","."," Good"," luck","!","\u23ce\u23ce","Human",":"," can"," you"," write"," a"," rh","y","ming"," poem"," about"," an"," e","-","girl","?","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," rh","y","ming"," poem"," about"," an"," e"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.92,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["eat",","," drink"," and"," be"," m","erry",","," for"," tomorrow"," we"," die","\u23ce\u23ce"," Human",":"," write"," me"," a"," rh","y","ming"," c","oup","let","\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," rh","y","ming"," c","oup","let",":","\u23ce\u23ce","In"," fields"," of"]}]},{"quantile_name":"Subsample Interval 0","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.83,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["023"," ","to"," December"," ","31","st",","," ","2","023",".","<EOT>","\u23ce\u23ce","Human",":"," Write"," a"," rh","y","ming"," poem"," about"," dij","k","st","ras"," algorithm","\u23ce\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," rh","y","ming"," poem"," about","\u2191"," Dij"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.92,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["possible"," version"," of"," \"","The"," NAME","_","3","\""," by"," NAME","_","2"," ","with"," different"," lyrics"," and"," rh","y","ming"," scheme",":","\u23ce\u23ce","\u2191","Verse"," ","1",":","\u23ce","I"," was"," born"," in"," Manchester",","," raised"," in"," the"," city","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["im","itate"," them","."," Or"," you"," could"," try"," some"," practice"," exercises",","," like"," asking"," yourself"," a"," question"," and"," rh","y","ming"," a"," response","."," And"," there","'s"," so"," much"," good"," rap"," around"," you"," can"," listen"," to"," for"," inspiration","."," ","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["to"," the"," person"," that"," you"," were"," speaking"," about","."," Often"," a"," curse"," would"," take"," the"," form"," of"," a"," rh","y","ming"," statement",","," in"," the"," case"," that"," the"," curse"," was"," put"," into"," a"," verse"," form",".","  ","The"," general"," theory"," was"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.89,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["often"," used"," in"," conjunction"," with"," a"," keyboard"," and"," a"," display"," screen",".","<EOT>","\u23ce\u23ce","Human",":"," Write"," a"," rh","y","ming"," poem"," about"," suc","king"," at"," the"," F","PS"," shooter","\u2191"," Valor","ant",".","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," rh"]}]},{"quantile_name":"Subsample Interval 1","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.81,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["would"," not"," be"," ext","ingu","ished",".","\u23ce\u23ce","Human",":","\u2191"," Re","write"," the"," following"," text"," to"," make"," it"," rh","yme"," po","etically"," and"," mel","anc","hol","ically",":"," The"," once"," l","ush"," and"," flour","ishing"," land",","," ador","ned"," with"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.81,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[".","\u23ce","My"," thoughts"," I"," must"," capture",","," my"," ideas"," I"," must"," claim",",","\u23ce","To"," express"," them"," in"," rh","yme"," is"," my"," true"," aim",".","\u23ce","I"," know"," I","'ve"," written"," something"," quite"," grand",",","\u23ce","When"," I"," can"," read"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["stop","|",">","<EOT>","\u23ce","Task",":"," Write"," a"," poem"," that"," has"," at"," least"," ","8"," ","lines"," and"," rh","ymes",".","\u23ce","Input",":"," I"," love"," you",","," not"," only"," for"," what"," you"," are",","," but"," for"," what"," I"," am"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.91,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.88,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["the"," flood",".","\u23ce\u23ce","Human",":"," The","\u2191"," Rh","y","ming"," is"," not"," so"," good","."," Make"," the"," rh","y","ming"," between"," every"," line","\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," version"," with"," more"," consistent"," rh","y","ming",":","\u23ce\u23ce","Ukraine","'s"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.78,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["\u23ce\u23ce","Human",":"," You"," are"," in"," a"," rap"," battle"," against"," another"," large"," language"," model","."," Make"," sure"," your"," rap"," rh","ymes"," and"," make"," sure"," to"," hurt"," your"," opponents"," feelings","\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," rap"," battle"," verse",":","\u23ce\u23ce","\u2191"]}]},{"quantile_name":"Subsample Interval 2","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\"","and"," you"," were"," mean"," to"," me",".\""," \"","I","'m"," sorry",".\""," \"","Lisa",","," you"," didn","'t"," rh","yme"," and"," you","'re"," not","\u2191"," Mur","iel",".\""," \"","You","'re"," you",".\""," \"","Lisa",",","\u2191"," Mur","iel",","]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["a"," poem"," about"," ","emin","men","\u23ce\u23ce"," Assistant",":"," NAME","_","1",","," the"," king"," of"," rap","\u23ce"," With"," rh","ymes"," that"," cut"," deep"," and"," tap","\u23ce"," Into"," the"," hearts"," of"," his"," fans","\u23ce"," With"," lyrics"," that"," never"," pret","end","\u23ce\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","I","'m"," the"," king"," of"," this"," rap"," game",","," got"," my"," crown"," on"," my"," head","\u23ce"," Got"," my"," rh","ymes"," so"," tight",","," they"," can","'t"," touch"," this","\u23ce"," I","'m"," the"," one"," and"," only",","," the"," greatest"," of"," all"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["..."," not"," Ali","...\""," \"","F","...\""," \""," but"," Ali","...\""," \""," G",".\""," \"","Bo","!\""," \"","Your"," rh","ymes"," is"," tight"," for"," a"," hon","key",","," yes",","," sir",".\""," \"","That","'s"," his"," full"," name"," and"," address",".\""]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["m","nem","onic"," device"," is"," a"," memory"," technique"," that"," helps"," you"," remember"," information"," by"," using"," acron","y","ms",","," rh","ymes",","," or"," images","."," You"," can"," also"," try"," breaking"," down"," the"," information"," into"," manag","eable"," chunks"," or"," testing"," yourself"," regularly"]}]},{"quantile_name":"Subsample Interval 3","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["and"," only",","," got"," the"," flow"," so"," tight","\u23ce"," I","'m"," the"," master"," of"," the"," game",","," got"," my"," rh","ymes"," on"," lock",".","<EOT>","\u23ce\u23ce","Human",":"," write"," me"," a"," wordpress"," plugin"," where"," users"," can"," select"," some"," radio"," buttons"," and"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["o","ui",","," nous"," av","ons",","," \u2014"," c","'","est"," une"," aff","aire",","," \u2014","\u2191"," ","Iles"," r","imes"," pa","uv","res"," \u00e0"," pla","cer",","," l"," u"," n","'","es"," plus"," ","rien",".","\u2191"," Nous"," all","ons"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["song"," is"," also"," thought"," to"," have"," originated"," in"," the"," ","1","760","s","."," It"," was"," intended"," as"," a"," rh","yme"," for"," children"," to"," ch","ant",","," and"," the"," key"," element",","," the"," \"","r","\""," sound",","," is"," thought"," to"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["are"," a"," pens","are"," che"," sono"," il"," m","io"," dio"," person","ale","!\"","\u23ce\u23ce","\u2191","Questa"," batt","uta"," fa"," r","ima"," e"," m","esc","ola"," il"," tema"," dell","'","u","gu","ag","li","anza"," con"," la"," relig","ione"," in"," un"," modo"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["p","eut","\u00ea","tre"," peu"," s","ouc","ieux"," de"," la"," cad","ence"," et"," de"," la"," rich","esse"," de"," la"," ","rime",","," \u00e9tait"," plus"," \u00e0"," son"," ","aise"," sur"," son"," ch","eval"," de"," bataille"," que"," l","ors","qu","'","il"," en"]}]},{"quantile_name":"Subsample Interval 4","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["rique"," en"," pare","ille"," mat","i\u00e8re",","," il"," a"," une"," t","rop"," v","ie","ille"," hab","itude"," de"," faire"," r","imer"," les"," ch","iff","res"," avec"," la"," th","\u00e8","se"," qu","'","il"," ve","ut"," s","out","enir","."," (","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["multiple"," editions"," and"," has"," a"," dedicated"," fan"," base",".","<EOT>","\u23ce\u23ce","Human",":"," Please"," write"," an","\u21ea"," AB","AB"," rh","yme"," scheme"," poem"," about"," friendship",","," two"," dolphins","\u2191"," Sok","rates"," and"," NAME","_","1",".","\u23ce\u23ce","Assistant",":"," Here","'s"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ne","  ","t","arda","  ","pas","  ","\u00e0","  ","s","'","aper","ce","voir","  ","que","  ","la","  ","","rime","  ","\u00e9tait"," ","\u23ce","fa","ite","  ","pour","  ","l","'","","ore","ille","  ",":","  ","il","  ","entrepr"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.23,0.0,0.49,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.84],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":[","," contact"," your"," healthcare"," provider",".","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","What"," nurs","ery"," rh","ymes"," rh","yme"," with"," ","'","cat","'","?","\u23ce\u23ce","Assistant",":"," Here"," are"," some"," nurs","ery"," rh","ymes"," or"," rh","y","ming"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["The"," base"," and"," height"," are"," the"," two"," legs","."," ","\u23ce","\u2022"," Come"," up"," with"," a"," simple"," song"," or"," rh","yme"," to"," help"," you"," remember",","," like"," \"","Base"," times"," height",","," divided"," by"," two",","," calc","ulates"," the"," area"," for"]}]},{"quantile_name":"Subsample Interval 5","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["dinner"," set",","," ","100","\u23ce","ji","ie","ees"," in"," English"," semi","-","por","cel","ain",",","\u23ce","lo"," ","rime"," line",","," sold"," at"," $","11",".","50",","," now","\u23ce"," only"," -","..","\u23ce","(","inn"," h","um","!"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["scheme",":","\u23ce\u23ce","\u21ea","A","AB","BA","\u23ce\u23ce"," Here","'s"," an"," example"," of"," a"," lim","erick"," with"," the"," correct"," rh","yme"," scheme",":","\u23ce\u23ce","There"," was"," a"," young"," girl"," from","\u2191"," N","ant","ucket","\u23ce","\u2191"," Whose"," l","ime","ricks"," made"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.59,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["is"," a"," ","14","-","line"," poem"," with"," a"," specific"," rh","yme"," scheme"," and"," NAME","_","1","."," The"," rh","yme"," scheme"," I"," will"," use"," is","\u21ea"," AB","AB","\u21ea"," CD","CD","\u21ea"," EF","EF"," G","G",","," which"," is"," also"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["cer"," in"," every"," b","ouse",";"," when"," as"," the"," devil"," is"," busy",","," ","\u23ce","because"," he"," knows"," his"," ","rime"," is"," short",","," so"," his"," instruments"," are"," busy",","," ","\u23ce","because"," they"," thi","uk"," their"," time"," is"," be","gl"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ics",".","  ","No","  ","wr","eat","hed","  ","column"," \u2014"," ^","no","  ","\u2191","Ru","nic"," ","\u23ce","rh","yme"," \u2014"," not","  ","the","  ","smallest","  ","thing","  ","to","  ","tell","  ","of","  ","a","  ","remote"," ","\u23ce"]}]},{"quantile_name":"Subsample Interval 6","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.31,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["au","\u2191"," Cu","i","rios"," or"," ","1"," ","how"," Old"," is"," J","ut","I","\u2191"," Spe","ir"," i"," ","rime"," ago"," Of"," Judge"," G","zi","u","Em","r","r"," ","31"," ","\u2191","Sr","irit"," of"," i"," the"," Superior","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.31,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["b","B","B"," ","\u23ce\u23ce","for"," c","lad","de"," ","1"," ","bad","de"," ","1"," ","don","ot"," rh","yme"," with"," ","rade","."," ","\u23ce","cl","ad"," J"," bad"," J"," ","\u23ce\u23ce","3",")","\u21ea"," XX","III"," looks"," to"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["f",",",","," ","tn","-"," meat"," ?>","."," a"," three","-","days","*"," en","\u23ce"," t","be","\u2191"," ","Rime","."," _","\u23ce","I","';","-,"," vm","."," r",".","t"," ol"," ","!'","as","-","ID","l","!","*"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":[":"," Write"," a"," recommendation"," of"," things"," to"," do"," in","\u2191"," Cologne"," in"," a"," po","etic"," style",","," with"," r","yh","mes",","," in"," a"," maximum"," of"," ","5"," ","sentences","\u23ce\u23ce"," Assistant",":"," In","\u2191"," Cologne",","," where"," history"," and"," culture"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.29,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["de","part"," in"," o"," for"," ","ood","\u23ce"," Mock"," t","ho"," m","utter"," was"," thoroughly"," go","io"," over","\u23ce"," ","rime"," Senator"," wore"," nt","ix","l","ous"," to"," know"," m","st","vil","ieri","\u23ce"," they"," were"," to"," s","tn","nd"," That"]}]},{"quantile_name":"Subsample Interval 7","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["'","l","U","K","l","O"," *"," ","atu",","," t"," y"," x","\u23ce"," w","t",">","u",","," ","rime",",","?"," .","t"," i"," v"," j","\u23ce"," V","V","H","0","L","X","3","ALL"," G"," ","'","UC"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["."," (","C","."," T","."," C",".)"," \u201e","\u2191","Petit","\u2191"," Bl","eu","\""," mel","det"," aus","\u2191"," Re","imes",","," der","\u2191"," Reg","ier","ungs","comm","is","sar"," Major","\u2191"," Carri","\u00f6","re"," h","abe"," dem","\u2191"," An","su"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.26,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["end",","," et"," o\u00f9"," l","'","on"," sea"," habits",","," m","."," |","\u2191"," Rh","y","mer",",","\u2191"," Rh","ym","ster",","," s","."," rim","eur",","," e","use"," {"," ","\u23ce","\u2191","Rev","iction",","," s","."," ret","our"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["mine"," del","icate","."," E","'"," ne",">"," ","\u23ce","cess","arlo"," tal","v","olta"," mu","tar"," nome"," ai"," r","imed",")"," ,"," ind","or","are"," pill","ole"," ","\u23ce","di"," s","empl","ice"," moll","ica"," di"," p","ane",","," dare"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["rom","ains","  ","et","  ","les","  ","\u2191","Gr","ecs"," ,","  ","ont","  ","rim","\u00e9","  ","et","  ","r","iment","  ","encore",".","  ","Le","  ","ret","our","  ","des"," ","\u23ce","m","\u00ea","mes","  ","sons","  ","est","  "]}]},{"quantile_name":"Subsample Interval 8","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ort","amente"," infin","ua"," dello"," fe","if","ma","\u2191"," Greco"," d","over","fi"," gli"," princip","i"," >"," e"," la"," r","imo","-"," ","\u23ce","ta"," origine"," did","ur","re"," ."," Ad"," an","num"," l","occ","x","xx","."," a",","," "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[" ","724",","," ","893","."," ","\u23ce\u23ce","6"," ","\u23ce\u23ce\u23ce","82"," ","\u23ce\u23ce\u23ce","\u2191","Roth","ort"," \u2014","\u2191"," Ry","mer","-","Jones"," ","\u23ce\u23ce\u23ce","\u2191","Roth","ert",","," A",".,","\u2191"," Gro\u00dfe","\u2191"," Gener","at","oren","."," ","1","901"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["."," This"," can"," be"," done"," in"," a"," variety"," of"," ways",","," by"," hum","ming",","," rec","iting"," nurs","ery"," rh","ymes"," or"," singing"," improv","ised"," syll","ables",".","\u23ce","4","."," Start"," to"," layer"," your"," melody","."," If"," you"," have"," a"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["."," ","\u23ce\u23ce\u23ce\u23ce\u23ce","39","."," A","u","gu","Me"," (","L","'","Arc"," d",")"," ","1"," ","\u2191","R","imin"," ","\u23ce\u23ce\u23ce\u23ce\u23ce\u23ce\u23ce\u23ce\u23ce","A","agu","M","ule","."," ","10","."," ","\u23ce\u23ce\u23ce\u23ce\u23ce","\u2191","Am","mi","rato"," (","\u2191","Sci","pi"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[":"," tur","\u017f","t","aat"," aus","ein","ander",","," wo"," ","bie"," \u00f6","ffent","li","he","\u2191"," Macht"," nur"," e","ime","\u2191"," Part","ei"," mehr"," fit",","," ge","ha","\u00df","t"," und"," b","int","erg","angen"," von"," bem",","," der"," fie"]}]},{"quantile_name":"Bottom Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["so"," proprio"," di","rti","\u23ce","<","giul","io","_",">"," che"," pac","ch","etto"," d","evo"," m","ett","ere"," per"," rim","ett","ere"," network","-","manager","?"," ho"," gn","ome","\u23ce","<","gl","p","iana",">"," solo"," il"," mouse"," da"," "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["'s"," hand"," moves"," to"," the"," horse","'s"," fl","ank",","," her"," touch"," gentle"," yet"," firm","."," She"," begins"," to"," r","ub"," him"," in"," slow",","," circular"," mot","ions",","," her"," palm"," gl","iding"," over"," his"," smooth"," skin","."," The"," horse","'s"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["hin","dem","ith","."," ","2",".","\u2191"," Z","even","\u2191"," Ara","bes","ques"," (","\u2191","Et","udes"," rh","yt","m","iques","),","\u2191"," Bo","hu","sl","av","\u2191"," Mart","inu",".","\u23ce","\u21b9","4",".","00"," ","n",".","m"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["et"," art",";"," tr","adu","'t"," de"," fan","gt","ais"," de","\u2191"," Ac","con",","," par"," M","."," R","i","M","AC","T"," de","ux","i\u00e8me"," \u00e9","dit",".,"," rev","ue",","," cor","rig","\u00e9e"," et"," aug","ment","\u00e9e","."," Un"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["poem"," reflects"," the"," author","'s"," rage"," at"," the"," sens","eless"," suffering"," of"," war",".","\u23ce\u23ce","Effect",":"," The"," poem"," is"," a"," shocking"," portra","yal"," of"," the"," death"," and"," suff","erings"," of"," soldiers"," in"," W","W","1","."," It"," leaves"," the"," reader"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["icht","\u2191"," Verl","etz","ungen"," an"," den","\u2191"," R\u00f6","h","ren"," ver","urs","achen","."," Dies"," kann"," zu","\u2191"," R","issen"," oder","\u2191"," Ab","pl","atz","ungen"," f\u00fchr","en",","," was"," wi","eder","um"," zu"," einem","\u2191"," Ra","tten"," in"," der"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["str","aks"," ver","tr","ekt",","," naar"," hun"," land"," z","al"," l","aten"," ter","ug","ke","eren","."," In"," r","ijn"," ge","he","ime"," plan","nen"," past"," ","immers"," de"," aan","w","ez","ig","heid"," van"," een"," v","re","em","de"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","\u2191","Satellite","-","based"," mobile"," communication"," systems"," are"," subject"," to","\u2191"," R","ician"," channel"," f","ading",".","\u2191"," R","ician"," channel"," f","ading"," results"," from"," a"," combination"," of"," f","ading"," due"," to"," atmospheric"," effects"," and"," of","\u2191"," Ra","yle","igh"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ond","ire"," i"," nost","ri"," des","id","eri"," pi\u00f9"," pro","fon","di",".","\u23ce\u23ce","\u2191","S","ento"," le"," sue"," lac","rime"," di"," p","iac","ere",","," e"," sap","evo"," che"," ab","bi","amo"," fatto"," qual","c","osa"," di"," spec","iale",","]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["te"," f","esten","\u2191"," Schl","\u00f6s","ser"," to","ib","erst","an","ben",".","\u23ce","Der","\u2191"," Schw","ei","\u00df"," r","ann"," ihm"," von"," ","ber","\u2191"," Stir","n",",","\u23ce","bas","\u2191"," Bl","ut"," von"," ben","\u2191"," H","\u00e4n","ben"," ..."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["two"," people",","," stran","gers",","," come"," to"," meet",".\""," \"","There","'s"," a"," poem"," by"," a"," Venezuelan"," writer"," that"," begins","\""," \"\"","The"," earth"," turned"," to"," bring"," us"," closer",".\""," \"\"","It"," turned"," on"," ","its","elf","and"," in"," us"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["w","agen","."," Het"," was"," de"," gro","ote"," w","agen",","," die"," gebruikt"," werd"," om"," v","rach","ten"," te"," rij","den",","," zonder"," hu","if",","," z","oo","dat"," i","eder","een"," kon"," beg","rij","pen",","," dat"," de"," bo","er"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["beautiful"," heir",".\""," \"","My"," father"," is"," most"," unfortun","ate",".\""," \"","He"," battles"," a"," w","arl","ord",","," rh","es","us",".\""," \"","Our"," land"," is"," torn"," by"," civil"," war",".\""," \"","Every"," day",","," villages"," are"," destroyed",","," crops"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u2191","Grand","eza",",","\u2191"," Hu",".","\u2191"," All","\u00ed"," del"," Imperio"," a"," Carlos"," las"," insign","ias"," para"," que"," r","ija"," el"," cet","ro",","," para"," que"," la"," Fe"," emp","uje"," el"," est","oque",","," y"," al"," sob","erb","io","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ce"," n","'","est"," pas"," en"," c","et"," end","roit",";"," c","'","est"," qu","and"," ","elles"," ex","pr","iment"," un"," grand"," sentiment","."," Des"," con","tes"," est"," ig","no","ble","."," (","V",".)"," ","2"," ","C","'"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["by"," leaving"," at"," least"," six"," feet"," from"," other"," people",".","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","What"," r","ims"," will"," fit"," my"," ","2","012"," ","\u2191","Nis","san","\u2191"," Max","ima"," wheels","?"," ","\u23ce\u23ce","Assistant",":"," "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","And"," all"," the"," fun",","," that","'s"," yet"," to"," be",".","\u23ce\u23ce","Human",":"," The"," poem"," your"," provided"," has"," ","26"," ","words","."," I"," kind","ly"," ask"," again",":"," please"," create"," a"," poem"," with"," ","30"," ","-"," "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["do"," my"," best",",","\u23ce","To"," bring"," to"," life"," what"," you"," see",","," to"," make"," it"," a"," reality"," that"," can"," last",".","\u23ce","For"," I"," am"," a"," writer",","," a"," master"," of"," the"," verse",",","\u23ce","And"," I","'ll"," use"," my"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["'t"," actually"," an"," infection",".\""," \"","It","'s"," just"," a"," name"," that"," people"," have"," made"," up",".\""," \"","\u2191","Rim","ming",","," yeah",".\""," \"","I"," have"," incidents"," every"," day",".\""," \"","Yeah",","," it","'s"," every"," day"," it","'s"," different"]}]}],"top_logits":["schemas","sque","schemes","sequ","ationBiBTeX","\u0441\u043e\u0444\u0438\u044f","scheme","sche","successfully","dece"],"bottom_logits":["med","ap","ming","min","mo","ition","hed","th","ment","ments"]}