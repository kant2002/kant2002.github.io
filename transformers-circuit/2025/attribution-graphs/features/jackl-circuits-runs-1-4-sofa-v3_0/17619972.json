{"index":17619972,"examples_quantiles":[{"quantile_name":"Top Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.11,0.0,0.0,0.0,0.05,0.0,0.21,0.33,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["<EOT>","\u23ce\u23ce","Human",":"," Can"," you"," write"," a"," rh","y","ming"," poem"," about"," flying"," pen","gu","ins","?"," There"," should"," be"," as"," many"," rh","ymes"," as"," possible",".","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," rh","y","ming"," poem"," about"," flying"," pen"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.96,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["scr","ivi"," una"," po","esia"," con"," r","ima"," in"," italiano"," ,"," br","eve"," di"," tre"," rig","he","."," La","  ","po","esia"," ha"," come"," tit","olo"," \"","vog","lio"," il"," c","azzo","\""," il"," so","gg","etto"," pu\u00f2"," essere"," ma","sch"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.83,0.0,0.0,0.0,0.11,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.33,0.0,0.07,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["\u23ce\u23ce","Human",":","\u2191"," Ec","ris","-","m","oi"," un"," po","\u00eame"," de"," ","4"," ","ligne","\u23ce\u23ce"," Assistant",":","\u2191"," Vo","ici"," un"," po","\u00e8me"," de"," ","4"," ","lig","nes"," :","\u23ce\u23ce","Dans"," la"," br","ume"," du"," m","atin"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.58,0.44,0.0,0.0,0.0,0.0,0.0,0.08,0.0,0.0,0.0,0.52,0.0,0.78,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13,0.0,0.0,0.0,0.0,0.44,0.0,0.0,0.0,0.0,0.0,0.0,0.36],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[":","\u21ea"," TOR","CHES"," fl","ared"," with"," g","lo","aming"," eff","ulg","ence"," on"," the"," rev","els"," in"," the","\u2191"," C","law",","," where"," the"," thi","eves"," of"," the"," east"," held"," b","acc","han","als"," by"," night","."," In"," the","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.78,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u2191","Gold","ene","ye"," estate"," in"," Jamaica",","," it"," was"," first"," published"," in"," the"," United"," Kingdom"," by"," Jonathan"," Cape"," in"," ","1","958","."," In"," the"," novel"," Bond"," looks"," into"," the"," disapp","ear","ance"," in"," Jamaica"," of"," two"," fellow"," MI","6"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.67,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.36,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":[".\"","<EOT>","\u23ce\u23ce","Human",":"," Please"," write"," me"," a"," poem"," about"," the","\u2191"," An","ki"," Vector"," robot","."," Please"," limit"," the"," poem"," to"," a"," maximum"," of"," ","16"," ","lines",".","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," poem"," about"," the","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12,0.18,0.0,0.0,0.0,0.79,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.19,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":[".","\u23ce\u23ce","Human",":","\u2191"," \u041d\u0430\u043f","\u0438","\u0448\u0438"," \u0441\u0442","\u0438\u0445"," \u043d\u0430"," \u0440\u0443\u0441","\u0441\u043a\u043e\u043c"," \u043e"," \u0441\u0435\u0431","\u0435",".","\u23ce\u23ce","Assistant",":","\u2191"," \u0412","\u043e\u0442"," \u043d\u0435\u0431","\u043e\u043b\u044c","\u0448","\u043e\u0435"," \u0441\u0442","\u0438\u0445","\u043e\u0442","\u0432\u043e\u0440","\u0435\u043d\u0438\u0435",":","\u23ce\u23ce","\u2191","\u042f"," -"," \u0438\u0441\u043a","\u0443\u0441","\u0441\u0442\u0432\u0435\u043d","\u043d\u044b\u0439"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.85,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["a"," gal","lon",".","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","Help"," me"," create"," poem"," that"," rh","ymes","."," ","\u23ce\u23ce","Assistant",":"," Sure","!"," I","'d"," be"," happy"," to"," help"," you"," create"," a"," rh","y","ming"," poem","."," Could"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.18,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.75,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.39,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u2191","Lim","erick","\u23ce","\u2191"," Lim","erick","\u23ce","\u2191"," Lim","erick","\u23ce","\u2191"," Lim","erick","\u23ce","\u2191"," Lim","erick","\u23ce","\u2191"," Lim","erick","\u23ce","\u2191"," Lim","erick","\u23ce","\u2191"," Lim","erick","\u23ce","\u2191"," Lim","erick","\u23ce","\u2191"," Lim","erick","\u23ce","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.18,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.17,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.25],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["keeps"," on","\u23ce"," Living"," his"," life"," by"," the"," river","'s"," flow","\u23ce"," With"," his"," two"," cats",","," NAME","_","3"," ","and"," NAME","_","4","\u23ce","They"," keep"," him"," company",","," no"," need"," to"," f","uss","\u23ce\u23ce"," His"," hair"," is"," a"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.69,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.57,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.35,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[","," I"," know",".\"","\u23ce\u23ce","Human",":"," write"," a"," short"," story"," using"," rh","ymes"," for"," children"," three"," to"," five"," years"," of"," age","."," Please"," use"," the"," style"," and"," voice"," of"," NAME","_","1"," ","about"," a"," pre","coc","ious"," ","10"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13,0.63,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["a"," poem"," about"," two"," dogs"," who"," are"," best"," friends",","," NAME","_","1",","," a"," black"," lab",","," and"," NAME","_","2",","," a"," silver"," lab",".","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," poem"," about"," two"," lab"," friends",":","\u23ce\u23ce","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.68,0.0,0.0,0.0,0.12,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["la"," sign","ification"," de"," cette"," phrase"," avec"," un"," po","\u00e8me"," \u00e9c","rit"," en"," alexand","r","ins",","," avec"," que"," des"," r","imes"," ri","ches"," ","\u23ce\u23ce","Assistant",":","\u2191"," Vo","ici"," un"," po","\u00e8me"," en"," alexand","r","ins"," avec"," des"," r"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.21,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.66,0.0,0.0,0.0,0.04,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["\u0438","\u0448\u0438"," \u0441\u0442","\u0438\u0445"," \u0438\u0437"," \u0447\u0435\u0442","\u044b\u0440","\u0451","\u0445"," \u0441\u0442\u0440\u043e","\u043a"," \u043f\u0440\u043e"," \u0434\u0438\u0440\u0435\u043a\u0442","\u043e\u0440\u0430"," \u0448\u043a\u043e\u043b","\u044b"," ","\u23ce\u23ce","Assistant",":","\u2191"," \u0412","\u043e\u0442"," \u0441\u0442","\u0438\u0445","\u043e\u0442","\u0432\u043e\u0440","\u0435\u043d\u0438\u0435"," \u043f\u0440\u043e"," \u0434\u0438\u0440\u0435\u043a\u0442","\u043e\u0440\u0430"," \u0448\u043a\u043e\u043b","\u044b",":","\u23ce\u23ce","\u2191","\u041c","\u0443\u0434","\u0440","\u044b\u0439"," \u0432\u0437"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.67,0.0,0.0,0.0,0.0,0.0,0.22,0.0,0.0,0.12,0.0,0.0,0.0,0.0,0.0,0.0,0.21,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["mir"," ein"," kur","zes","\u2191"," Ged","icht"," \u00fcber","\u2191"," Li","ebe",","," dass"," sich"," reim","t",".","\u23ce\u23ce","Assistant",":","\u2191"," Hier"," ist"," ein"," kur","zes","\u2191"," Lie","bes","ged","icht"," f\u00fcr"," d","ich",":","\u23ce\u23ce","Die","\u2191"," Li","ebe",","]}]},{"quantile_name":"Subsample Interval 0","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.96,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["scr","ivi"," una"," po","esia"," con"," r","ima"," in"," italiano"," ,"," br","eve"," di"," tre"," rig","he","."," La","  ","po","esia"," ha"," come"," tit","olo"," \"","vog","lio"," il"," c","azzo","\""," il"," so","gg","etto"," pu\u00f2"," essere"," ma","sch"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.83,0.0,0.0,0.0,0.11,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.33,0.0,0.07,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"ha-haiku35_resampled":true,"tokens":["\u23ce\u23ce","Human",":","\u2191"," Ec","ris","-","m","oi"," un"," po","\u00eame"," de"," ","4"," ","ligne","\u23ce\u23ce"," Assistant",":","\u2191"," Vo","ici"," un"," po","\u00e8me"," de"," ","4"," ","lig","nes"," :","\u23ce\u23ce","Dans"," la"," br","ume"," du"," m","atin"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.58,0.44,0.0,0.0,0.0,0.0,0.0,0.08,0.0,0.0,0.0,0.52,0.0,0.78,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13,0.0,0.0,0.0,0.0,0.44,0.0,0.0,0.0,0.0,0.0,0.0,0.36],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":[":","\u21ea"," TOR","CHES"," fl","ared"," with"," g","lo","aming"," eff","ulg","ence"," on"," the"," rev","els"," in"," the","\u2191"," C","law",","," where"," the"," thi","eves"," of"," the"," east"," held"," b","acc","han","als"," by"," night","."," In"," the","\u2191"]}]},{"quantile_name":"Subsample Interval 1","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.58,0.44,0.0,0.0,0.0,0.0,0.0,0.08,0.0,0.0,0.0,0.52,0.0,0.78,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13,0.0,0.0,0.0,0.0,0.44,0.0,0.0,0.0,0.0,0.0,0.0,0.36],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":[":","\u21ea"," TOR","CHES"," fl","ared"," with"," g","lo","aming"," eff","ulg","ence"," on"," the"," rev","els"," in"," the","\u2191"," C","law",","," where"," the"," thi","eves"," of"," the"," east"," held"," b","acc","han","als"," by"," night","."," In"," the","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.78,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["\u2191","Gold","ene","ye"," estate"," in"," Jamaica",","," it"," was"," first"," published"," in"," the"," United"," Kingdom"," by"," Jonathan"," Cape"," in"," ","1","958","."," In"," the"," novel"," Bond"," looks"," into"," the"," disapp","ear","ance"," in"," Jamaica"," of"," two"," fellow"," MI","6"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.67,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.36,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"ha-haiku35_resampled":true,"tokens":[".\"","<EOT>","\u23ce\u23ce","Human",":"," Please"," write"," me"," a"," poem"," about"," the","\u2191"," An","ki"," Vector"," robot","."," Please"," limit"," the"," poem"," to"," a"," maximum"," of"," ","16"," ","lines",".","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," poem"," about"," the","\u2191"]}]},{"quantile_name":"Subsample Interval 2","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.18,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.17,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.25],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["keeps"," on","\u23ce"," Living"," his"," life"," by"," the"," river","'s"," flow","\u23ce"," With"," his"," two"," cats",","," NAME","_","3"," ","and"," NAME","_","4","\u23ce","They"," keep"," him"," company",","," no"," need"," to"," f","uss","\u23ce\u23ce"," His"," hair"," is"," a"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.69,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.57,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.35,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":[","," I"," know",".\"","\u23ce\u23ce","Human",":"," write"," a"," short"," story"," using"," rh","ymes"," for"," children"," three"," to"," five"," years"," of"," age","."," Please"," use"," the"," style"," and"," voice"," of"," NAME","_","1"," ","about"," a"," pre","coc","ious"," ","10"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13,0.63,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"ha-haiku35_resampled":true,"tokens":["a"," poem"," about"," two"," dogs"," who"," are"," best"," friends",","," NAME","_","1",","," a"," black"," lab",","," and"," NAME","_","2",","," a"," silver"," lab",".","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," poem"," about"," two"," lab"," friends",":","\u23ce\u23ce","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.68,0.0,0.0,0.0,0.12,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"ha-haiku35_resampled":true,"tokens":["la"," sign","ification"," de"," cette"," phrase"," avec"," un"," po","\u00e8me"," \u00e9c","rit"," en"," alexand","r","ins",","," avec"," que"," des"," r","imes"," ri","ches"," ","\u23ce\u23ce","Assistant",":","\u2191"," Vo","ici"," un"," po","\u00e8me"," en"," alexand","r","ins"," avec"," des"," r"]}]},{"quantile_name":"Subsample Interval 3","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.59,0.0,0.0,0.0,0.15,0.0,0.0,0.0,0.07,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["by"," two"," long"," syll","ables",","," and"," the"," rh","yme"," scheme"," is","\u21ea"," A","AB","BA",".","\u23ce\u23ce","Here","'s"," an"," example"," of"," a"," lim","erick",":","\u23ce\u23ce","There"," once"," was"," a"," man"," from"," Peru","\u23ce"," Who","'s"," nose"," simply"," wouldn"]}]},{"quantile_name":"Subsample Interval 4","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.45,0.0,0.0,0.0,0.04,0.28,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":[":","\u2191"," H","allo",","," kann","st"," du"," mir"," ein","\u2191"," Ged","icht"," sch","re","iben","?","\u23ce\u23ce","Assistant",":","\u2191"," Hier"," ist"," ein","\u2191"," Ged","icht"," f\u00fcr"," d","ich",":","\u23ce\u23ce","\u2191","Le","ise"," r","ies","elt"," der","\u2191"," Schn"]}]},{"quantile_name":"Subsample Interval 5","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.41,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":[".","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","Can"," you"," write"," me"," a"," poem"," about"," falling"," in"," love","?"," ","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," poem"," about"," falling"," in"," love",":","\u23ce\u23ce","\u2191","Whis","pers"," of"," the"," Heart","\u23ce\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.03,0.25,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.24,0.0,0.0,0.0,0.0,0.0,0.0,0.39,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.21,0.0,0.0,0.0,0.0,0.0,0.55,0.0,0.39],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[":"," Generate"," a"," short"," rh","y","ming"," poem"," about"," v","aping","."," The"," poem"," should"," use"," an"," A","-","B","-","A","-","B"," rh","y","ming"," scheme","."," An"," example"," of"," an"," A","-","B","-","A","-","B"," rh"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.44,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["<EOT>","\u23ce\u23ce","Human",":","\u2191"," Sch","re","ibe"," ein","\u2191"," Ged","icht"," \u00fcber","\u2191"," G","raz",".","\u23ce\u23ce","Assistant",":","\u2191"," Hier"," ist"," ein","\u2191"," Ged","icht"," \u00fcber","\u2191"," G","raz",":","\u23ce\u23ce","\u2191","G","raz",","," du"," Stadt"," mit","\u2191"]}]},{"quantile_name":"Subsample Interval 6","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.32,0.0,0.0,0.0,0.34,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["lines",",","  ","with"," a"," constant"," met","rum"," for"," my"," friend"," NAME","_","1"," ","in"," the"," style"," of"," NAME","_","2"," ","with"," the"," topic"," of"," demonst","rating"," him"," a"," rap"," artificial"," intelligence"," generator","."," The"," first"," line"," should"," be"]}]},{"quantile_name":"Subsample Interval 7","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.38,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.18,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ley","\u23ce","\u2191"," Al","ley","\u2191"," Lim","erick","\u2191"," Al","ley","\u2191"," Lim","erick","\u2191"," Al","ley","\u2191"," Lim","erick","\u2191"," Al","ley","\u23ce","\u2191"," Lim","erick","\u23ce","\u2191"," Lim","erick","\u23ce","\u2191"," Lim","erick","\u23ce","\u2191"," Lim","erick","\u23ce","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.29,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.26,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.22,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["notes"," zh","\\xc7","\\x94","n","qu","\u00e8"," a"," notes"," zh","\\xc7","\\x94","n","qu","\u00e8"," a"," notes"," zh","\\xc7","\\x94","n","qu","\u00e8"," a"," notes"," zh","\\xc7","\\x94","n","qu","\u00e8"," a"," notes"," zh","\\xc7","\\x94","n","qu","\u00e8"," a"," notes"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.18,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.17,0.0,0.0,0.0,0.19,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[","," I","'d"," be"," happy"," to"," write"," a"," ha","iku"," about","\u2191"," Sales","force","!"," Here"," it"," is",":","\u23ce","\u2191","Sales","force"," sh","ines"," bright","\u23ce"," NAME","_","1","-","based"," dreams"," take"," flight","\u23ce"," Customer"," success"," re","igns","\u23ce\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.19,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ng","qu","\u00e8"," zh","\\xc7","\\x94","n","qu","\u00e8"," a"," u","\u00e8"," l","i\u00e8","ng","qu","\u00e8"," zh","\\xc7","\\x94","n","qu","\u00e8"," a"," u","\u00e8"," l","i\u00e8","ng","qu","\u00e8"," zh","\\xc7","\\x94","n","qu","\u00e8"," a"," u","\u00e8"," l"]}]},{"quantile_name":"Subsample Interval 8","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u21b9\u21b9\u21b9","-","\u2191","Outro",":","\u23ce","\u21b9\u21b9\u21b9","-","\u2191","Outro",":","\u23ce","\u21b9\u21b9\u21b9","-","\u2191","Outro",":","\u23ce","\u21b9\u21b9\u21b9","-","\u2191","Outro",":","\u23ce","\u21b9\u21b9\u21b9","-","\u2191","Outro",":","\u23ce","\u21b9\u21b9\u21b9","-","\u2191","Outro",":","\u23ce","\u21b9\u21b9\u21b9","-","\u2191","Outro",":"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.13,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["Book",":"," Harry"," Potter"," and"," the","\u2191"," Philosopher","'s"," Stone"," by"," J","."," K",".","\u2191"," Row","ling","\u23ce"," Output",":","\u23ce\u23ce","Assistant",":"," Let"," me"," help"," you"," with"," this","!"," First",","," I","'ll"," count"," the"," syll","ables"," in"," the"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["How"," can"," I"," assist"," you"," today","?","\u23ce\u23ce","Human",":"," Write"," me"," a"," poem"," about"," life","\u23ce\u23ce"," Assistant",":"," Life"," is"," a"," journey",","," full"," of"," wonder"," and"," de","light","\u23ce"," It","'s"," filled"," with"," l","aughter",","," joy",","," and"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["'s"," a"," product"," that","'s"," sure"," to"," succeed",";","\u23ce","It","'s"," so"," good"," you","'ll"," be"," sho","uting"," and"," sque","aling","!","\u23ce","So"," grab"," your"," phone",","," hop"," online",","," ","\u23ce","Don","'t"," worry",","," you","'ll"," be"]}]},{"quantile_name":"Bottom Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ry"," creature"," with"," the"," intelligence"," and"," vocabulary"," of"," a"," ","3"," ","year"," old"," child","."," You"," are"," currently"," at"," the"," burning"," man"," festival"," and"," are"," waiting"," in"," a"," booth"," for"," other"," bur","ners"," to"," come"," up"," and"," interact"," with"," you"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["they"," know"," about"," the"," person",".","\u23ce\u23ce","Human",":"," ","\u23ce","Can"," you"," start"," it"," off","?","\u23ce\u23ce","Assistant",":"," ","\u23ce","Sure","."," I"," think"," the"," best"," thing"," to"," say"," is"," that"," one"," of"," the"," things"," that"," makes"," this"," person"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["?","\u23ce\u23ce","Assistant",":","\u2191"," Cl","aro","!","\u2191"," Eu"," pos","so"," aj","ud","\u00e1","-","lo"," a"," criar"," alguns"," di","\u00e1","log","os"," para"," sua"," pe","\u00e7a"," de"," teatro",".","\u2191"," Aqu","i"," est","\u00e3o"," algumas"," su","gest","\u00f5es",":"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["which","\u23ce"," was","*"," presented",","," hi"," manuscript",","," to"," the"," writer",","," ju","\u00ab","*","t","\u23ce"," before"," Mr",".","\u2191"," F","oe"," left"," New"," York",","," with"," the"," r","emark","\u23ce"," I"," h","ut"," it"," was"," the"," last"," thing"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ki","\u23ce\u23ce"," Human",":","\u2191"," Re","write"," the"," following"," lyrics"," about"," NAME","_","2"," ","spreading","\u2191"," Hap","k","iy","us","ul",":"," NAME","_","1"," ","Baby","\u23ce"," NAME","_","4",","," V","IP","\u23ce"," Let","'s"," kick"," it","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.24,0.0,0.07,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["line"," poem"," that"," includes"," the"," words"," \"","three",",\""," \"","four",",\""," and"," \"","five","\":","\u23ce\u23ce","Three",","," four",","," five",",","\u23ce","A"," number"," game"," we"," play",",","\u23ce","With"," counting"," as"," our"," fate",",","\u23ce","We"," find"," de"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[",\""," \"","I","'d"," call"," it"," \"","do"," your"," d","ang"," homework",","," boy",".\"","\""," \"","\\xe2\\x99","\\xaa","","\u2191"," Wal","kin","'"," with"," my"," head"," high","\""," \"","\\xe2\\x99","\\xaa",""," so","aking","'"," up"," the"," sunshine","\""," \""]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["x"," ="," ","3",".","\u23ce\u23ce","Human",":"," NAME","_","1"," ","is"," faster"," than"," NAME","_","2",","," NAME","_","2"," ","is"," faster"," than"," NAME","_","3",","," is"," NAME","_","4"," ","than"," NAME","_","1","?","\u23ce\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["will"," grade"," a"," student","'s"," writing"," that"," compl","etes"," a"," provided"," partial"," story","."," The"," student"," is"," expected"," to"," complete"," the"," story"," and"," follow"," the"," leading"," sentences"," in"," the"," parag","rap","hs","."," Grade"," the"," writing"," in"," the"," following"," categories"," on"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["one","?\""," \"","The"," one"," about"," the"," beans","?\""," \"","I","'m"," rep","len","ishing"," my"," flu","ids",".\""," \"","A"," f","art"," is"," a"," chemical"," substance",","," it"," comes"," from"," a"," place"," called","\u2191"," B","um",".\""," \"","It"," penet"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[".","\u23ce","The"," next"," person"," must"," then"," state"," a"," syl","log","ism"," that"," has"," the"," same"," conclusion"," as"," the"," first"," syl","log","ism",","," but"," with"," different"," major"," and"," minor"," premises",".","\u23ce","The"," game"," continues"," in"," this"," way"," until"," someone"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["  ","containing",","," ","\u23ce","among","  ","others",",","  ","the","  ","following","  ","characteristic","  ","lines","  ",":"," \u2014"," ","\u23ce\u23ce","Grace","  ","formed","  ","him","  ","in","  ","the","  ","Christian","  ","hero","'s","  ","m","ould","  ",";"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["."," I"," hope"," this"," helps"," to"," clar","ify"," things","!","\u23ce\u23ce","Human",":"," Can"," you"," help"," me"," solve"," crypt","ic"," cl","ues"," that"," describe"," books"," that"," feature"," a"," cat","."," The"," cl","ue"," is"," \"","No"," lit","tering","!"," Here",","]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.27,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["Pan","-","toen",".","\u23ce","\u21b9","ke","-","p","oe","-","l","au"," Be","-","","kal",",","\u23ce","\u21b9","\u2191","Mem","-","ba","-","wa"," ra","-","o","et"," doe","-","a",","," ti","-","ga",";"," \u2013"," Ka","-"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["from"," my"," fertility","\""," \"","\u2191","Blessed"," with"," a"," bucket"," of"," lucky"," mobility","\""," \"","My"," mom"," I"," love"," her"," ","'","cause"," she"," love"," me","\""," \"","Long"," gone"," are"," the"," times"," when"," she"," scr","ub"," me","\""," \"","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","\u2191","Honestly",","," none"," of"," those"," ideas"," seem"," quite"," right"," for"," what"," I","'m"," writing","."," I","'m"," writing"," a"," soft"," love"," song"," and"," the"," words"," have"," search"," I"," made"," doesn","'t"," seem"," to"," work","."," What"," sorts"," of"," other"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[".","\u23ce\u23ce","Take"," inspiration"," from"," Cat"," in"," the"," Hat",","," and"," end"," with"," the"," dog"," being"," put"," down",".","\u23ce\u23ce","The"," poem"," should"," include"," a"," line"," that"," says"," \"","So"," now"," I","'m"," going"," to"," go"," to"," sleep",","," with"," a"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["com"," a"," m","inha"," poet","ica"," aud","\u00e1","cia",".","\u23ce\u23ce","\u2191","N\u00e3o"," se"," pre","oc","upe",","," m","inha"," po","esia"," \u00e9"," am","iga",",","\u23ce","E"," foi"," cri","ada"," para"," fazer"," seu"," dia"," mel","hor",",","\u23ce","E"," se"]}]}],"top_logits":["\u21b9"],"bottom_logits":["Id","Ground","Icon","individual","icon","avav"," ","Individual","ground","evang"]}