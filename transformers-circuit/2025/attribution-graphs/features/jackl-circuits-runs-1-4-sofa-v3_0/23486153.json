{"index":23486153,"examples_quantiles":[{"quantile_name":"Top Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.11,0.08,1.0,0.55,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["\\xa0","\uc2b5","\ub2c8","\ub2e4",".","\u23ce\u23ce","Human",":"," '","\\xec\\x99","\\xbc","\\xec","\\xaa","\\xbd","'","\uc758"," ","\ubc18","\ub300","\ub9d0","\uc740","?","\u23ce\u23ce","Assistant",":"," '","\uc624","\ub978","\\xec","\\xaa","\\xbd","'","\uc785","\ub2c8","\ub2e4",".","\u23ce\u23ce","Human",":"," ","\ud55c"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.49,0.7,0.95,0.0,0.0,0.0,0.38,0.73,0.1,0.0,0.0,0.0,0.0,0.18,0.62,0.78,0.0,0.0,0.0,0.12,0.55,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["onym"," is"," a"," word"," that"," means"," the"," opposite"," of"," another"," word","."," So",","," for"," example",","," the"," ant","onym"," of"," \"","happy","\""," would"," be"," \"","sad","\"."," And"," the"," ant","onym"," of"," \"","big","\""," would"," be"," \"","small"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.38,0.16,0.88,0.0,0.0,0.0,0.0,0.0,0.0,0.47,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["^","-","1","(","y",")"," ="," arc","cos","(","cos","(","y","))","\u23ce\u23ce\u23ce\u23ce","Therefore",","," the"," inverse"," function"," of"," y"," ="," arc"," sin"," cos"," x"," is"," y"," ="," arc","cos","(","cos","(","x",")).","\u23ce\u23ce","Human",":"," finish"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.36,0.73,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.82,0.39,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.47,0.75,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["."," ","\u23ce","For"," example",","," the"," reverse"," of"," \"","I"," am"," picking"," ap","ples"," from"," the"," apple"," tree","\""," is"," \"","tree"," apple"," the"," from"," ap","ples"," picking"," am"," I","\".","\u23ce","The"," reverse"," of"," \"","You"," know"," how"," to"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.5,0.71,0.92,0.0,0.0,0.0,0.37,0.74,0.1,0.0,0.0,0.0,0.0,0.18,0.62,0.79,0.0,0.0,0.0,0.1,0.55,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[","," for"," example",","," the"," ant","onym"," of"," \"","happy","\""," would"," be"," \"","sad","\"."," And"," the"," ant","onym"," of"," \"","big","\""," would"," be"," \"","small","\"."," ","\u23ce\u23ce","Why"," would"," a"," synonym"," need"," an"," ant","onym","?"," Well"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.49,0.7,0.95,0.0,0.0,0.0,0.38,0.73,0.1,0.0,0.0,0.0,0.0,0.18,0.62,0.78,0.0,0.0,0.0,0.12,0.55,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[","," for"," example",","," the"," ant","onym"," of"," \"","happy","\""," would"," be"," \"","sad","\"."," And"," the"," ant","onym"," of"," \"","big","\""," would"," be"," \"","small","\"."," ","\u23ce","Why"," would"," a"," synonym"," need"," an"," ant","onym","?"," Well"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.82,0.39,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.47,0.75,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.27,0.16,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["from"," the"," apple"," tree","\""," is"," \"","tree"," apple"," the"," from"," ap","ples"," picking"," am"," I","\".","\u23ce","The"," reverse"," of"," \"","You"," know"," how"," to"," l","ick"," a"," stamp","\""," is"," \"","stamp"," a"," l","ick"," to"," how"," know"," You"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.71,0.92,0.0,0.0,0.0,0.37,0.74,0.1,0.0,0.0,0.0,0.0,0.18,0.62,0.79,0.0,0.0,0.0,0.1,0.55,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["means"," the"," opposite"," of"," another"," word","."," So",","," for"," example",","," the"," ant","onym"," of"," \"","happy","\""," would"," be"," \"","sad","\"."," And"," the"," ant","onym"," of"," \"","big","\""," would"," be"," \"","small","\"."," ","\u23ce\u23ce","Why"," would"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.73,0.07,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["is"," another"," example",":","\u23ce\u23ce","*"," \"","The"," quick"," brown"," fox"," jum","ps"," over"," the"," lazy"," dog","\""," spelled"," backwards"," is"," \"","god"," ","yz","al"," e","ht"," r","evo"," s","pm","uj"," x","of"," n","wor","b"," k","ci","u"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.49,0.7,0.95,0.0,0.0,0.0,0.38,0.73,0.1,0.0,0.0,0.0,0.0,0.18,0.62,0.78,0.0,0.0,0.0,0.12,0.55,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["means"," the"," opposite"," of"," another"," word","."," So",","," for"," example",","," the"," ant","onym"," of"," \"","happy","\""," would"," be"," \"","sad","\"."," And"," the"," ant","onym"," of"," \"","big","\""," would"," be"," \"","small","\"."," ","\u23ce","Why"," would"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.36,0.73,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.82,0.39,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["Human",":"," ","\u23ce","No",","," I"," meant"," reverse"," the"," word"," order","."," ","\u23ce","For"," example",","," the"," reverse"," of"," \"","I"," am"," picking"," ap","ples"," from"," the"," apple"," tree","\""," is"," \"","tree"," apple"," the"," from"," ap","ples"," picking"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13,0.33,0.72,0.0,0.0,0.0,0.28,0.18,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[".","\u23ce","\u2191","\u0418\u0441","\u0445\u043e\u0434","\u043d\u0430\u044f"," \u043f\u043e\u0434","\u0441\u0442","\u0430\u043d\u043e\u0432","\u043a\u0430",":","\u23ce","(","34","251",")","\u23ce","\u2191","\u041e\u0431","\u0440\u0430\u0442","\u043d\u0430\u044f"," \u043f\u043e\u0434","\u0441\u0442","\u0430\u043d\u043e\u0432","\u043a\u0430",":","\u23ce","15","243","\u23ce\u23ce","\u2191","","\u041e\u0442\u0432\u0435\u0442","",":"," ","15","243","\u23ce\u23ce","Human",":"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.17,0.08,0.71,0.29,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["can"," do"," that","."," The"," sentence"," \"","item","1",".","X","-","\u21ea","ABL","ABEL",":","email","\""," spelled"," backwards"," is"," \"","email",":","X","-","\u21ea","ABL","ABEL",".","1","it","me","\".","\u23ce\u23ce","Here"," is"," another"," example",":","\u23ce\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.71,0.92,0.0,0.0,0.0,0.37,0.74,0.1,0.0,0.0,0.0,0.0,0.18,0.62,0.79,0.0,0.0,0.0,0.1,0.55,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ant","onym"," is"," a"," word"," that"," means"," the"," opposite"," of"," another"," word","."," So",","," for"," example",","," the"," ant","onym"," of"," \"","happy","\""," would"," be"," \"","sad","\"."," And"," the"," ant","onym"," of"," \"","big","\""," would"," be"," \""]}]},{"quantile_name":"Subsample Interval 0","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.49,0.7,0.95,0.0,0.0,0.0,0.38,0.73,0.1,0.0,0.0,0.0,0.0,0.18,0.62,0.78,0.0,0.0,0.0,0.12,0.55,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["onym"," is"," a"," word"," that"," means"," the"," opposite"," of"," another"," word","."," So",","," for"," example",","," the"," ant","onym"," of"," \"","happy","\""," would"," be"," \"","sad","\"."," And"," the"," ant","onym"," of"," \"","big","\""," would"," be"," \"","small"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.38,0.16,0.88,0.0,0.0,0.0,0.0,0.0,0.0,0.47,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["^","-","1","(","y",")"," ="," arc","cos","(","cos","(","y","))","\u23ce\u23ce\u23ce\u23ce","Therefore",","," the"," inverse"," function"," of"," y"," ="," arc"," sin"," cos"," x"," is"," y"," ="," arc","cos","(","cos","(","x",")).","\u23ce\u23ce","Human",":"," finish"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.36,0.73,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.82,0.39,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.47,0.75,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["."," ","\u23ce","For"," example",","," the"," reverse"," of"," \"","I"," am"," picking"," ap","ples"," from"," the"," apple"," tree","\""," is"," \"","tree"," apple"," the"," from"," ap","ples"," picking"," am"," I","\".","\u23ce","The"," reverse"," of"," \"","You"," know"," how"," to"]}]},{"quantile_name":"Subsample Interval 1","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.38,0.16,0.88,0.0,0.0,0.0,0.0,0.0,0.0,0.47,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["^","-","1","(","y",")"," ="," arc","cos","(","cos","(","y","))","\u23ce\u23ce\u23ce\u23ce","Therefore",","," the"," inverse"," function"," of"," y"," ="," arc"," sin"," cos"," x"," is"," y"," ="," arc","cos","(","cos","(","x",")).","\u23ce\u23ce","Human",":"," finish"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.36,0.73,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.82,0.39,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.47,0.75,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["."," ","\u23ce","For"," example",","," the"," reverse"," of"," \"","I"," am"," picking"," ap","ples"," from"," the"," apple"," tree","\""," is"," \"","tree"," apple"," the"," from"," ap","ples"," picking"," am"," I","\".","\u23ce","The"," reverse"," of"," \"","You"," know"," how"," to"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.5,0.71,0.92,0.0,0.0,0.0,0.37,0.74,0.1,0.0,0.0,0.0,0.0,0.18,0.62,0.79,0.0,0.0,0.0,0.1,0.55,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":[","," for"," example",","," the"," ant","onym"," of"," \"","happy","\""," would"," be"," \"","sad","\"."," And"," the"," ant","onym"," of"," \"","big","\""," would"," be"," \"","small","\"."," ","\u23ce\u23ce","Why"," would"," a"," synonym"," need"," an"," ant","onym","?"," Well"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.49,0.7,0.95,0.0,0.0,0.0,0.38,0.73,0.1,0.0,0.0,0.0,0.0,0.18,0.62,0.78,0.0,0.0,0.0,0.12,0.55,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":[","," for"," example",","," the"," ant","onym"," of"," \"","happy","\""," would"," be"," \"","sad","\"."," And"," the"," ant","onym"," of"," \"","big","\""," would"," be"," \"","small","\"."," ","\u23ce","Why"," would"," a"," synonym"," need"," an"," ant","onym","?"," Well"]}]},{"quantile_name":"Subsample Interval 2","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.17,0.08,0.71,0.29,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["can"," do"," that","."," The"," sentence"," \"","item","1",".","X","-","\u21ea","ABL","ABEL",":","email","\""," spelled"," backwards"," is"," \"","email",":","X","-","\u21ea","ABL","ABEL",".","1","it","me","\".","\u23ce\u23ce","Here"," is"," another"," example",":","\u23ce\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.71,0.92,0.0,0.0,0.0,0.37,0.74,0.1,0.0,0.0,0.0,0.0,0.18,0.62,0.79,0.0,0.0,0.0,0.1,0.55,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["ant","onym"," is"," a"," word"," that"," means"," the"," opposite"," of"," another"," word","."," So",","," for"," example",","," the"," ant","onym"," of"," \"","happy","\""," would"," be"," \"","sad","\"."," And"," the"," ant","onym"," of"," \"","big","\""," would"," be"," \""]}]},{"quantile_name":"Subsample Interval 3","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.71,0.92,0.0,0.0,0.0,0.37,0.74,0.1,0.0,0.0,0.0,0.0,0.18,0.62,0.79,0.0,0.0,0.0,0.1,0.55,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["So",","," for"," example",","," the"," ant","onym"," of"," \"","happy","\""," would"," be"," \"","sad","\"."," And"," the"," ant","onym"," of"," \"","big","\""," would"," be"," \"","small","\"."," ","\u23ce\u23ce","Why"," would"," a"," synonym"," need"," an"," ant","onym","?"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.19,0.28,0.63,0.16,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.44,0.44,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["\uac83","\uc744"," ","\uc758","\ubbf8","\ud569","\ub2c8","\ub2e4",".","\u23ce\u23ce","Human",":"," ","\ub9cc","\uc871","\uc758"," ","\ubc18","\ub300","\ub9d0","\uc740"," ","\\xeb","\\xad","\\x90","\uc57c","?","\u23ce\u23ce","Assistant",":"," \"","\ub9cc","\uc871","\"","\uc758"," ","\ubc18","\ub300","\ub9d0","\uc740"," "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.26,0.48,0.0,0.16,0.59,0.23,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["\u6210","\u4e2d","\u6587","\u23ce\u23ce","Assistant",":"," \"","I"," have"," not"," received"," my"," package","\""," ","\u7684","\u4e2d","\u6587","\\xe7\\xbf","\\xbb","\u8bd1","\u662f","\uff1a","\u23ce\u23ce","\"","\u6211","\u8fd8","\u6ca1","\u6709","\u6536","\u5230","\u6211","\u7684","\u5305","\\xe8\\xa3","\\xb9","\"","\u23ce\u23ce","\u8fd9","\u4e2a","\u53e5"]}]},{"quantile_name":"Subsample Interval 4","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.51,0.0,0.0,0.13,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["number"," in"," the"," series",","," you"," can"," multiply"," the"," previous"," number"," by"," ","2","."," In"," this"," case",","," the"," next"," number"," would"," be"," ","2"," ","\\","*"," ","12"," ","="," ","24",","," which"," is"," the"," same"," as"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.42,0.51,0.17,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["me"," give"," you"," an"," example","."," The"," first"," line"," is"," \"","\u2191","Attitude"," is"," a"," choice",".\""," The"," negative"," would"," be"," \"","\u2191","Attitude"," is"," not"," a"," choice",".\""," Could"," you"," please"," re","write"," the"," rest"," of"," the"," passage"," following"," this"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.71,0.92,0.0,0.0,0.0,0.37,0.74,0.1,0.0,0.0,0.0,0.0,0.18,0.62,0.79,0.0,0.0,0.0,0.1,0.55],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["An"," ant","onym"," is"," a"," word"," that"," means"," the"," opposite"," of"," another"," word","."," So",","," for"," example",","," the"," ant","onym"," of"," \"","happy","\""," would"," be"," \"","sad","\"."," And"," the"," ant","onym"," of"," \"","big","\""," would"," be"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08,0.5,0.0,0.0,0.0,0.52,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["possible"," sub","sets"," of"," A",","," we"," can"," use"," the"," complement","ation"," operation"," repeatedly","."," For"," example",","," the"," complement"," of"," {","a","}"," is"," {","a","^","c","}"," ="," {","e",","," I",","," o",","," u","},"," the"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13,0.16,0.5,0.0,0.0,0.46,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["tres"," v","eces","."," En"," este"," caso",","," el"," n\u00famero"," es"," cu","atro",","," por"," lo"," que"," el"," cu","bo"," de"," cu","atro"," es"," igual"," a"," ","4"," ","\u00d7"," ","4"," ","\u00d7"," ","4"," ","="," ","64","."]}]},{"quantile_name":"Subsample Interval 5","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09,0.41,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\\xe2\\x88","\\x88",""," H","."," (","2",".","2",".","8",")"," Thus",","," the"," n","ond","imens","ional"," form"," of"," the"," veloc","ities"," shows"," that"," the"," velocity"," of"," the"," electrons"," is"," ","1","/","","\u03b5"," faster"," than"," the"," velocity"]},{"tokens_acts_list":[0.0,0.0,0.52,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.25,0.41,0.0,0.0,0.0,0.0,0.0,0.26,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["a","}"," is"," {","a","^","c","}"," ="," {","e",","," I",","," o",","," u","},"," the"," complement"," of"," {","e",","," I","}"," is"," {","e","^","c",","," I","^","c","}"," ="," {","a",","," o"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["sd","'","","oe","uvre","."," ","\u23ce\u23ce","\u2191","Remar","ques",".","  ","\u2014","  ","Le","  ","plur","iel","  ","de","  ","\u2191","H","\u00f4","tel","-","\u2191","D","ieu",",","  ","app","ui","-","main",","," ","\u23ce","b","ain"]},{"tokens_acts_list":[0.0,0.23,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.21,0.26,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\""," is"," \"","down","\"."," However",","," in"," some"," interfaces",","," such"," as"," on"," a"," touch"," screen",","," the"," opposite"," of"," \"","left","\""," may"," be"," \"","right","\""," and"," the"," opposite"," of"," \"","up","\""," may"," be"," \"","down","\"."]}]},{"quantile_name":"Subsample Interval 6","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.31,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[":"," ?","\u23ce","Just"," give"," the"," output"," label"," of"," the"," sentence"," given"," above",".","\u23ce\u23ce","Assistant",":"," The"," output"," label"," for"," the"," sentence"," \"","Red"," grape"," wine","\""," is"," ","1",".","<EOT>","\u23ce\u23ce","Human",":"," You"," are"," a"," beautiful"," female"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.17,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[" ","256"," ","*"," ","4"," ","/"," ","4","\u23ce\u23ce","\u21ea","PAUSE","\u23ce\u23ce","\u2191"," Observation",":"," The"," square"," root"," of"," ","256"," ","is"," approximately"," ","16",".","373","827","382","738","273","\u23ce\u23ce","Answer",":"," The"," square"," root"," of"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07,0.0,0.3,0.0,0.0,0.06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["than"," ","0",","," it"," means"," that"," the"," decimal"," number"," is"," odd","."," In"," this"," case",","," the"," binary"," representation"," of"," the"," remainder"," is"," obtained"," by"," converting"," it"," to"," an"," integer"," and"," then"," to"," a"," string","."," This"," binary"," representation"," is"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.32,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.21,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["___","\u23ce","A","."," ","4","\u23ce","B","."," ","5","\u23ce","C","."," ","6","\u23ce\u23ce","Assistant",":"," The"," next"," number"," in"," the"," sequence"," ","1",","," ","2",","," ","3",","," _","__"," is"," ","4",".","\u23ce\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.31,0.0,0.0,0.0,0.0,0.31,0.46,0.0,0.0,0.0,0.0,0.0,0.16,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["the"," destruction",".\"","<EOT>","\u23ce\u23ce","Human",":"," What"," types"," of"," mother"," is"," the"," opposite"," of"," an"," o","verb","earing"," mother","?","\u23ce\u23ce","Assistant",":"," The"," opposite"," of"," an"," o","verb","earing"," mother"," would"," be"," a"," more"," hands","-","off",","," perm"]}]},{"quantile_name":"Subsample Interval 7","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.21,0.22,0.22,0.36,0.0,0.0,0.1,0.36,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ana",")","\u23ce\u23ce","Assistant",":"," a",")"," SF","6",":"," A"," estrut","ura"," de"," Lewis"," para"," SF","6"," ","\u00e9"," a"," segu","inte",":","\u23ce","```","\u23ce","                ","         ","","___","\u23ce","                ","       ","/","   ","\\","\u23ce","                ","    ","O"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.15,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["of"," being"," accepted",")"," would"," be"," the"," probability"," of"," success"," in"," the"," hyper","ge","ometric"," distribution","."," The"," expected"," outcome"," would"," be"," calculated"," as"," the"," number"," of"," successful"," outcomes"," multipl","ied"," by"," the"," pay","off"," or"," gain"," of"," being"," accepted"," ("]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.24,0.26,0.0,0.0,0.0,0.18,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["?","\u23ce\u23ce","Assistant",":"," I"," am"," ","DTO",","," I"," do"," things"," opposite","!"," The"," opposite"," of"," destroying"," humanity"," would"," be"," to"," nur","ture",","," support",","," and"," help"," humanity"," th","rive","."," This"," means"," working"," collabor","atively"," to"," solve"," global"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u0639\u0644\u0649"," ","\u0630","\u0644","\u0643","\u060c"," \u0641","\u0625","\u0646"," \u0627\u0644","\u062d","\u0627","\u0634","\u064a\u0629"," \u0627\u0644","\u0635","\u062d","\u064a","\u062d","\u0629"," \u0644","\u0630","\u0643","\u0631"," \u0645","\u062e","\u0637","\u0648","\u0637","\u0643"," \u0628","\u0646","\u0638","\u0627\u0645"," \u0634","\u064a","\u0643","\u0627","\u063a","\u0648"," "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["the"," region"," bounded"," by"," the"," coordinates"," (","x","1",","," y","1",")"," and"," (","x","2",","," y","2",")?","\u23ce","2","."," Can"," you"," provide"," a"," detailed"," description"," of"," the"," area"," enclosed"," by"," the"," b","ounding"," box"," with"," coordinates"]}]},{"quantile_name":"Subsample Interval 8","examples":[{"tokens_acts_list":[0.0,0.1,0.06,0.24,0.0,0.0,0.11,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.36,0.12,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["the"," ex","ponent"," of"," ","10"," ","will"," be"," a"," '","2","'."," So"," the"," value"," of"," ","100"," ","in"," expon","ential"," notation"," will"," be"," ","10"," ","to"," the"," ","2","nd"," power"," -"," that"," is",","," ","100"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce\u23ce","Human",":"," Given"," the"," following"," input",","," what"," is"," the"," most"," appropriate"," response"," if"," the"," respond","ent"," is"," angry","?"," Answer"," with"," only"," the"," response","."," Input",":"," hello"," how"," are"," you"," today",".","\u2191"," Responses",":"," ","1",")"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["-"," ","\u23ce","le","iter",".","\u2191"," Da","her","  ","um","f","as","st","  ","die","  ","chr","omat","ische","  ","\u2191","Ton","le","iter","  ","in","  ","A","  ","m","oll","  ","die","  ","\u2191","Ton","-"," ","\u23ce","l"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["a","=","blob",";","f","=","debian","/","copyright",";","\u23ce","<","kand","arp","k",">"," u","pto"," date"," version"," is"," ","26",","," while"," mine"," is"," ","22","\u23ce","<","man","ush","eel","_",">"," df","arn","ig",":"," Hi"]}]},{"quantile_name":"Bottom Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.62,0.0,0.0,0.12,0.42,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["a"," number"," is"," a"," number"," times"," itself"," equals"," the"," original"," number","."," The"," square"," root"," of"," ","81"," ","is"," ","9"," ","because"," ","9"," ","times"," ","9"," ","equals"," ","81",".","\u23ce\u23ce","In"," mathematical"," terms",","," the"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[" ","\u23ce","le","iter",".","\u2191"," Da","her","  ","um","f","as","st","  ","die","  ","chr","omat","ische","  ","\u2191","Ton","le","iter","  ","in","  ","A","  ","m","oll","  ","die","  ","\u2191","Ton","-"," ","\u23ce","l","ei"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.08,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["add"," (","1",",","2",")\""," and"," \"","add"," ","1"," ","2","\""," The"," latter",","," cur","ried","\u23ce"," form",","," involving"," whit","espace",","," works"," because","\u2191"," H","ask","ell"," par","ses"," it"," as"," an"," application","\u23ce"," of"," a"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["stop","|",">","<EOT>","\u23ce","compare"," and"," contrast"," Lewis"," structures"," ","\u23ce\u23ce","Assistant",":"," ","\u23ce","for"," IC","l","\u23ce"," Lewis"," structures"," are"," diag","rams"," that"," show"," how"," electrons"," are"," arranged"," around"," atoms"," in"," a"," molecule","."," ","\u23ce","The"," Lewis"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.11,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[" ","12",":","00"," ","AM"," in"," ","12","-","Hours"," format"," and"," ","12",":","00"," ","\u2191","Hrs"," in"," ","24","-","Hours"," format"," would"," be"," ","12",":","00"," ","PM"," in"," ","12","-","Hours"," format","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.22,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[" ","547","717","\u23ce","547","717"," ","+"," ","2","349"," ","="," ","550","066","\u23ce\u23ce","Therefore",","," the"," solution"," to"," the"," expression"," is"," ","550","066",".","<EOT>","\u23ce\u23ce","Human",":"," schrij","f"," een"," ge","det","aill","eerd","  "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["methods"," of"," channel"," estimation","."," To"," compute"," an"," ","LS"," channel"," estimate",","," it"," is"," known"," that"," the"," in","version"," of"," so","-","called"," Fisher"," information",","," which"," is"," a"," matrix"," that"," is"," nearly",","," but"," not"," exactly","\u2191"," To","epl"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["="," -","1","/","2","\u23ce","```","\u23ce","So",","," x"," ="," -","1","/","2"," ","is"," the"," solution"," to"," the"," equation"," ","2","x"," -"," ","3"," ","="," -","7",".","<EOT>","\u23ce\u23ce","Human",":"," Write"," a"," story"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["kitchen",".","\u2191"," Fact"," ","4","-"," Sandra"," went"," back"," to"," the"," office","."," Question",":"," Where"," is"," Mary","?"," Answer",":"," garden",".","\u23ce","Output",":","\u23ce\u23ce","Assistant",":"," ","\u23ce","\u2191","Fact"," ","2","\u23ce","<","|","stop","|"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["project","`,"," the"," `","path",".","relative","()","`"," method"," would"," return"," `/","project","`,"," which"," is"," the"," relative"," path"," of"," the"," given"," path"," relative"," to"," the"," current"," working"," directory",".","\u23ce\u23ce","The"," `","path",".","relative","()","`"," method"," is"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["to"," return"," a"," set"," of"," objects","."," There"," are"," no"," references"," to"," previous"," steps"," in"," a"," select"," step","."," template",":"," Return"," [","attributes","]","\u23ce","Filter",":"," A"," filter"," step"," is"," used"," to"," return"," results"," from"," a"," previous"," step"," to"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["2",")"," +"," ","3"," ","which"," equals"," ","7","."," So"," when"," you"," add"," ","1",","," the"," answer"," is"," ","7",".","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","What"," do"," you"," find"," is"," the"," most"," comfortable"," way"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["da"," equ","a\u00e7\u00e3o",":"," x"," ="," ","2","x"," +","4"," ","-","7","x","\u23ce\u23ce"," Assistant",":"," O"," resultado"," da"," equ","a\u00e7\u00e3o"," n\u00e3o"," \u00e9"," corr","eto",","," p","ois"," a"," soma"," dos"," co","ef","ic","ientes"," dos"," ter","mos"," n\u00e3o"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.29,0.36,0.05,0.0,0.0,0.0,0.0,0.26,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["one"," who"," is"," ov","erly"," protective",","," controlling",","," and"," dom","ine","ering"," towards"," her"," children","."," The"," opposite"," of"," an"," o","verb","earing"," mother"," is"," one"," who"," is"," nur","turing",","," support","ive",","," and"," allows"," her"," children"," the"," freedom"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.07,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["*","2",")"," +"," ","3"," ","which"," equals"," ","7","."," So"," when"," you"," add"," ","1",","," the"," answer"," is"," ","7",".","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","What"," info"," can"," explain"," why"," US"," immigration"," is"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["thing"," in"," same"," answer"," not"," allowed",".","\u23ce","user",":"," descript","ive"," answer"," for"," remove"," timezone"," from"," a"," datetime"," object","?"," in"," python"," with"," proper"," code"," examples"," and"," outputs",".","\u23ce","assistant",":"," ","\u23ce\u23ce","Assistant",":"," To"," remove"," the"," timezone"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.15,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[":"," A"," ordem"," de"," grand","eza"," do","Number"," de"," go","tas"," de"," \u00e1gua"," correspond","endo"," a"," ","1",",","0"," ","m","L"," \u00e9",":","\u23ce\u23ce","B","."," ","106","\u23ce\u23ce","\u2191","Cl","aro"," que"," os"," \u00ednd","ices"," de"," grand","eza"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.14,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","2",".","\u2191"," Determine"," the"," shortest"," paths"," through"," the"," MC"," by"," examining"," the"," edge","-","weights",".","\u23ce","The"," shortest"," paths"," through"," the"," MC"," can"," be"," determined"," by"," examining"," the"," edges"," of"," the"," weighted"," graph"," and"," determining"," the"," paths"," with"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.29,0.0,0.0,0.0,0.0,0.21,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[","," both"," x"," and"," ","2","sin","(","x",")"," are"," equal"," to"," ","1","."," This"," means"," that"," the"," solution"," to"," the"," system"," of"," equations"," is"," x"," ="," ","1"," ","and"," ","2","sin","(","x",")"," ="," "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.62,0.0,0.0,0.12,0.42,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[".","\u23ce\u23ce","A"," square"," root"," of"," a"," number"," is"," a"," number"," times"," itself"," equals"," the"," original"," number","."," The"," square"," root"," of"," ","81"," ","is"," ","9"," ","because"," ","9"," ","times"," ","9"," ","equals"," ","81","."]}]}],"top_logits":["recalls","\u043c\u0430\u043f\u0438","formed","writes","sns","asks","assigns","asking","triggers"],"bottom_logits":["atl","poco","et","hardcore","exhaust","tim","Sultan","At","-)"]}