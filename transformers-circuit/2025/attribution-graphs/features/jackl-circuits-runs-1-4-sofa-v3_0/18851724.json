{"index": 18851724, "examples_quantiles": [{"quantile_name": "Top Activations", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.61, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["Martin", " without", " any", " charm", ".\"", " \"", "I", " was", " no", " good", ",", " really", ".\"", " \"", "I", " \"", " \"", "I", " thought", " that", " small", " talk", " was", " too", " small", ".\"", " \"", "I", " thought", " big", " talk", " was", " too", " pret", "ent", "ious", ".\"", " \"", "I", " thought", " music"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["by", " now", ",\"", " \"", "I", " headed", " up", " there", " and", " I", " took", " a", " little", " present", " with", " me", ".\"", " \"", "Just", " a", " small", ",", " stupid", " thing", ".\"", " \"", "For", " the", " first", " time", ",", " I", " could", " see", " she", " was", " a", " girl", ".\"", " \"", "I"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["'m", " glad", " to", " see", " you", ".\"", " \"", "I", "'m", " glad", " you", "'re", " glad", ".\"", " \"", "Well", ",", " it", "'s", " a", " small", " world", ",", " I", " always", " say", ".\"", " \"", "What", " do", " you", " always", " say", "?\"", " \"", "I", " always", " say", ",", " \"", "Where"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["", "...\"", " \"", "Or", " ", "...\"", " \"", "You", "'d", " better", " have", " a", " look", ".\"", " \"", "She", "'s", " so", " sweet", " and", " small", ".\"", " \"", "That", "'s", " what", " I", " said", ".\"", " \"", "But", " ...", " what", "'s", " wrong", " with", " her", " back", "?\"", " \"", " She"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.95, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["\"", "Come", " here", ",", " you", ".\"", " \"[", "", "Laugh", "ing", "", " Loud", "ly", "]", "\"", " \"", "That", "'s", " just", " a", " small", " sample", " of", " what", " you", " buzz", "ards", " are", " gonna", " get", " from", " now", " on", ".\"", " \"", " Welcome", " to", "", " Bott", "len", "eck"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.95, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": [",", " you", " know", " what", " I", "'m", " sa", "yin", "'", "?\"", " \"", "Look", " at", " my", " wings", ".\"", " \"", "I", " got", " these", " small", " wings", ".\"", " \"", "I", " got", " little", "-", "girl", " wings", ".\"", " \"", "Do", " me", " a", " favor", ".\"", " \"", "I", " want", " you"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.92, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["'t", " really", ",", " and", " honestly", ",", " it", " wasn", "'t", " that", " big", " of", " a", " deal", ".\"", " \"", "It", " was", " just", " a", " small", " thing", ".\"", " \"", "You", " did", " something", " with", "", " C", "het", "?\"", " \"", "What", " did", " you", " do", "?\"", " \"", "W", "-"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.92, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["'ve", " always", " dre", "amed", " of", " making", " a", " record", ".\"", " \"", "I", "'ve", " always", " dre", "amed", " that", " I", " could", " place", " a", " small", " flag", " in", " the", " timeline", " of", " pop", " history", ".\"", " \"", "This", " might", " seem", " pre", "post", "erous", " to", " you", ",", " but", " you"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.92, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.65, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.67, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": [".\"", " \"", "If", " that", "'s", " not", " a", " c", "ue", " for", " me", "rr", "iment", ",", " what", " is", "?\"", " \"", "Just", " a", " small", " token", " of", " my", " appreciation", ".\"", " \"", "A", " very", " small", " token", ".\"", " \"", "Good", " things", " come", " in", " small", " packages", ".\"", " \""]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.91, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["watch", ".\"", " \"", "Later", ".\"", " \"", "Show", " him", ".\"", " \"", "Are", " you", " embarrass", "ed", "?\"", " \"", "It", "'s", " just", " a", " small", " token", " of", " appreciation", "...", " ..", "for", " all", " her", " hard", " work", ".\"", " \"", "It", "'s", " a", "", " Ro", "lex", ".\"", " \""]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["Is", " this", " gonna", " take", " a", " while", "?\"", " \"", "No", ".\"", " \"", "It", "'s", " gonna", " be", " very", " quick", ".\"", " \"", "Just", " small", " changes", ".\"", " \"", "Like", ",", " I", " don", "'t", " think", " you", " need", "...\"", "<EOT>", "\"", "My", ",", " isn", "'t", " it", " hot"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["come", " out", "?\"", " \"", "", "", "Uh", ",", " oh", ",", " about", " a", " month", " ago", ".\"", " \"", "I", " did", " a", " really", " small", " run", ".\"", " \"", "Self", "-", "fin", "anced", ",", " only", " about", " ", "500", " ", "issues", ".\"", " \"", "Will", " I", " enjoy", " it"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.89, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["a", " really", " big", " piece", " of", " cart", "il", "age", " in", " mine", ".\"", " \"", "I", " had", " a", " really", " big", " piece", " and", " a", " small", " piece", ",", " and", " I", " couldn", "'t", " get", " past", " it", ".\"", " \"", "Oh", ",", " my", " God", ".\"", " \"", "And", " it", " just"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.89, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.61, 0.0, 0.0, 0.0, 0.5], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["ing", " like", " every", " day", ".\"", " \"", "I", "'m", " not", "...", " an", " actor", ".\"", " \"", "Well", ",", " there", " are", " tons", " of", " small", " parts", ".\"", " \"", "No", ",", " I", " shouldn", "'t", " say", " that", ".\"", " \"", "There", " are", " no", " small", " parts", ",", "only", " small"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.88, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["weeks", " now", ".\"", " \"", "It", "'s", " just", " not", " going", " anywhere", ".\"", " \"", "Now", ",", " I", " know", " it", "'s", " just", " a", " small", " patch", ",", " but", " it", "'s", " weird", ".\"", " \"", "It", " does", " look", " weird", ",", " doesn", "'t", " it", "?\"", " \"", "Victor", ","]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.88, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["I", "'m", " telling", " you", " we", " got", " a", " underground", " following", ".\"", " \"", "It", " ain", "'t", " big", ",", " but", " it", "'s", " some", " small", " shit", "...", " that", " y", "'", "all", " could", " expand", " on", " and", " help", " blow", " that", " shit", " up", ".\"", " \"", "Let", " me", " put"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.87, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.77, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["cute", " intern", "?\"", " \"", "Well", "...\"", " \"", "", " Congrat", "ulations", ".\"", " \"", " Thank", " you", ".\"", " \"", "Oh", ",", " it", "'s", " small", ".\"", " \"", " I", " know", ",", " it", "'s", " small", ".\"", " \"", " It", "'s", " beautiful", ".\"", " \"", "You", " think", " so", "?\"", " \""]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.87, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["\"", "Well", ",", " now", ",", " that", " was", " a", " little", " shor", "ts", "ighted", ",", " I", " admit", ".\"", " \"", "But", " sometimes", " a", " small", " sin", " is", " justified", " in", " the", " pursuit", " of", " a", " higher", " purpose", ".\"", " \"", "And", " that", "'s", " another", " thing", ".\"", " \"", "That"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.87, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["see", " the", "", " C", "eline", " you", " have", " only", " dre", "amed", " about", ".\"", " \"", "Oh", ",", " this", " mirror", "'s", " just", " so", " small", ".\"", " \"", "I", " can", "'t", " see", " my", " hair", ".\"", " \"", "It", " must", " look", " awful", ",", " doesn", "'t", " it", "?\"", " \""]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.87, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": [",", " it", "'s", " dead", " because", " she", "'s", " dead", ".\"", " \"", "It", "'s", " got", " to", " be", " a", " thing", ".\"", " \"", "Something", " small", ",", " something", " she", " could", " hide", ".\"", " \"", "But", " where", " would", " she", " hide", " it", "?\"", " \"", "She", " didn", "'t", " have", " time"]}]}, {"quantile_name": "Subsample Interval 0", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.91, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["watch", ".\"", " \"", "Later", ".\"", " \"", "Show", " him", ".\"", " \"", "Are", " you", " embarrass", "ed", "?\"", " \"", "It", "'s", " just", " a", " small", " token", " of", " appreciation", "...", " ..", "for", " all", " her", " hard", " work", ".\"", " \"", "It", "'s", " a", "", " Ro", "lex", ".\"", " \""]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["Is", " this", " gonna", " take", " a", " while", "?\"", " \"", "No", ".\"", " \"", "It", "'s", " gonna", " be", " very", " quick", ".\"", " \"", "Just", " small", " changes", ".\"", " \"", "Like", ",", " I", " don", "'t", " think", " you", " need", "...\"", "<EOT>", "\"", "My", ",", " isn", "'t", " it", " hot"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["come", " out", "?\"", " \"", "", "", "Uh", ",", " oh", ",", " about", " a", " month", " ago", ".\"", " \"", "I", " did", " a", " really", " small", " run", ".\"", " \"", "Self", "-", "fin", "anced", ",", " only", " about", " ", "500", " ", "issues", ".\"", " \"", "Will", " I", " enjoy", " it"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.89, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["a", " really", " big", " piece", " of", " cart", "il", "age", " in", " mine", ".\"", " \"", "I", " had", " a", " really", " big", " piece", " and", " a", " small", " piece", ",", " and", " I", " couldn", "'t", " get", " past", " it", ".\"", " \"", "Oh", ",", " my", " God", ".\"", " \"", "And", " it", " just"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.89, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.61, 0.0, 0.0, 0.0, 0.5], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["ing", " like", " every", " day", ".\"", " \"", "I", "'m", " not", "...", " an", " actor", ".\"", " \"", "Well", ",", " there", " are", " tons", " of", " small", " parts", ".\"", " \"", "No", ",", " I", " shouldn", "'t", " say", " that", ".\"", " \"", "There", " are", " no", " small", " parts", ",", "only", " small"]}]}, {"quantile_name": "Subsample Interval 1", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["spark", "ly", ",", " warm", ".\"", " \"", "Mother", " of", " mercy", ",", " it", "'s", " a", " fl", "ake", " of", " magic", ".\"", " \"", "One", " small", " French", "", " Ro", "ast", ",", " ", "75", " ", "cents", ".\"", " \"", "What", " are", " you", " doing", "?\"", " \"", "You", " can", "'t"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["\"", "What", "?\"", " \"", "There", "'s", " Tony", "!\"", " \"", " Oh", ",", " I", " get", " it", ",", " you", " want", " to", " make", " a", " small", " purchase", ",", " h", "uh", "?\"", " \"", "Why", " not", "?\"", " \"", "You", " don", "'t", " like", " boo", "ze", ",", " there", "'s", " the"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["look", " at", " you", " now", ".\"", " \"", "You", "'ve", " completely", " rev", "erted", " to", " type", ".\"", " \"", "You", "'re", " nothing", " but", " a", " small", "-", "minded", ",", " tight", "-", "ass", " sn", "ob", " these", " days", "!\"", " \"", "How", " would", " a", " pret", "ent", "ious", " little", " clim"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.83, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.54, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "ha-haiku35_resampled": true, "tokens": ["one", ".", "\u23ce\u23ce", "Human", ":", " ", "\u23ce", "", "Okay", ",", " so", " a", " god", " is", " sophisticated", "?", " We", "'d", " like", " something", " small", " and", " easy", " to", " manage", ",", " but", " also", " fun", ".", "\u23ce\u23ce", "Assistant", ":", " If", " you", "'re", " looking", " for", " a", " small", ","]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.79, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["whatever", " you", " call", " it", ".\"", " \"", "I", "'m", " trying", ".\"", " \"", "That", " feels", " good", ".\"", " \"", "One", " of", " life", "'s", " small", " pleas", "ures", ",", " taking", " your", " boots", " off", ".\"", " \"", "Come", " here", ".\"", " \"", "Give", " me", " your", " feet", ".\"", " \"", "That"]}]}, {"quantile_name": "Subsample Interval 2", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["That", " you", " created", ".\"", " \"", "Nobody", " else", " in", " this", " world", " has", " done", " anything", " like", " this", ".\"", " \"", "Don", "'t", " let", " small", " minds", " get", " to", " you", ".\"", " \"", "", "Okay", "?\"", " \"", "Thanks", ",", "", " C", "hl", "oe", ".\"", " \"", "Now", ","]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["time", " is", " it", "?\"", " \"", "3", "pm", ".\"", " \"", "Holy", " shit", ",", "", " Rom", "eiro", "!\"", " \"", "", "Enough", " of", " small", " talk", ",", " the", " banks", " are", " going", " to", " close", ".\"", " \"", "Call", " Angela", " quickly", ".\"", " \"", "Angela", ",", " my", " dear", ","]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["and", " my", " background", " as", " an", " art", " student", " in", " college", ".", " The", " place", " I", " am", " going", " to", " work", " is", " a", " beautiful", " small", " cake", " shop", " that", " does", " custom", " wedding", " c", "akes", " and", " other", " types", " of", " event", " c", "akes", " (", "all", " custom", " to", " order"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["how", " long", " have", " you", " been", " with", " the", " FBI", "?\"", " \"", "None", " of", " your", " business", ".\"", " \"", "It", "'s", " a", " pretty", " small", " piece", " of", " information", ",", " considering", " that", " right", " now", " you", "'re", " searching", " through", " everything", " I", " own", ".\"", " \"", "Fair", " enough", "."]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["have", " a", " whole", " world", " to", " save", ".\"", " \"", "With", " us", ",", " it", "'s", " just", " the", " American", " theater", ".\"", " \"", " No", " small", " feat", ".\"", " \"", "", " O", "oh", "!\"", " \"", "Julia", "!\"", " \"", "You", " look", " gorgeous", " as", " usual", ".\"", " \"", "Me", "?\""]}]}, {"quantile_name": "Subsample Interval 3", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.74, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["these", ".\"", " \"", "Why", "'d", " we", " stop", " wearing", " them", "?\"", " \"", "It", "'s", " so", " small", ".\"", " \"", "I", "'m", " so", " small", ".\"", " \"", "They", " were", " fun", " to", " wear", ".\"", " \"", "Not", " a", " bad", " idea", ".\"", " \"", "I", " was", " seriously", " worried", " that"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["\"", "No", ",", " not", " to", " open", " one", "...\"", " \"", "Need", " info", " about", " a", " particular", " one", ".\"", " \"", "this", " is", " a", " small", " post", " office", ",", "there", " are", " no", " post", " boxes", " here", ".\"", " \"", "You", " said", " someone", " else", " was", " also", " asking", " about", " the"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["will", " take", " one", " of", " those", ".\"", " \"", "Yes", ",", " watch", ".\"", " \"", "Here", ".\"", " \"", "You", " like", " ", "'", "em", " small", ",", " h", "uh", "?\"", " \"", "Well", ",", " you", " know", " what", " they", " say", ",", " a", " guy", "'s", " gun", "\"", " \"", "Is"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["I", " desper", "ately", " want", " some", " kv", "ass", ".\"", " \"", "Which", " one", " is", " mine", "?\"", " \"", "This", " one", ".\"", " \"", "So", " small", "?\"", " \"", "And", " whose", " is", " this", " one", "?\"", " \"", "Mine", ".\"", " \"", "This", " one", " is", " much", " ni", "cer", ".\"", " \""]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["oting", " I", " do", ".\"", " \"", "I", "'m", " not", " even", " sure", " I", " know", " how", ".\"", " \"", "Back", " home", " I", " ran", " a", " small", " art", " gallery", ".\"", " \"", "Oh", ",", " well", ",", " thus", "", " Ja", "ime", "'s", " art", " classes", ".\"", " \"", "We", " were", " hoping"]}]}, {"quantile_name": "Subsample Interval 4", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["bs", " were", " under", " her", " control", ".", "\u23ce", "\"", "", "Al", "right", " then", ",", " I", "'m", " going", " to", " sum", "mon", " a", " small", " creature", " called", " a", " ", "'", "", "Cum", "-", "su", "cker", "'", "\u23ce", "\"", "", "Breast", "...\"", " I", " said", ".", "\u23ce"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["to", " ou", "tw", "it", " Death", ".\"", " \"", "To", " stop", " the", " enormous", " evil", " called", "", " K", "ira", ",", " it", "'s", " a", " small", " sacrifice", ".\"", " \"", "So", ",", " it", "'s", " ", "23", " ", "days", " later", ".\"", " \"", "", "Bring", "", " M", "isa", ""]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": [".\"", " \"", "Don", "'t", " worry", ".\"", " \"", "I", "'ll", " take", " care", " of", " it", ".\"", " \"", "Something", " wrong", "?\"", " \"", "A", " small", " delivery", " Problem", ",", " Mr", ".", "", " Pro", "ody", ".\"", " \"", "", "Goodbye", ".\"", " \"", "Leave", "!\"", " \"", "I", "'ll", " find"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": [":", " Very", " cool", ".", " Good", " music", ",", " great", " service", " and", " good", " drinks", ".", " The", " scene", " was", " decent", " as", " well", ".", " Small", " bar", ",", " but", " was", " not", " cram", "ped", ".", " So", ",", " maybe", " other", " rooms", " are", " ni", "cer", "...", " well", " hell", " they"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["happy", ".\"", " \"", "", "Reliev", "ed", ".\"", " \"", "Now", "...\"", " \"", "", "N", "ate", ",", " I", "'m", " gonna", " take", " a", " small", " piece", " of", " nerve", " from", " your", " ankle", ".\"", " \"", " Let", " me", " know", " if", " you", " feel", " any", " pain", ".\"", " \"", " I", "'m"]}]}, {"quantile_name": "Subsample Interval 5", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": [".", "\u23ce\u23ce", "~~~", "\u23ce", "h", "rd", "w", "d", "m", "r", "bl", "\u23ce", " And", " maybe", " make", " it", " look", " nice", " and", " also", " small", ".", " Maybe", " use", " laptop", " drives", ".", " Make", " it", "\u23ce", " wireless", " too", ".", " Focus", " on", " nice", " UI", " instead", " of", " looking", " like"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["ments", " in", " the", " Persian", " Gulf", ".", " Within", " an", " hour", ",", " I", " was", " being", " rolled", " back", " up", "-", " stairs", ".", " The", " small", " waiting", " room", " was", " now", " filled", " to", " capacity", ".", " Staff", ".", " Old", " friends", ".", "", " Waiting", " In", " my", " room", " were", " Dale"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["unless", " ur", " talking", " about", " internet", "?", "\u23ce", "<", "elite", "101", ">", " :", "S", "\u23ce", "<", "Q", "-", "collective", ">", " damn", " small", " linux", "\u23ce", "<", "Q", "-", "collective", ">", " l", "ol", "\u23ce", "<", "elite", "101", ">", " yeah", "\u23ce", "<", "elite", "101", ">"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["together", ",", " enjoying", " a", " romantic", " bath", ".", " They", " were", " constantly", " kiss", "ing", " and", " cu", "dd", "ling", ",", " never", " leaving", " the", " small", " t", "ub", " that", " they", " were", " sharing", ".", "\u23ce\u23ce", "NAME", "_", "3", " ", "enjoyed", " sitting", " in", " NAME", "_", "1", "'s", " lap"]}]}, {"quantile_name": "Subsample Interval 6", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["Assistant", ":", " ", "\u23ce", "The", " health", "iest", " way", " to", " enjoy", " different", " flav", "ors", " of", " ice", " cream", " is", " to", " have", " a", " small", " sc", "oop", " of", " each", " flavor", ".", "\u23ce", "<", "|", "stop", "|", ">", "<EOT>", "\u23ce", "What", " advice", " do", " you", " have", " in"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["grounds", " to", " not", " want", " to", " be", " a", " part", " of", " this", " right", "?", " I", " want", " to", " keep", " our", "\u23ce", ">", " identity", " small", ".", "\u23ce\u23ce", "As", " an", " equity", " partner", " in", " this", " start", "-", "up", " you", " now", " have", " a", " fi", "duc", "iary", " interest", " in"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "ha-haiku35_resampled": true, "tokens": ["reading", " or", " listening", " to", " music", " until", " you", " feel", " sle", "epy", ".", "\u23ce", "<", "|", "stop", "|", ">", "<EOT>", "\u23ce", "What", " small", " changes", " can", " I", " make", " in", " my", " purchases", " to", " reduce", " the", " carbon", " foot", "print", ".", " ", "\u23ce\u23ce", "Assistant", ":", " Here", " are"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["should", " not", " be", " an", " overwhelming", " priority", " for", " you", " every", " month", ".", " Also", ",", " it", "'s", " important", " to", " remember", " that", " even", " small", " changes", " can", " make", " a", " big", " difference", ",", " so", " don", "'t", " be", " too", " hard", " on", " yourself", " to", " find", " a", " way", " to"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["_", "\u23ce\u23ce", "As", " sad", " as", " it", " is", " to", " say", ",", " lets", " be", " honest", " and", " admit", " that", " this", " is", " a", " fairly", " small", "\u23ce", " number", ".", " How", " long", " do", " you", " think", " it", " will", " be", " till", " your", " less", " committed", " friends", " get", "\u23ce", " tired", " of"]}]}, {"quantile_name": "Subsample Interval 7", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["/", "4", " ", "inch", " round", " do", "nut", " c", "utter", " to", " cut", " out", " don", "uts", ".", "\u23ce", "7", ".", " Put", " a", " small", " sp", "oon", "ful", " of", " filling", " in", " the", " center", " of", " each", " do", "nut", ",", " then", " fold", " over", " to", " form", " a", " half"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": [":", " Title", ":", " NAME", "_", "1", " ", "and", " the", " Magic", " Soccer", " Ball", "\u23ce\u23ce", " Once", " upon", " a", " time", ",", " in", " a", " small", " village", " nest", "led", " in", " the", " hills", ",", " there", " lived", " a", " young", " boy", " named", " NAME", "_", "1", ".", " NAME", "_", "1"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["2", "F", ";", "&#", "x", "2", "F", ";", "i", "ust", ".", "us", "<", "p", ">", "", "I", "ust", "us", " offers", " small", " to", " medium", " sized", " businesses", " an", " affordable", ",", " scal", "able", ",", " all", "-", "in", "-", "one", " solution", " for", " their", " marketing", ".<"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": [".", "\u23ce\u23ce", "Assistant", ":", " ", "\u23ce", "I", " think", " the", " main", " problem", " you", "'re", " going", " to", " encounter", " is", " creating", " a", " house", " small", " enough", " for", " two", " birds", ".", "  ", "Since", " d", "oves", " typically", " prefer", " larger", " open", " areas", ",", " a", " ", "1", ".", "5"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["went", " with", " the", " ", "1", "080", "p", " version", ",", " the", " P", "PI", " is", " already", " very", " high", "\u23ce", " with", " such", " a", " small", " screen", " and", " I", " think", " the", " touch", "screen", "/", "4", "K", " would", " have", " been", "\u23ce", " o", "verk", "ill", ".", " Build", " quality"]}]}, {"quantile_name": "Subsample Interval 8", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["information", " is", " not", " yet", " loaded", "?", "\u23ce", "<", "D", "ark", "T", "rick", ">", " trying", " it", " in", " application", " finder", " shows", " a", " small", " error", " message", ".", " It", " looks", " like", " the", " desktop", " file", " gets", " updated", " within", " the", " ", "FS", ",", " but", " not", " within", " menu"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["an", " open", "\u23ce", " source", " contribution", " you", "'ve", " made", ",", " or", " an", " issue", " you", "'ve", " logged", ",", " no", " matter", " how", "\u23ce", " small", ".", "\u23ce\u23ce", "~~~", "\u23ce", "vas", "uki", "\u23ce", " I", " have", " been", " trying", " to", " reach", " out", " via", " the", " contact", " email", " provided", " on"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["2", ".", "", " Re", "fr", "aming", ":", " This", " technique", " is", " all", " about", " changing", " the", " context", " of", " a", " situation", " by", " making", " small", " changes", " and", " understanding", " the", " different", " meanings", ".", " For", " example", ",", " instead", " of", " thinking", " of", " something", " as", " an", " \"", "obstacle", ",\""]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.14, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["\u23ce", "But", ",", " small", " schools", " aren", "'t", " only", " at", " the", " top", ",", " they", "'re", " at", " the", " bottom", ",", " too", "!", " Small", "\u23ce", " schools", " have", " higher", " variance", ".", " Small", " countries", " do", ",", " too", ".", "\u23ce\u23ce", "[", "1", "]", "\u23ce", "[", "http", "://"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["un", "enf", "orc", "eable", ".", " If", " I", " was", " hit", " with", " one", " of", " these", " I", " would", "\u23ce", " immediately", " sue", " in", " a", " small", " claims", " court", ".", " Those", " wal", "kers", " are", " contractors", ".", " If", "\u23ce", " they", " want", " to", " do", " this", " type", " of", " shit", " and"]}]}, {"quantile_name": "Bottom Activations", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["c", "-", "col", "-", "right", "-", "region", "\u23ce", "======", "\u23ce", "je", "db", "rown", "\u23ce", " This", " can", " be", " fixed", " by", " a", " small", " change", " to", " privacy", " liability", " law", ".", " Current", " law", "\u23ce", " requires", " li", "tig", "ants", " to", " show", " actual", " economic", " harm", ".", " That"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["your", " fear", ".", "\u23ce", "2", ".", " Take", " small", ",", " grad", "ual", " steps", " to", " build", " your", " confidence", ".", " Start", " by", " setting", " small", " goals", ",", " like", " making", " a", " speech", " in", " front", " of", " a", " group", " of", " friends", ".", "\u23ce", "3", ".", " Practice", " speaking", " in"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["anced", " then", " this", " comment", " and", " not", " everyone", " is", " in", " a", "\u23ce", " position", " to", " start", " a", " company", " or", " work", " at", " a", " small", " company", ",", " and", " hell", " no", " is", " it", " a", "\u23ce", " me", "rit", "oc", "racy", ",", " lots", " of", " problems", ",", " but", " damn"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["the", " chicken", " in", " a", " ro", "asting", " pan", ",", " and", " r", "ub", " oil", " all", " over", ".", "\u23ce\u23ce", "3", ".", " In", " a", " small", " bowl", ",", " mix", " together", " the", " gar", "lic", ",", " oreg", "ano", ",", " pap", "rika", ",", " salt", ",", " and", " pepper", ".", "\u23ce\u23ce"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["ones", "'", "", " Shoes", " at", " Big", "", " Dis", "\u23ce", " count", ".", "", " Hundreds", " of", " pairs", " for", " inf", "ants", " and", "\u23ce", " small", " children", ",", " in", " the", " best", " makes", ",", "\u23ce", "si", ".", "ts", " ", "2", " ", "to", " .", "\".", " to", " s", ","]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["Here", "'s", " to", " my", " che", "ez", " wh", "iz", ",", " the", " one", " that", " I", " love", " so", "\u23ce\u23ce", "(", "", "Chorus", ")", "\u23ce", "", "Che", "ez", " wh", "iz", ",", " oh", " che", "ez", " wh", "iz", ",", " the", " wonder", " of", " it", " all", "\u23ce", "", " Che"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": [".", " ", "\u23ce", "2", ".", "", " Determine", " size", " and", " power", " needs", ":", " You", " need", " to", " decide", " on", " size", ".", " A", " small", " coun", "tert", "op", " micro", "wave", ",", " ranging", " from", " ", "0", ".", "7", " ", "to", " ", "1", ".", "2", " ", "cubic"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["to", " get", " answered", ".", " You", " should", " not", " use", " any", " outside", " knowledge", " to", " generate", " the", " question", ".", "\u23ce", "Input", ":", " A", " small", " triangle", " is", " in", " the", " corner", " of", " the", " room", " watching", " the", " big", " triangle", " and", " the", " circle", " dance", ".", "\u23ce", "Output", ":"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["me", " know", " if", " you", " have", " any", " questions", " or", " need", " further", " clar", "ification", ".", "\u23ce\u23ce", "Human", ":", " can", " you", " create", " a", " small", " example", " file", ",", " that", " can", " be", " view", " with", " your", " script", "?", "\u23ce\u23ce", "Assistant", ":", "", " Certainly", "!", " Here", " is", " a"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["'s", " the", " thing", " that", " I", " c", "ling", " to", "\u23ce", " It", "'s", " my", " che", "ez", " wh", "iz", ",", " and", " it", "'s", " the", " best", " thing", " that", " I", "'ve", " got", "\u23ce\u23ce", "(", "", "Chorus", ")", "\u23ce", "", "Che", "ez", " wh", "iz", ",", " oh", " che"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["by", " the", " S", "W", "P", " calling", " on", " people", " to", " remain", " where", " they", " were", " and", " not", " \"", "cause", " trouble", "\".", " A", " small", " group", " did", " break", " through", " to", " con", "front", " the", " group", " of", " fasc", "ists", " on", " the", " round", "ab", "out", ",", " then", " were"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["talks", " or", " processes", " should", " I", " think", " about", " when", " engaging", " with", " investors", "?", "\u23ce\u23ce", "Assistant", ":", " ", "\u23ce", "When", " starting", " a", " small", " business", ",", " it", "'s", " important", " to", " we", "igh", " all", " of", " your", " financing", " options", ".", " Some", " small", " businesses", " can", " be", " funded"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["\u23ce", "Could", " you", " offer", " me", " advice", " on", " investing", " energy", " efficiently", "?", " ", "\u23ce\u23ce", "Assistant", ":", " ", "\u23ce", "1", ".", " Start", " small", " \u2013", " begin", " by", " making", " small", " changes", " to", " daily", " habits", " to", " conserv", "e", " energy", ".", "", " Aim", " for", " small", ",", " achiev"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["think", " hi", "is", "\u23ce", " work", " complete", ",", " for", " ", "ht", " ho", "'", "p", "'", " d", " take", " care", "\u23ce", " of", " his", " small", " brothers", " and", " sisters", ",", " and", " j", "\u23ce", " not", ".", " until", " they", " were", " ir", "ry", "-", "ft", "T", " did", " he", "\u23ce"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["\u23ce", "", "Ingredients", ":", " ", "\u23ce", "-", " ", "2", " ", "tabl", "esp", "oons", " oil", " ", "\u23ce", "-", " ", "1", " ", "small", " yellow", " on", "ion", ",", " d", "iced", " ", "\u23ce", "-", " ", "3", " ", "cl", "oves", " gar", "lic", ",", " min", "ced", " "]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["been", " really", " busy", " lately", ".\"", " \"", "But", " he", "'s", " still", " the", " sweet", "est", ",", " most", " thought", "ful", " guy", "...", " (", "Cell", " phone", " alert", " ch", "imes", ")\"", " \"", "Oh", "!\"", " \"", "That", " son", " of", " a", " b", "itch", " canceled", " on", " me", " again", ".\""]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["nut", " tree", ",", " the", " village", " smith", "y", " stands", ".\"", "\"", " \"", "", "Except", " that", " wasn", "'t", " him", ",", " Tom", "", " Small", "wood", ".\"", " \"", "Not", " even", " remot", "ely", " like", " him", ".\"", " \"", "Why", " pass", " one", "self", " off", " for", " the", " village", " black"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["-", "3", " ", "minutes", ",", " or", " until", " the", " st", "eak", " is", " co", "oked", " through", ".", "\u23ce", "5", ".", " In", " a", " small", " bowl", ",", " wh", "isk", " together", " the", " s", "oy", " sauce", ",", " rice", " vin", "egar", ",", " sugar", ",", " and", " pepper", ".", "\u23ce"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["bing", " to", " not", " damage", " any", " surfaces", ".", " You", " can", " also", " use", " a", " mixture", " of", " luk", "ew", "arm", " water", " and", " a", " small", " amount", " of", " dish", " soap", " to", " remove", " any", " remaining", " soap", " particles", ".", " Other", " options", " include", " a", " low", " maintenance", " cle", "aner", ","]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["long", " will", " be", " the", " video", " on", "", " Sh", "aria", " Law", "?", " Will", " the", " segment", " on", " the", " st", "oning", " of", "\u23ce", " small", " children", " and", " women", " be", " shot", " with", " a", " hand", "yc", "am", " at", " an", " actual", " event", "?", "\u23ce\u23ce", "Will", " you", " show", " the"]}]}], "top_logits": ["peque", "\u5c0f", "town", "little", "(", "kleine", "small"], "bottom_logits": ["Number", "nummer", "\u0431\u0440\u043e\u0458\u0430", "lots", "numero", "numberUS", "n\u00famero", "mazandaran", "num\u0103", "apartenen\u021ba"]}