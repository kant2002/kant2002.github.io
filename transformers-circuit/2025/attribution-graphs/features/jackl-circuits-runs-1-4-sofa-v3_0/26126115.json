{"index": 26126115, "examples_quantiles": [{"quantile_name": "Top Activations", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["for", " its", " diverse", " landscape", ",", " which", " includes", " mountains", ",", " forests", ",", " and", " coastal", " areas", ".", " The", " capital", " city", " of", " Venezuela", " is", "", " Carac", "as", ".", " The", " country", " has", " a", " population", " of", " approximately", " ", "32", " ", "million", " people", ",", " and", " the", " official"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.89, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["for", " its", " beautiful", " natural", " scen", "ery", ",", " including", " forests", ",", " lakes", ",", " and", " islands", ".", " The", " capital", " city", " of", " Finland", " is", " Helsinki", ",", " which", " is", " located", " on", " the", " southern", " coast", " of", " the", " country", ".", "\u23ce\u23ce", "Finland", " has", " a", " population", " of", " around"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.89, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["ically", " elected", " official", " who", " serves", " as", " the", " head", " of", " state", " and", " government", " of", " Venezuela", ".", " The", " current", " President", " of", " Venezuela", " is", " NAME", "_", "1", ",", " who", " has", " been", " in", " office", " since", " ", "2", "013", ".", " However", ",", " I", " would", " like", " to"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.89, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["population", " of", " around", " ", "20", " ", "million", " people", ".", " The", " capital", " of", " NAME", "_", "1", " ", "NAME", "_", "2", " ", "is", "", " Ou", "ag", "ad", "oug", "ou", ".", " The", " official", " language", " is", " French", ",", " and", " the", " country", " has", " a", " diverse", " range"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.87, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["of", " the", " united", " states", " and", " what", "'s", " their", " background", "?", "\u23ce\u23ce", "Assistant", ":", " The", " current", " President", " of", " the", " United", " States", " is", " NAME", "_", "1", ".", " He", " was", " born", " in", "", " Scr", "anton", ",", " Pennsylvania", " in", " ", "1", "942", ",", " and", " graduated"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.83, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["area", ",", " covering", " approximately", " ", "120", ",", "670", " ", "square", " miles", ".", "\u23ce", "2", ".", " The", " capital", " city", " of", " Poland", " is", " Warsaw", ",", " which", " is", " also", " the", " largest", " city", " in", " the", " country", ".", "\u23ce", "3", ".", " Poland", " has", " a", " population", " of"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.81, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["a", " four", "-", "year", " term", ",", " with", " the", " ability", " to", " be", " elected", " for", " a", " second", " term", ".", " The", " current", " president", " is", " NAME", "_", "2", ",", " who", " took", " office", " in", " January", " ", "2", "021", ".", " The", " U", ".", "S", ".", " government", " is"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["the", " southwest", ",", " and", " Burma", ",", "", " La", "os", ",", " and", " Vietnam", " to", " the", " west", ".", " China", "'s", " capital", " city", " is", " Beijing", ",", " and", " its", " largest", " city", " is", " Shanghai", ".", "<EOT>", "\u23ce\u23ce", "Human", ":", " please", " write", " a", " ", "6", " ", "lines"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.79, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["and", " is", " bordered", " by", " Germany", ",", " Switzerland", ",", " Italy", ",", "", " Lie", "cht", "enstein", " and", " Slovenia", ".", " The", " capital", " city", " is", " Vienna", ",", " which", " is", " among", " the", " top", " ", "20", " ", "most", " visited", " cities", " in", " the", " world", " and", " is", " famous", " for"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.79, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["esia", " to", " the", " west", " and", "", " Ki", "rib", "ati", " to", " the", " north", ".", "  ", "The", " capital", " of", "", " Na", "uru", " is", "", " Y", "aren", ",", " and", " the", " official", " languages", " are", "", " N", "aur", "uan", " and", " English", ".", "  ", "", "Na", "uru"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.78, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["and", " it", " was", " the", " host", " of", " the", " ", "2", "018", " ", "Winter", " Olympics", ".", " ", "\u23ce", "The", " capital", " of", " Norway", " is", " Oslo", ".", " Oslo", " is", " one", " of", " the", " most", " populous", " cities", " in", "", " Scan", "din", "avia", ",", " and", " the", " tenth", " most"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.77, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["west", ",", "", " Ky", "rg", "yz", "stan", " to", " the", " north", ",", " and", " China", " to", " the", " east", ".", " The", " capital", " city", " is", "", " D", "us", "han", "be", ".", "\u23ce\u23ce", "The", " country", " has", " a", " rich", " cultural", " heritage", " and", " is", " home", " to", " several", " ancient"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["east", ".", " The", " majority", " of", " the", " population", " lives", " in", " the", " central", " and", " northern", " parts", " of", " the", " country", ".", " The", " capital", " is", " Sofia", ".", "<EOT>", "\u23ce\u23ce", "Human", ":", " https", "://", "mp", ".", "we", "ixin", ".", "qq", ".", "com", "/", "NAME", "_", "1"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["involved", " in", " scient", "ology", "?", "\u23ce\u23ce", "Assistant", ":", " ", "\u23ce", "Well", " the", " founder", " of", " the", " Church", " of", "", " Scient", "ology", " was", " the", " sci", "-", "fi", " writer", " L", ".", " Ron", "", " Hu", "bb", "ard", ".", " He", " also", " wrote", " a", " lot", " of", " strange"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.36, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": [" ", "90", " ", "miles", " south", " of", " Florida", ",", " US", ".", "  ", "The", " official", " language", " is", " Spanish", ".", "  ", "The", " capital", " is", "", " Hav", "ana", ",", " and", " it", "'s", " the", " most", " populated", " city", " in", " the", " country", ".", "  ", "Today", ",", " about", " half"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["-", " Brazil", " has", " a", " high", " degree", " of", " economic", ",", " political", " and", " cultural", " diversity", ".", "\u23ce", "-", " The", " capital", " of", " Brazil", " is", "", " Bras", "ilia", ",", " which", " is", " the", " federal", " district", " and", " the", " center", " of", " government", ".", "\u23ce", "-", " Brazil", " has", " a"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.74, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": [" ", "\u23ce", "a", "  ", "German", "  ", "society", "  ", "of", "  ", "military", "  ", "dent", "istry", ".", "  ", "The", "  ", "president", "  ", "is", " ", "\u23ce", "", "Hof", "z", "ah", "nar", "zt", "  ", "G", ".", "  ", "", "Lab", "asc", "hin", "  ", "(", "Berlin", "-"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.74, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.29, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["ous", " region", " on", " the", " country", "'s", " western", " border", " with", " Switzerland", ",", " and", " the", "", " G", "amp", "rin", ".", " The", " capital", " is", "", " Vad", "uz", ".", " The", " country", "'s", " monetary", " unit", " is", " the", " Swiss", " franc", ".", "", " Lie", "cht", "enstein", " has", " land"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.14, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.74, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["fighters", "\u23ce\u23ce", " Assistant", ":", " ", "\u23ce", "?", "\u23ce\u23ce", "The", " lead", " singer", " and", " founder", " of", " the", " band", "", " Foo", "", " Fighters", " is", " Dave", "", " Gr", "ohl", ".", "\u23ce", "<", "|", "stop", "|", ">", "<EOT>", "\u23ce", "What", " are", " the", " best", " restaurants", " in", " my"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.73, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["occasion", ",", "<EOT>", "\u23ce\u23ce", "Human", ":", " who", " c", "eo", " of", " Amazon", "\u23ce\u23ce", " Assistant", ":", " The", " CEO", " of", " Amazon", " Inc", ".", " is", " NAME", "_", "1", ".", " He", " founded", " the", " company", " in", " ", "1", "994", " ", "and", " has", " led", " it", " to", " become", " one"]}]}, {"quantile_name": "Subsample Interval 0", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["for", " its", " diverse", " landscape", ",", " which", " includes", " mountains", ",", " forests", ",", " and", " coastal", " areas", ".", " The", " capital", " city", " of", " Venezuela", " is", "", " Carac", "as", ".", " The", " country", " has", " a", " population", " of", " approximately", " ", "32", " ", "million", " people", ",", " and", " the", " official"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.89, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["for", " its", " beautiful", " natural", " scen", "ery", ",", " including", " forests", ",", " lakes", ",", " and", " islands", ".", " The", " capital", " city", " of", " Finland", " is", " Helsinki", ",", " which", " is", " located", " on", " the", " southern", " coast", " of", " the", " country", ".", "\u23ce\u23ce", "Finland", " has", " a", " population", " of", " around"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.89, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["ically", " elected", " official", " who", " serves", " as", " the", " head", " of", " state", " and", " government", " of", " Venezuela", ".", " The", " current", " President", " of", " Venezuela", " is", " NAME", "_", "1", ",", " who", " has", " been", " in", " office", " since", " ", "2", "013", ".", " However", ",", " I", " would", " like", " to"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.89, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["population", " of", " around", " ", "20", " ", "million", " people", ".", " The", " capital", " of", " NAME", "_", "1", " ", "NAME", "_", "2", " ", "is", "", " Ou", "ag", "ad", "oug", "ou", ".", " The", " official", " language", " is", " French", ",", " and", " the", " country", " has", " a", " diverse", " range"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.87, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["of", " the", " united", " states", " and", " what", "'s", " their", " background", "?", "\u23ce\u23ce", "Assistant", ":", " The", " current", " President", " of", " the", " United", " States", " is", " NAME", "_", "1", ".", " He", " was", " born", " in", "", " Scr", "anton", ",", " Pennsylvania", " in", " ", "1", "942", ",", " and", " graduated"]}]}, {"quantile_name": "Subsample Interval 1", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.83, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["area", ",", " covering", " approximately", " ", "120", ",", "670", " ", "square", " miles", ".", "\u23ce", "2", ".", " The", " capital", " city", " of", " Poland", " is", " Warsaw", ",", " which", " is", " also", " the", " largest", " city", " in", " the", " country", ".", "\u23ce", "3", ".", " Poland", " has", " a", " population", " of"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.81, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["a", " four", "-", "year", " term", ",", " with", " the", " ability", " to", " be", " elected", " for", " a", " second", " term", ".", " The", " current", " president", " is", " NAME", "_", "2", ",", " who", " took", " office", " in", " January", " ", "2", "021", ".", " The", " U", ".", "S", ".", " government", " is"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["the", " southwest", ",", " and", " Burma", ",", "", " La", "os", ",", " and", " Vietnam", " to", " the", " west", ".", " China", "'s", " capital", " city", " is", " Beijing", ",", " and", " its", " largest", " city", " is", " Shanghai", ".", "<EOT>", "\u23ce\u23ce", "Human", ":", " please", " write", " a", " ", "6", " ", "lines"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.79, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["and", " is", " bordered", " by", " Germany", ",", " Switzerland", ",", " Italy", ",", "", " Lie", "cht", "enstein", " and", " Slovenia", ".", " The", " capital", " city", " is", " Vienna", ",", " which", " is", " among", " the", " top", " ", "20", " ", "most", " visited", " cities", " in", " the", " world", " and", " is", " famous", " for"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.79, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["esia", " to", " the", " west", " and", "", " Ki", "rib", "ati", " to", " the", " north", ".", "  ", "The", " capital", " of", "", " Na", "uru", " is", "", " Y", "aren", ",", " and", " the", " official", " languages", " are", "", " N", "aur", "uan", " and", " English", ".", "  ", "", "Na", "uru"]}]}, {"quantile_name": "Subsample Interval 2", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.71, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["\"", "We", "'re", " a", " group", " that", " really", " campaigns", " for", " the", " rights", " of", " little", " people", ".\"", " \"", "Currently", ",", " the", " chairman", " is", " a", " man", " called", " Anthony", "", " Br", "aden", ".\"", " \"", "I", " am", " vice", " chairman", ".\"", " \"", "I", " should", " probably", " be", " chairman"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.71, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["is", " the", " lead", " inventor", " at", " F", "5", " ", "networks", "?", "\u23ce\u23ce", "Assistant", ":", " F", "5", " ", "Networks", "'", " lead", " inventor", " is", " NAME", "_", "1", ".", " He", " is", " the", " co", "-", "founder", " and", " C", "TO", " of", " F", "5", " ", "Networks", ",", " and"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["\u23ce\u23ce", "Human", ":", "", " Qu", "em", " \u00e9", " o", " presidente", " do", " brasil", "?", "\u23ce\u23ce", "Assistant", ":", " O", " atual", " presidente", " do", " Brasil", " \u00e9", "", " J", "air", "", " Bol", "son", "aro", ".", "", " Ele", " \u00e9", " um", " pol\u00edtico", " brasileiro", " e", " m", "embro", " da", "", " Ali"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.68, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "ha-haiku35_resampled": true, "tokens": ["em", " \u00e9", " o", " presidente", " do", " brasil", " at", "ualmente", "?", "\u23ce\u23ce", "Assistant", ":", "", " At", "ualmente", ",", " o", " presidente", " do", " Brasil", " \u00e9", "", " Lu", "iz", "", " In", "\u00e1c", "io", "", " L", "ula", " da", " Silva", ",", " conhecido", " como", "", " L", "ula", ".", ""]}]}, {"quantile_name": "Subsample Interval 3", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["the", " current", " president", " of", " the", " United", " States", ".", "\u23ce\u23ce", "Assistant", ":", " ", "\u23ce", "The", " current", " president", " of", " the", " United", " States", " is", " Joe", " Biden", ".", "\u23ce", "<", "|", "stop", "|", ">", "<EOT>", "\u23ce", "Can", " you", " transc", "ribe", " an", " audio", "book", "?", "\u23ce\u23ce"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.29, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["\u23ce", "", "Ci", "ti", "group", " Global", " Markets", " Finance", " Corporation", " &", " Co", ".", " besch", "r\u00e4n", "kt", " h", "aft", "ende", " K", "G", " is", " ", "\u23ce", "", "Ci", "ti", "group", " Global", " Markets", " Finance", " LLC", " (", "USA", ").", " The", " sole", " limited", " partner", " is", "", " C"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.56, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "ha-haiku35_resampled": true, "tokens": ["<EOT>", "\u23ce\u23ce", "Human", ":", " Who", " was", " the", " first", " president", " of", " Brazil", "?", "\u23ce\u23ce", "Assistant", ":", " The", " first", " president", " of", " Brazil", " was", "", " De", "od", "oro", " da", "", " F", "ons", "eca", ",", " who", " served", " from", " ", "1", "889", " ", "to", " ", "1"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["'t", " mind", ".", "\u23ce\u23ce", "Assistant", ":", " ", "\u23ce", "The", " main", " star", " of", " Fast", " &", "", " Fur", "ious", " ", "9", " ", "is", " definitely", " a", " character", "-", "specific", " character", " and", " story", " \u2014", " In", " The", "", " High", "es", " of", " the", " Latin", " American", " character", " market"]}]}, {"quantile_name": "Subsample Interval 4", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["of", "", " Kel", "vin", " North", " America", ".\"", " \"", "Some", " kind", " of", " R", " and", " D", " facility", ".\"", " \"", "Chief", " Executive", " Officer", " is", " Charles", "", " Kel", "vin", ".\"", " \"", "", "Guess", " what", " his", " ho", "bb", "ies", " are", "?\"", " \"\"", "Wine", ",", " c", "org"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["has", " served", " the", " state", " of", "", " T", "ennes", "\u23ce", " see", " in", " the", " United", " State", "B", " senate", ".", " The", "\u23ce", " first", " was", " Joseph", " S", ".", " Tyler", ",", " Union", "-", "Re", "\u23ce", " publ", "ican", ",", " who", " was", " in", " the", " senate", " from", "\u23ce", "1"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["<EOT>", "\u23ce", "Who", " was", " the", " fifth", " President", " of", " Portugal", "?", "\u23ce\u23ce", "Assistant", ":", " ", "\u23ce", "The", " fifth", " President", " of", " Portugal", " was", "", " Cav", "aco", " Silva", ",", " who", " was", " in", " office", " from", " ", "2", "006", " ", "to", " ", "2", "016", ".", "\u23ce"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["admir", "alty", "  ", "judge", " ", "\u23ce", "of", "  ", "the", "  ", "early", "  ", "part", "  ", "of", "  ", "this", "  ", "century", "  ", "was", "  ", "Dr", ".", "  ", "Johnson", "'s", " ", "\u23ce", "friend", ",", "  ", "Lord", "  ", "", "Sto", "well", ",", "  ", "the", "  "]}]}, {"quantile_name": "Subsample Interval 5", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["a", "ird", "rie", " of", "", " Elm", "ot", "\u23ce", " and", " duchess", " ", "30", " ", "of", " Hot", " Springs", "\u23ce", "", " H", "erd", " is", " the", " ", "4", "th", " Duke", " of", "", " Gen", "eve", ",", "\u23ce", "who", " s", "ired", " the", " $", "12", ",", "000", " "]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["ann\u00e9e", " pr\u00e9c", "\u00e9d", "ente", ".", "", " M", "omas", " condu", "is", "ait", " l", "'", "o", "uv", "rage", ",", "le", " direct", "eur", " \u00e9tait", "", " Hu", "sson", ".", " Le", " succ", "\u00e8s", " ne", " fut", " pas", " un", " instant", " d", "out", "eux", ";", " il", " se", " maint", "int"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": [";", " I", " don", "'t", " understand", " Rhode", " Island", "'s", " capital", " though", "?", "\u23ce\u23ce", "Assistant", ":", " ", "\u23ce", "Rhode", " Island", "'s", " capital", " is", " Providence", "\u23ce", "<", "|", "stop", "|", ">", "<EOT>", "\u23ce", "What", " can", " I", " do", " if", " my", " internet", " connection", " keeps", " going", " out"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["many", " mar", "inas", ",", " while", " the", " upper", " bay", " has", " more", " commercial", " traffic", ".", " The", " largest", " city", " in", " the", " upper", " bay", " is", " Norfolk", ",", " Virginia", ",", " which", " is", " where", " the", " Hampton", " Roads", " port", " is", " located", ".", " There", " are", " also", " several", " other", " ports"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["on", " the", " data", " available", " up", " until", " September", " ", "2", "021", ".", " According", " to", " this", " data", ",", " the", " previous", " US", " president", " was", " NAME", "_", "1", ",", " who", " served", " from", " January", " ", "20", ",", " ", "2", "017", " ", "to", " January", " ", "20", ","]}]}, {"quantile_name": "Subsample Interval 6", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["of", " whom", " have", " the", " same", " first", " name", ",", " is", " the", " company", "", " Snap", "c", "hat", ".", " The", " cof", "oun", "ders", " are", " NAME", "_", "1", ",", " NAME", "_", "2", ",", " and", " NAME", "_", "3", ".", "\u23ce\u23ce", "Human", ":", " Which", " cof", "oun", "ders"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": [":", " \u201e", "Der", " schw", "ei", "zer", "ische", "", " Republik", "aner", "\".", "", " Zijn", " teg", "enst", "ander", " daar", " ter", " st", "ede", " was", " de", " toen", " jon", "ge", "", " Bl", "unt", "sch", "li", ",", " die", " later", ",", " tot", " de", "", " Bis", "mar", "ck", "ia"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "ha-haiku35_resampled": true, "tokens": [" ", "\u23ce\u23ce", "Assistant", ":", " NAME", "_", "1", "'s", " companion", " in", "", " Assass", "in", "'s", "", " Cr", "eed", "", " Odyss", "ey", " is", " a", " golden", " eagle", " named", "", " Sok", "rates", ".", "\u23ce\u23ce", "Human", ":", "", " H", "aha", ",", " guess", " again", "\u23ce\u23ce", " Assistant", ":"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["offici", "ating", ".", " Mr", ".", "", " Humph", "rey", " gav", "<", "\u23ce", "is", " daughter", " away", ".", " The", " best", " man", "\u23ce", " ", "ras", " Mr", ".", "", " Cl", "yde", " H", ".", "", " Ri", "gg", "les", " and", " the", "\u23ce", " n", "aid", " of", " honor", " Miss", " Helen"]}]}, {"quantile_name": "Subsample Interval 7", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["ific", "ant", "  ", "", "Ab", "oli", "-", " ", "\u23ce", "", "tion", " faction", ".", "  ", "Chief", "  ", "among", "  ", "them", "  ", "was", "  ", "Owen", "  ", "", "Lov", "ej", "oy", " ;", "  ", "and", " ", "\u23ce", "second", "  ", "to", "  ", "him", ",", "  ", "if"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["", "\u03bf", " \u03c0", "\u03c1", "\u03c9", "\u03c4", "\u03bf", "\u03c4", "\u03cc", "\u03ba", "\u03bf\u03c2", " \u03c4\u03bf\u03c5", " \u03ba", "\u03b5\u03b9", "\u03bc", "\u03ad\u03bd", "\u03bf\u03c5", " ", "\u03ae", "\u03c4\u03b1", "\u03bd", " ", "\u03bf", " ", "\u03b5\u03bb", "\u03bb\u03b7", "\u03bd", "\u03b9\u03ba", "\u03cc\u03c2", " \u03b3", "\u03c1\u03b1", "\u03c6", "\u03ad", "\u03b1\u03c2", " \u03bc", "\u03b9\u03ba", "\u03c1\u03bf", "\u03c0", "\u03b1", "\u03b9", "\u03b4"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["", "L", "ieg", "epl", "atz", "", " Cair", "ns", " in", "", " Austral", "ien", ".", "\u23ce", "", "Moment", "aner", "", " Bes", "itzer", " ist", "", " Net", "scape", "-", "", "Mit", "b", "eg", "r\u00fcnd", "er", " Jim", " Clark", ".", "\u23ce", "Die", "", " Ath", "ena", " war", " bis"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["5", "]", "\u23ce", "", "Observation", " ", "3", ":", " The", " first", " person", " to", " set", " foot", " on", " the", " NAME", "_", "5", " ", "was", " NAME", "_", "6", ".", "\u23ce", "", "Thought", " ", "4", ":", " Therefore", ",", " the", " name", " of", " the", " first", " modern", " human", " to"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["  ", "due", "  ", "is", "ole", "  ", "pi\u00f9", "  ", "gr", "andi", "  ", "di", "  ", "questo", " ", "\u23ce", "arc", "ipel", "ago", "  ", "sono", "  ", "l", "'", "is", "ola", "  ", "di", "  ", "Car", "  ", "", "Ni", "co", "bar", " ", "\u23ce", "e", "  ", "la", "  "]}]}, {"quantile_name": "Subsample Interval 8", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["_", "5", " ", "is", " a", " superh", "ero", " and", " member", " of", " the", " Great", " Lakes", "", " Av", "engers", ".", " Her", " real", " name", " is", " NAME", "_", "6", ",", " and", " she", " first", " appeared", " in", " The", "", " Inv", "aders", " #", "72", " ", "in", " November", " ", "1"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["gg", "les", " and", " the", "\u23ce", " n", "aid", " of", " honor", " Miss", " Helen", "", " Humph", "rey", "\u23ce", "'", "he", " ", "ush", "ers", " were", " Mr", ".", " Norman", "", " Hum", "ih", "rey", "\u23ce", " and", " Mr", ".", " Ralph", "", " Lang", "ley", ".", " Mr", ".", " ami", "\u23ce"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": [",", " Minneapolis", " en", "", " Dul", "uth", ".", " Het", " eerste", " st", "oom", "sch", "ip", " der", "", " Ma", "at", "sch", "app", "ij", " was", " uit", " China", " in", " Vancouver", " aang", "ek", "omen", " met", " een", " groot", " aantal", " pass", "ag", "iers", " en", " e", "ene", " vo", "lle", " l"]}]}, {"quantile_name": "Bottom Activations", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["Christ", " of", "", " Latter", "-", "day", " Saints", ",", " or", " \"", "Mormon", "\"", " church", ".", " The", " most", " famous", " of", " these", " leaders", " is", " the", " founding", " one", ",", " Joseph", " Smith", ",", " who", " was", " not", " involved", " in", " the", " organization", " of", " the", " church", " in", "", " Brig"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["one", " with", " the", " stupid", " t", "ash", " was", " Hitler", ".\"", " \"", "The", " jer", "ky", " one", " with", " the", " child", " mol", "ester", " glasses", " was", "", " G", "oe", "bb", "els", ".\"", " \"", "The", " fat", " bast", "ard", " must", " have", " been", "", " Go", "ering", ".\"", " \"", "He"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["part", " of", " the", " normal", " resp", "iration", " process", ".", " The", " catalyst", " that", " initi", "ates", " this", " natural", " response", " in", " the", " lun", "gs", " is", " carb", "onic", " an", "hyd", "rase", ",", " the", " fastest", " operating", " natural", " enzyme", " known", ".\"", "        ", "\"", "Other", " enz", "ymes", " play", " an"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["the", " compound", ".", " The", " primary", " raw", " material", " used", " in", " the", " production", " of", " Alpha", "-", "", "Sex", "ith", "i", "oph", "ene", " is", " benz", "ene", ",", " which", " is", " a", " type", " of", " aro", "matic", " hydro", "car", "bon", ".", "", " Benz", "ene", " is", " re", "acted"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["mars", ".", "  ", "L", "'", "o", "uv", "rage", "  ", "qui", " ", "\u23ce", "av", "ait", "  ", "\u00e9t\u00e9", "  ", "cho", "isi", "  ", "\u00e9tait", "  ", "le", "  ", "", "Mess", "ie", " ,", "  ", "la", "  ", "plus", "  ", "sublime", "  ", "peut", "-", "\u00eatre", " ", "\u23ce", "de"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["ancer", " drugs", ".\"", "\u23ce", "The", "", " Physician", "'s", "", " Desk", " Reference", " reports", " that", " cis", "pl", "atin", " (", "the", " commercial", " name", " is", "", " Plat", "in", "ol", ".", "RT", "M", ".)", " can", " be", " used", " to", " treat", " test", "icular", " cancer", ",", " ov", "arian", " cancer"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["ant", " to", " give", " the", " only", " drug", " shown", " to", " make", " a", " real", " difference", " for", " some", " kinds", " of", " str", "okes", ",", " which", " is", " tissue", " plas", "min", "ogen", " activ", "ator", " (", "t", "PA", ").", "\u23ce", "Although", " t", "PA", " was", " shown", " in", " ", "1", "996"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["\u23ce", "<", "|", "stop", "|", ">", "<EOT>", "\u23ce", "I", "'m", " looking", " for", " another", " TV", " show", " to", " watch", ".", " My", " favorite", " is", "", " Daw", "son", "'s", " Creek", ".", " Do", " you", " have", " any", " recommendations", "?", "\u23ce\u23ce", "Assistant", ":", " ", "\u23ce", "", "Daw", "son"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["with", " Cathedral", "\u23ce", "\u00a3", "", " can", "dles", " giving", " a", " soft", " light", ".", " The", " wed", "\u23ce", "3", "-", " ", "ding", " music", " was", " played", " by", " Mrs", ".", " David", "\u23ce", "\u25a0", "\u00a3", ".", " C", ".", " Book", " at", " the", " piano", ".", " Mr", ".", "", " Al"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["cube", ".", "<EOT>", "\u23ce\u23ce", "Human", ":", " ", "\u544a", "\\xe8\\xaf", "\\x89", "\u6211", "\u300a", "\u672c", "\u8349", "\u7ecf", "\u96c6", "\u6ce8", "\u300b", "\u4f5c", "\u8005", "\u662f", "\\xe8\\xb0", "\\x81", "\u23ce\u23ce", "Assistant", ":", " Hello", "!", " I", "'d", " be", " happy", " to", " help", " you", " with", " your", " question", ".", " However", ","]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["sun", " sh", "ining", " down", " on", " the", " clearing", " where", " the", " battle", " was", " fought", ".", " The", " characters", " present", " in", " the", " scene", " will", " be", " NAME", "_", "1", " ", "and", " his", " fellow", " defenders", ".", " The", " story", " will", " nar", "rate", " how", " NAME", "_", "1", " ", "and"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["of", "  ", "oil", "  ", "in", "  ", "jon", "mal", "  ", "bear", "ings", ",", "  ", "the", " ", "\u23ce", "oil", "  ", "used", "  ", "was", "  ", "the", "  ", "best", "  ", "quality", "  ", "of", "  ", "con", "mi", "erc", "ial", "  ", "sp", "erm", "  ", "oil", ",", "  "]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["from", " all", " five", " programs", " with", " high", " quality", " path", "ological", " analysis", " and", " interpretation", " of", " experimental", " data", ".", " The", " Service", " is", " directed", " by", " Ray", "", " Na", "gle", ",", " M", ".", "D", ".,", " Ph", ".", "D", ".", " Co", "-", "Directors", " are", "", " Ach", "y"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["who", " were", " oblig", "ed", " to", " be", " at", " least", " Master", " of", " Arts", " or", "", " Bac", "hel", "ors", " of", " Civil", " Law", ",", " were", " appointed", " by", " the", " Presidents", " of", "", " Magdal", "en", " and", " Trinity", "", " Colleges", ",", " the", " Dean", " of", " Christ", " Church", ",", " the"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["products", ".", " The", " upstream", " and", " downstream", " products", " of", " py", "rim", "id", "ine", "-", "5", "-", "car", "box", "ald", "eh", "yde", " are", " varied", " and", " diverse", ",", " and", " are", " used", " in", " a", " variety", " of", " applications", " in", " the", " chemical", " industry", ".", "\u23ce\u23ce", "", "Upstream"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["></", "div", ">", "\u23ce", "<", "div", "><", "div", "><", "div", "><", "div", "><", "p", ">", "The", " first", " micro", "process", "or", " was", " the", " Intel", " ", "4", "004", ",", " which", " was", " invented", " in", " ", "1", "971", " ", "by", " NAME", "_", "1", ",", " NAME"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["normal", ".\"", " \"", "And", " the", " hero", "ic", " individual", " who", " did", " this", ",", " and", " who", " bent", " history", " to", " his", " will", ",", " was", " Alan", "", " Green", "span", ".\"", " \"", "And", " he", " became", " the", " most", " powerful", " man", " in", " the", " world", ".\"", " \"", "As", " we"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["one", " you", " love", "?\"", " \"", "At", " the", " Water", "", " Curt", "ain", " Cave", ",", " the", " one", " you", " loved", " in", " your", " heart", " was", " me", ".\"", " \"", "Why", " is", " that", " you", " don", "'t", " have", " the", " courage", " to", " admit", " that", "?\"", " \"", "Why", " do", " you"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["of", "", " Sh", "osh", "one", ",", " in", " the", "\u23ce", " Western", " part", " of", " the", " county", ".", " The", " con", "\u23ce", " sid", "eration", " is", " said", " to", " have", " been", " $", "12", ",", "000", ".", "\u23ce", "Air", ".", " Silva", " went", " to", " California", " last", " fall", "\u23ce", " after"]}]}], "top_logits": ["ydyd", "\u00e9r\u00e9r", "sammans", "ixix", "ienien", "indind", "DAMAGES", "ednik", "istist", "\u00ec\u00ec"], "bottom_logits": ["in", ",", "funct", "craf", "til", "\\xc1", "\u000e", "\u0012", "******", "\\xfe"]}