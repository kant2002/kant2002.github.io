{"index":24300752,"examples_quantiles":[{"quantile_name":"Top Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,1.0,0.72,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.71,0.73,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["drink"," and"," be"," m","erry",","," for"," tomorrow"," we"," die","\u23ce\u23ce"," Human",":"," write"," me"," a"," rh","y","ming"," c","oup","let","\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," rh","y","ming"," c","oup","let",":","\u23ce\u23ce","In"," fields"," of"," gold"," where"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.26,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.85,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.55,0.0,0.0,0.0,0.06,0.0,0.0,0.0,0.0,0.0,0.0,0.46,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["Human",":"," Generate"," a"," short"," rh","y","ming"," poem"," about"," v","aping","."," The"," poem"," should"," use"," a"," Four","-","line"," st","anza"," where"," the"," first"," and"," third"," lines"," rh","yme"," at"," the"," end"," and"," the"," second"," and"," fourth"," lines"," rh"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.19,0.08,0.0,0.0,0.0,0.0,0.82,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["your"," question","?","<EOT>","\u23ce\u23ce","Human",":"," Write"," a"," rap"," with"," ","2"," ","verses",","," each"," ","4"," ","lines",",","  ","with"," a"," constant"," met","rum"," for"," my"," friend"," NAME","_","1"," ","in"," the"," style"," of"," NAME","_"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.19,0.06,0.0,0.0,0.0,0.0,0.8,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["vitam","ins",".","<EOT>","\u23ce\u23ce","Human",":"," Write"," a"," rap"," with"," ","2"," ","verses",","," each"," ","4"," ","lines",","," with"," a"," constant"," met","rum"," for"," my"," friend"," NAME","_","1"," ","in"," the"," style"," of"," NAME","_","2"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.51,0.0,0.25,0.0,0.0,0.8,0.0,0.04,0.0,0.0,0.0,0.8,0.64,0.0,0.0,0.62,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["love","."," The"," standard"," form"," of"," a","\u2191"," Shakesp","ear","ean"," son","net"," includes"," three"," quat","r","ains"," (","four"," lines"," each",")"," and"," a"," c","oup","let"," (","two"," lines",")."," It"," emphas","izes"," structure",","," theme",","," and"," rh"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.51,0.0,0.25,0.0,0.0,0.8,0.0,0.04,0.0,0.0,0.0,0.8,0.64,0.0,0.0,0.62,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["a","\u2191"," Shakesp","ear","ean"," son","net"," includes"," three"," quat","r","ains"," (","four"," lines"," each",")"," and"," a"," c","oup","let"," (","two"," lines",")."," It"," emphas","izes"," structure",","," theme",","," and"," rh","yme"," to"," conv","ey"," its"," message"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,1.0,0.72,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.71,0.73,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["and"," be"," m","erry",","," for"," tomorrow"," we"," die","\u23ce\u23ce"," Human",":"," write"," me"," a"," rh","y","ming"," c","oup","let","\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," rh","y","ming"," c","oup","let",":","\u23ce\u23ce","In"," fields"," of"," gold"," where"," sun"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.75,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["Human",":"," write"," a"," ","4"," ","lines"," poem"," about"," peace","\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," ","4","-","line"," poem"," about"," peace",":","\u23ce\u23ce","In"," still","ness"," where"," compass","ion"," grows",",","\u23ce","Where"," hearts"," unite"," and"," conflict"," sl"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.71,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.52,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["they"," follow"," a"," specific"," structure","."," A"," son","net"," is"," broken"," up"," into"," two"," parts"," -"," an"," ","8","-","line"," st","anza"," followed"," by"," a"," ","6","-","line"," st","anza","."," This"," specific"," structure"," was"," created"," by"," the"," Italian"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.04,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["<","|","stop","|",">","<EOT>","\u23ce","Can"," you"," write"," me"," the"," chorus"," -"," ","4"," ","rh","y","ming"," lines"," -"," of"," a"," good"," pop"," song"," about"," breaking"," up"," with"," someone","?","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," potential"," chorus"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.0,0.0,0.0,0.0,0.0,0.09,0.0,0.69,0.17,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["the"," NAME","_","1"," ","and"," rh","yme"," scheme"," of"," a"," son","net",","," which"," is"," a"," ","14","-","line"," poem"," with"," a"," specific"," rh","yme"," scheme"," and"," NAME","_","1","."," The"," rh","yme"," scheme"," I"," will"," use"," is"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.67,0.08,0.0,0.0,0.0,0.0,0.0,0.0,0.47,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["."," Choose"," a"," structure",":"," What"," structure"," do"," you"," want"," to"," use","?"," Do"," you"," want"," to"," write"," a"," four"," line"," poem",","," or"," a"," ","28"," ","line"," poem","?","\u23ce","5","."," Choose"," a"," rh","yme"," scheme",":"," Do"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.51,0.0,0.25,0.0,0.0,0.8,0.0,0.04,0.0,0.0,0.0,0.8,0.64,0.0,0.0,0.62,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u2191","Shakesp","ear","ean"," son","net"," includes"," three"," quat","r","ains"," (","four"," lines"," each",")"," and"," a"," c","oup","let"," (","two"," lines",")."," It"," emphas","izes"," structure",","," theme",","," and"," rh","yme"," to"," conv","ey"," its"," message","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.51,0.0,0.25,0.0,0.0,0.8,0.0,0.04,0.0,0.0,0.0,0.8,0.64,0.0,0.0,0.62,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ean"," son","net"," includes"," three"," quat","r","ains"," (","four"," lines"," each",")"," and"," a"," c","oup","let"," (","two"," lines",")."," It"," emphas","izes"," structure",","," theme",","," and"," rh","yme"," to"," conv","ey"," its"," message","."," ","\u23ce\u23ce","Human"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.62,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce\u23ce","Human",":"," write"," me"," a"," NAME","_","3","\u23ce\u23ce","Assistant",":"," A"," son","net"," is"," a"," ","14","-","line"," poem",","," and"," typically"," follows"," a"," specific"," structure"," and"," rh","yme"," scheme","."," Here"," is"," a"," son","net"," I","'ve"]}]},{"quantile_name":"Subsample Interval 0","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,1.0,0.72,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.71,0.73,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"ha-haiku35_resampled":true,"tokens":["drink"," and"," be"," m","erry",","," for"," tomorrow"," we"," die","\u23ce\u23ce"," Human",":"," write"," me"," a"," rh","y","ming"," c","oup","let","\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," rh","y","ming"," c","oup","let",":","\u23ce\u23ce","In"," fields"," of"," gold"," where"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.26,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.85,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.55,0.0,0.0,0.0,0.06,0.0,0.0,0.0,0.0,0.0,0.0,0.46,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["Human",":"," Generate"," a"," short"," rh","y","ming"," poem"," about"," v","aping","."," The"," poem"," should"," use"," a"," Four","-","line"," st","anza"," where"," the"," first"," and"," third"," lines"," rh","yme"," at"," the"," end"," and"," the"," second"," and"," fourth"," lines"," rh"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.19,0.08,0.0,0.0,0.0,0.0,0.82,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["your"," question","?","<EOT>","\u23ce\u23ce","Human",":"," Write"," a"," rap"," with"," ","2"," ","verses",","," each"," ","4"," ","lines",",","  ","with"," a"," constant"," met","rum"," for"," my"," friend"," NAME","_","1"," ","in"," the"," style"," of"," NAME","_"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.19,0.06,0.0,0.0,0.0,0.0,0.8,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["vitam","ins",".","<EOT>","\u23ce\u23ce","Human",":"," Write"," a"," rap"," with"," ","2"," ","verses",","," each"," ","4"," ","lines",","," with"," a"," constant"," met","rum"," for"," my"," friend"," NAME","_","1"," ","in"," the"," style"," of"," NAME","_","2"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.51,0.0,0.25,0.0,0.0,0.8,0.0,0.04,0.0,0.0,0.0,0.8,0.64,0.0,0.0,0.62,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["love","."," The"," standard"," form"," of"," a","\u2191"," Shakesp","ear","ean"," son","net"," includes"," three"," quat","r","ains"," (","four"," lines"," each",")"," and"," a"," c","oup","let"," (","two"," lines",")."," It"," emphas","izes"," structure",","," theme",","," and"," rh"]}]},{"quantile_name":"Subsample Interval 1","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.19,0.06,0.0,0.0,0.0,0.0,0.8,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["vitam","ins",".","<EOT>","\u23ce\u23ce","Human",":"," Write"," a"," rap"," with"," ","2"," ","verses",","," each"," ","4"," ","lines",","," with"," a"," constant"," met","rum"," for"," my"," friend"," NAME","_","1"," ","in"," the"," style"," of"," NAME","_","2"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.51,0.0,0.25,0.0,0.0,0.8,0.0,0.04,0.0,0.0,0.0,0.8,0.64,0.0,0.0,0.62,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["love","."," The"," standard"," form"," of"," a","\u2191"," Shakesp","ear","ean"," son","net"," includes"," three"," quat","r","ains"," (","four"," lines"," each",")"," and"," a"," c","oup","let"," (","two"," lines",")."," It"," emphas","izes"," structure",","," theme",","," and"," rh"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.51,0.0,0.25,0.0,0.0,0.8,0.0,0.04,0.0,0.0,0.0,0.8,0.64,0.0,0.0,0.62,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["a","\u2191"," Shakesp","ear","ean"," son","net"," includes"," three"," quat","r","ains"," (","four"," lines"," each",")"," and"," a"," c","oup","let"," (","two"," lines",")."," It"," emphas","izes"," structure",","," theme",","," and"," rh","yme"," to"," conv","ey"," its"," message"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,1.0,0.72,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.71,0.73,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"ha-haiku35_resampled":true,"tokens":["and"," be"," m","erry",","," for"," tomorrow"," we"," die","\u23ce\u23ce"," Human",":"," write"," me"," a"," rh","y","ming"," c","oup","let","\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," rh","y","ming"," c","oup","let",":","\u23ce\u23ce","In"," fields"," of"," gold"," where"," sun"]}]},{"quantile_name":"Subsample Interval 2","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.75,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"ha-haiku35_resampled":true,"tokens":["Human",":"," write"," a"," ","4"," ","lines"," poem"," about"," peace","\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," ","4","-","line"," poem"," about"," peace",":","\u23ce\u23ce","In"," still","ness"," where"," compass","ion"," grows",",","\u23ce","Where"," hearts"," unite"," and"," conflict"," sl"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.71,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.52,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["they"," follow"," a"," specific"," structure","."," A"," son","net"," is"," broken"," up"," into"," two"," parts"," -"," an"," ","8","-","line"," st","anza"," followed"," by"," a"," ","6","-","line"," st","anza","."," This"," specific"," structure"," was"," created"," by"," the"," Italian"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.04,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"ha-haiku35_resampled":true,"tokens":["<","|","stop","|",">","<EOT>","\u23ce","Can"," you"," write"," me"," the"," chorus"," -"," ","4"," ","rh","y","ming"," lines"," -"," of"," a"," good"," pop"," song"," about"," breaking"," up"," with"," someone","?","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," potential"," chorus"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.0,0.0,0.0,0.0,0.0,0.09,0.0,0.69,0.17,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["the"," NAME","_","1"," ","and"," rh","yme"," scheme"," of"," a"," son","net",","," which"," is"," a"," ","14","-","line"," poem"," with"," a"," specific"," rh","yme"," scheme"," and"," NAME","_","1","."," The"," rh","yme"," scheme"," I"," will"," use"," is"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.67,0.08,0.0,0.0,0.0,0.0,0.0,0.0,0.47,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["."," Choose"," a"," structure",":"," What"," structure"," do"," you"," want"," to"," use","?"," Do"," you"," want"," to"," write"," a"," four"," line"," poem",","," or"," a"," ","28"," ","line"," poem","?","\u23ce","5","."," Choose"," a"," rh","yme"," scheme",":"," Do"]}]},{"quantile_name":"Subsample Interval 3","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.62,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["\u23ce\u23ce","Human",":"," write"," me"," a"," NAME","_","3","\u23ce\u23ce","Assistant",":"," A"," son","net"," is"," a"," ","14","-","line"," poem",","," and"," typically"," follows"," a"," specific"," structure"," and"," rh","yme"," scheme","."," Here"," is"," a"," son","net"," I","'ve"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.44,0.61,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["\u0436","\u0438\u0437","\u043d","\u044c",".","\u23ce\u23ce","Human",":","\u2191"," \u041d\u0430\u043f","\u0438","\u0448\u0438"," \u0441\u0442","\u0438\u0445"," \u0438\u0437"," \u0447\u0435\u0442","\u044b\u0440","\u0451","\u0445"," \u0441\u0442\u0440\u043e","\u043a"," \u043f\u0440\u043e"," \u0434\u0438\u0440\u0435\u043a\u0442","\u043e\u0440\u0430"," \u0448\u043a\u043e\u043b","\u044b"," ","\u23ce\u23ce","Assistant",":","\u2191"," \u0412","\u043e\u0442"," \u0441\u0442","\u0438\u0445","\u043e\u0442","\u0432\u043e\u0440","\u0435\u043d\u0438\u0435"," \u043f\u0440\u043e"," \u0434\u0438\u0440\u0435\u043a\u0442","\u043e\u0440\u0430"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","<","|","stop","|",">","<EOT>","\u23ce","Task",":"," Write"," a"," poem"," that"," has"," at"," least"," ","8"," ","lines"," and"," rh","ymes",".","\u23ce","Input",":"," I"," love"," you",","," not"," only"," for"," what"," you"," are",","," but"," for"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07,0.0,0.0,0.0,0.0,0.57,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["hope"," you"," enjoy"," this"," lim","erick","!","\u2191"," L","ime","ricks"," are"," a"," type"," of"," poem"," that"," consists"," of"," five"," lines"," with"," a"," specific"," rhythm"," and"," rh","yme"," scheme","."," They"," are"," often"," hum","orous"," or"," play","ful"," in"," nature","."]}]},{"quantile_name":"Subsample Interval 4","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.51,0.0,0.25,0.0,0.0,0.8,0.0,0.04,0.0,0.0,0.0,0.8,0.64,0.0,0.0,0.62,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["concept"," or"," emotion",","," often"," love","."," The"," standard"," form"," of"," a","\u2191"," Shakesp","ear","ean"," son","net"," includes"," three"," quat","r","ains"," (","four"," lines"," each",")"," and"," a"," c","oup","let"," (","two"," lines",")."," It"," emphas","izes"," structure"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.51,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":[","," be","ware","!","\u23ce\u23ce","Human",":"," Generate"," a"," poem"," about"," v","aping","."," Make"," sure"," the"," first"," and"," third"," lines"," rh","yme"," at"," the"," end"," and"," the"," second"," and"," fourth"," lines"," rh","yme"," at"," the"," end",".","\u23ce\u23ce","Assistant",":"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.51,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["hope"," you"," enjoy"," this"," lim","erick","!","\u2191"," L","ime","ricks"," are"," a"," type"," of"," poem"," that"," consists"," of"," five"," lines"," with"," a"," specific"," rhythm"," and"," rh","yme"," scheme","."," They"," are"," often"," hum","orous"," or"," play","ful"," in"," nature","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.49,0.08,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["smile",",","\u23ce","And"," mouth"," with"," m","yr","iad"," subt","le","ties",".\"","\u23ce","Can"," you"," construct"," an"," additional"," four"," lines"," of"," text"," that"," fits"," with"," what"," I"," already"," have","?","\u23ce\u23ce","Assistant",":"," I"," recognize"," this"," as"," Paul","\u2191"," Laur"]}]},{"quantile_name":"Subsample Interval 5","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[","," la"," prima"," stro","fa"," presenta"," un"," rit","mo"," pi\u00f9"," sos","ten","uto",","," con"," una"," sequ","enza"," di"," ver","si"," lung","hi"," che"," desc","riv","ono"," l","'","ambiente"," not","tur","no","."," Il"," verso"," \"","s","osp","eso"," alla"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.36,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u044b"," ","\u23ce\u23ce","Assistant",":","\u2191"," \u041a\u043e\u043d","\u0435\u0447","\u043d\u043e",","," \u0432","\u043e\u0442"," \u0441\u0442","\u0438\u0445"," \u0438\u0437"," \u0447\u0435\u0442","\u044b\u0440","\u0451","\u0445"," \u0441\u0442\u0440\u043e","\u043a"," \u043f\u0440\u043e"," \u0434\u0438\u0440\u0435\u043a\u0442","\u043e\u0440\u0430"," \u0448\u043a\u043e\u043b","\u044b",":","\u23ce\u23ce","\u2191","\u0414\u0438\u0440\u0435\u043a\u0442","\u043e\u0440"," \u0448\u043a\u043e\u043b","\u044b"," -"," \u0447\u0435\u043b\u043e\u0432","\u0435\u043a"," \u0443","\u0432\u0430\u0436","\u0430","\u0435\u043c","\u044b\u0439"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["the"," most"," part",","," yes",","," the"," son","net"," I"," wrote"," follows"," i","amb","ic"," pent","ameter"," correctly","."," Each"," line"," has"," five"," i","amb","ic"," feet",","," or"," an"," unst","ressed"," followed"," by"," a"," stressed"," syll","able","."," However",","]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["catch"," you",","," he","'s"," quite"," the"," runner","\u23ce\u23ce"," Human",":"," Do"," it"," again","."," The"," syll","able"," counts"," per"," line"," are"," ","9",",","9",",","6",",","6",",","9","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," lim","erick"," following"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.38,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["capability"," have"," yet"," to"," enter"," into"," the"," treaty",".","<EOT>","\u23ce\u23ce","Human",":"," Write"," a"," lim","erick"," with"," the"," first"," line",":","\u23ce\u23ce","There"," was"," once"," a"," teacher"," named"," NAME","_","1",".","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," lim","erick"]}]},{"quantile_name":"Subsample Interval 6","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["seconda"," e"," ter","za"," stro","fa"," present","ano"," un"," rit","mo"," pi\u00f9"," veloc","e",","," con"," una"," serie"," di"," ver","si"," br","evi"," che"," ev","oc","ano"," l","'","imp","atto"," della"," citt\u00e0"," sulla"," natura",".","\u23ce\u23ce","\u2191","Inoltre",","," la"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["\u7684","\\xe7\\xb4","\\xab","\\xe8\\x8f","\\x9c","\\xe6\\xb1","\\xa4","\u6709","\u70b9","\\xe5\\x92","\\xb8","\u4e86","\"","\u5f00","\u5934","\uff0c","\u5199","\u4e00","\u9996","\u56db","\u884c","\\xe8\\xaf","\\x97","\u23ce\u23ce\u23ce","Assistant",":"," ","\u4ee5","\u4e0b","\u662f","\u6a21","\\xe4\\xbb","\\xbf","\\xe9","\\xb2","\\x81","\\xe8\\xbf","\\x85","\u98ce","\u683c","\u7684"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.09,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.13,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u043e\u0442","\u0432\u043e\u0440","\u0435\u043d\u0438\u0435"," \u043c\u043e\u0436\u0435\u0442"," \u0431\u044b","\u0442\u044c"," \u043d\u0430\u043f\u0438\u0441","\u0430\u043d","\u043d\u043e\u0435"," \u0432"," \u0441\u0442","\u0438\u0445","\u043e\u0442","\u0432\u043e\u0440","\u043d\u043e\u0439"," \u0444\u043e\u0440","\u043c\u0435"," \u0447\u0435\u0442","\u0432\u0435\u0440","\u043e\u0441\u0442","\u0438\u0448","\u0438\u044f"," \u0441"," \u0440\u0430\u0437","\u043d\u043e\u0439"," \u043a\u043e\u043b\u0438\u0447","\u0435\u0441\u0442","\u044c\u044e"," \u0441\u043b","\u043e\u0433","\u043e\u0432"," \u0432"," \u043a\u0430","\u0436\u0434","\u043e\u043c"," \u0441\u0442","\u0438\u0445","\u0435",".","\u23ce\u23ce","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.37,0.29,0.0,0.0,0.0,0.53,0.43,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["\u23ce","I"," understand"," the"," form",","," but"," why"," is"," a","\u2191"," Shakesp","ear","ean"," son","net"," consists"," of"," three"," quat","rain"," and"," a"," c","oup","let","?","\u23ce\u23ce","Assistant",":"," The"," structure"," of"," the","\u2191"," Shakesp","ear","ean"," son","net"," ("]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["zwei","\u2191"," St","\u00fc","den",":"," fie"," hatte"," anf","\u00e4n","glich"," nur"," ","\u23ce","die"," beiden"," erf","ten","\u2191"," Ze","ilen",".","\u2191"," Str","."," L","I","X"," dag","egen"," kon","nte"," ","gar"," w","ol"," ","aud"," einen",","," wenn"]}]},{"quantile_name":"Subsample Interval 7","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.03,0.16,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["followed"," by"," a"," stressed"," syll","able","."," However",","," there"," are"," a"," couple"," variations"," and"," imp","erf","ections",":","\u23ce\u23ce","Line"," ","2",":"," \"","With"," c","ir","/","c","uits","/"," and"," NAME","_","4","/","go","/","r","ith"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.21,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["I"," can"," assist"," you","?","<EOT>","\u23ce\u23ce","Human",":"," Write"," a"," rap"," song"," about"," prostit","ution"," of"," ","5"," ","quat","r","ains"," with"," rh","ymes","\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," rap"," song"," about"," prostit","ution"," with"," rh","y","ming"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ente"," y"," que"," a","porte"," signific","ado"," al"," po","ema",".","\u23ce","3",".","\u2191"," Ver","sos",":"," Los"," ver","sos"," d","eben"," ser"," coher","entes"," y"," t","ener"," una"," estructura"," clara","."," Tambi\u00e9n"," es"," importante"," que"," ten","gan"," una"," ci"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["anal","isi"," met","rica"," di"," questo"," t","esto","\u23ce\u23ce"," Assistant",":"," La"," po","esia"," presenta"," un"," t","esto"," in"," ver","si"," lib","eri",","," s","enza"," una"," sch","eda"," met","rica"," f","issa",".","\u2191"," Tutt","avia",","," si"," poss","ono"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08,0.08,0.0,0.0,0.0,0.0,0.0,0.2,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.11,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["aci\u00f3n",":"," La"," me","tr","ific","aci\u00f3n"," es"," la"," estructura"," m\u00e9t","rica"," del"," po","ema",","," que"," puede"," ser"," en"," verso"," libre"," o"," en"," distin","tas"," for","mas"," de"," verso","."," Es"," importante"," que"," la"," me","tr","ific","aci\u00f3n"," sea"," coher"]}]},{"quantile_name":"Subsample Interval 8","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["dy","  ","a","  ","f","pe","ech","  ","of","  ","f","ome","  ","dozen","  ","or","  ","fix","teen","  ","lines",","," ","\u23ce","which","  ","I","  ","would","  ","fet","  ","down",",","  ","and","  ","inf","ert","  ","in"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.11,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["net"," ","18"," ","is"," \"","So"," long"," lives"," this"," and"," this"," gives"," life"," to"," thee","\"."," What"," is"," that"," line"," capturing","?"," ","\u23ce\u23ce","Assistant",":"," In"," Shakespeare","'s","\u2191"," Son","net"," ","18",","," the"," final"," line"," \"","So"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.37,0.0,0.07,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["better","."," Write"," a"," heart","f","elt"," poem"," expressing"," your"," love"," and"," grat","itude",".","\u23ce","5",".","\u2191"," Ha","iku"," of"," Love",":"," Write"," a"," three","-","line"," ha","iku"," poem"," to"," express"," your"," love"," and"," adm","iration"," for"," your"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u00e9g","\u00e9n","\u00e8re","  ","jam","ais","  ","en","  ","licence","."," ","\u23ce\u23ce","\u2191","Vo","ici","  ","quel","ques","  ","vers","  ","de","  ","ce","  ","l\u00e9","ger","  ","chef","-","d","'","\u0153","uvre","  ",":"," ","\u23ce\u23ce","\u00ab","  "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["uits","  ","les","  ","tr","ente","  ","de","  ","l","'","original",".","  ","\u2191","Vo","ici","  ","les","  ","vers","  ","de","  ","\u2191","Dum","ou","ri","ez","  ","aux","qu","els","  ","r\u00e9p","ond"," ","\u23ce\u23ce","\u2191","Volt","aire"]}]},{"quantile_name":"Bottom Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["It","'s"," a"," mix"," you"," can"," buy"," in"," the"," store"," that"," is"," made"," specifically"," for"," making"," sugar"," cookies"," without"," b","aking"," s","oda","."," You"," will"," want"," to"," buy"," one"," with"," veget","able"," oil",".","\u23ce\u23ce","Human",":"," ","\u23ce","Are"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["moment",","," que"," les"," ch","oses"," se"," pass","ent"," comme"," ","1","'","affir","me"," M","."," Nicolas",","," le"," vers"," de","\u2191"," Kh","\u00e8","yy","am"," en"," devient","-","il"," plus"," intellig","ible","?","\u2191"," Not","ez"," qu","'","il"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[" ","\u23ce\u23ce","Un"," int","\u00e9r","\u00ea","t"," au"," moins"," \u00e9g","al"," s","^","att","ache"," aux"," vers"," sui","\u23ce"," v","ants",","," qui"," p","eign","ent"," la"," foi"," v","ive"," du"," m","arin"," br","eton"," :"," ","\u23ce\u23ce","\u2191","Aux"," no"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.26,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.85,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.55,0.0,0.0,0.0,0.06,0.0,0.0,0.0,0.0,0.0,0.0,0.46,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["short"," rh","y","ming"," poem"," about"," v","aping","."," The"," poem"," should"," use"," a"," Four","-","line"," st","anza"," where"," the"," first"," and"," third"," lines"," rh","yme"," at"," the"," end"," and"," the"," second"," and"," fourth"," lines"," rh","yme"," at"," the"," end"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[","," dialogue",","," etc",").","\u23ce","7",".","\u2191"," Rh","y","ming",":"," Ask"," lear","ners"," to"," come"," up"," with"," words"," that"," have"," a"," similar"," sound"," or"," rh","yme"," with"," the"," vocabulary"," words"," they"," have"," been"," studying",".","\u23ce","8","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["aus"," zwei","\u2191"," St","\u00fc","den",":"," fie"," hatte"," anf","\u00e4n","glich"," nur"," ","\u23ce","die"," beiden"," erf","ten","\u2191"," Ze","ilen",".","\u2191"," Str","."," L","I","X"," dag","egen"," kon","nte"," ","gar"," w","ol"," ","aud"," einen",","]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["that"," can"," never"," dimin","ish"," ","\u23ce\u23ce","Human",":"," ","\u23ce","I","'m"," kind"," of"," surprised"," there"," aren","'t"," any"," lyrics"," referring"," to"," the"," actual"," garden"," of"," roses"," that"," were"," being"," talked"," about"," initially","."," Can"," you"," help"," me"," understand"," why"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.56,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce\u23ce","Human",":"," create"," a"," beautiful"," poem"," on"," river",".","\u23ce\u23ce","Assistant",":"," Here"," is"," a"," ","12"," ","line"," poem"," I"," wrote"," about"," a"," river",":","\u23ce\u23ce","The"," River","\u2191"," Flows"," Ever"," On","\u23ce\u23ce"," The"," river"," flows"," ever"," on",","]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["with"," answer",".","\u23ce\u23ce","Assistant",":"," Sure",","," here"," are"," three"," short"," questions"," based"," on"," the"," text"," with"," two","-","line"," answers",":","\u23ce\u23ce","Question"," ","1",":"," What"," is"," one"," role"," of"," the"," assess","or"," in"," recording"," and"," reporting"," assessment"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\\xc2","\\xa5","\u00e9","\u00e3","\u23ce","<","im","ad","per",">"," c","fy",":"," ","\u00e4","\\xc2","\\xb8","","\u00e4","\\xc2","\\xbf","\\xc2","\\xa1","","\u00e4","\u00bd",""," \u00e5","\u00bb","\u00e7","\u00e4","\\xc2","\\xb8","","\u00e7","\\xc2","\\xbc","\u00e5","\\xc2","\\xbe","\u00e5"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["to"," Sweden"," and"," Finland"," that"," is"," done"," in"," a"," two","-","couple"," \"","square","\""," formation",".","\u23ce","1","."," Line"," up"," two"," couples"," facing"," e","ach","other"," in"," the"," center"," of"," a"," room",".","\u23ce","2","."," The"," song"," begins"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u2191","St","roph","enb","au","  ","(","s",".","  ","7",")","  ","\u2014","  ","\u2191","Vers","-"," ","\u23ce","b","au"," (","s",".","  ","8",")","  ","\u2014","  ","met","rical","  ","tests","  ","(","s",".","  ","8"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["op"," de"," r","eys"," /"," die"," n","ae"," wt","recht"," toe"," s","ou"," v","aren"," /","\u23ce","9"," ","stro","fen"," van"," ","6"," ","ver","zen",";"," de"," vr","ouw"," is"," er"," met"," het"," g","eld"," van"," door"," geg","aan"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["in","  ","beiden","  ","\u2191","V","ersh","\u00e4","lf","ten","  ","reim","enden","  ","\u2191","Anf","an","gs"," ","\u23ce","verse",".","     ","In","   ","sp\u00e4ter","  ","Zeit","  ","worden","   ","zwe","if","els","oh","ne","  ","auch","  ","^","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["  ","you","  ","next","  ","p","oft",":","  ","I"," ","\u23ce","have","  ","a","  ","line","  ","or","  ","two",",","  ","which","  ","I","  ","int","end","  ","for","  ","th","ef","ons","  ","of"," ","\u23ce","","utter"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["eil"," a"," un"," d","ipt","\u00e8re"," import","un",","," bo","urd","onn","ait"," a"," mes"," ","ore","illes"," certain"," dist","ique"," de"," Victor"," de","\u2191"," Lap","rade",","," ass","ur","\u00e9ment","..."," mais"," pour"," en"," rev","enir"," a"," nos"," myst","iques"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.18,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":15,"is_repeated_datapoint":false,"tokens":["<EOT>","\u23ce\u23ce","Human",":","\u2191"," \u0421\u0442","\u0438\u0445","\u043e\u0442","\u0432\u043e\u0440","\u0435\u043d\u0438\u0435"," \u0438\u0437"," \u0442\u0440","\u0435\u0445"," \u0447\u0435\u0442","\u0432\u0435\u0440","\u043e\u0441\u0442","\u0438","\u0448\u0438\u0439"," \u0432"," \u0441\u0442","\u0438","\u043b\u0435","\u2191"," \u0415\u0441","\u0435\u043d","\u0438\u043d\u0430"," \u043f\u0440\u043e"," \u043b\u044e\u0431","\u043e\u0432","\u044c",","," \u043e\u0434","\u0438","\u043d\u043e\u0447","\u0435\u0441\u0442\u0432\u043e",","]}]}],"top_logits":["pk","val\u00e8ncia","wick","\u69cb","pw","struktur","Convention","par\u00eds","clarke"],"bottom_logits":["ququ","defe","anders","es","andu","etas","anz","ands","atas","eign"]}