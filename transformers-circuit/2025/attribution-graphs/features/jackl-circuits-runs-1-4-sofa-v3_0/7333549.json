{"index":7333549,"examples_quantiles":[{"quantile_name":"Top Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.66,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ade","uring",">"," ab","ent","ley",":"," in"," theory",","," no","."," But"," the"," tests"," require"," a"," package"," like"," rabbit","-","management"," or"," so",","," and"," this"," is"," currently"," not"," available"," in"," precise","."," the"," LP"," P","PA"," needs"," to"," be"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.35,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.86,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["971",")"," are"," widely"," used"," prok","ary","otic"," in"," vit","ro"," translation"," systems",".","\u23ce","In"," the"," case"," of"," the"," rabbit"," ret","icul","oc","yte"," method",","," cell","-","free"," ret","icul","oc","yte"," lys","ates"," continue"," to"," synthes","ize"," protein"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.82,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["anded"," rats"," (","1","-","3",".","5","),"," cult","ured"," cells"," (","4","-","6",")"," and"," b","anded"," rabb","its"," (","7",")"," are",":"," ","1",")"," Does"," hy","pert","ro","phy"," reduce"," S","ER","C","a","2"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.33,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.82,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["I"," cell"," lines"," of"," human"," or"," rabbit"," origin"," as"," virus"," source","."," It"," was"," found"," that"," certain"," infected"," lines"," killed"," rabb","its"," in"," a"," dose"," dependent"," manner",";"," animals"," inj","ected"," with"," the"," let","hal"," lines"," developed"," high"," temperature"," within"," four"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.81,0.48,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["80","x","9","d"," includes"," humans",","," non","-","human"," mammals"," (","e",".","g","."," dogs",","," cats",","," rabb","its",","," cattle",","," horses",","," sheep",","," go","ats",","," sw","ine",","," deer",","," or"," the"," like",")"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.81,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[":"," Here"," are"," five"," tools"," similar"," to"," Java"," Message"," Service",":","\u23ce\u23ce","*"," NAME","_","1","\u23ce","*"," R","ab","bit","M","Q","\u23ce","*"," Active","M","Q","\u23ce","*"," Apache","\u2191"," Q","p","id","\u23ce","*"," Google"," Cloud","\u2191"," Pub"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.81,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["mon","ocl","onal"," and"," pol","ycl","onal"," antib","odies",","," the"," experimental"," animal"," is"," su","itably"," a"," go","at",","," rabbit",","," rat"," or"," mouse","."," If"," desired",","," the"," immun","ogen"," of"," the"," invention"," may"," be"," administered"," as"," a"," conjug"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.81,0.4,0.0,0.0,0.0,0.23,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.06,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[")"," [","10","-","3","]","\u23ce","-","q","ueue","bot",":#","ubuntu","-","release","-"," New",":"," accepted"," rab","bi","tv","cs"," [","sync","]"," (","focal","-","proposed",")"," [","0",".","18","-","1","]","\u23ce","-","q"]},{"tokens_acts_list":[0.0,0.0,0.0,0.53,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.78,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[" ","R","ab","bit","M","Q"," also"," provides"," various"," tools"," and"," libraries"," for"," building"," applications",","," including"," the"," R","ab","bit","M","Q"," Python"," client"," library",","," which"," is"," a"," popular"," choice"," for"," building"," R","ab","bit","M","Q","-","based"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.78,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce\u23ce","Yet"," most"," discussions"," I"," read"," on"," here"," are"," the"," exact"," opposite","."," They"," quickly","\u23ce"," dissol","ve"," into"," unnecessary"," rabbit"," ho","ling"," about"," some"," particular"," n","iche"," topic"," a","\u23ce"," comm","enter"," particularly"," c","ares"," about","."," Few"," people"," compl"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.77,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["proposal"," is"," to"," examine"," whether"," pren","atal"," gluc","oc","or","tic","oids"," affect"," nep","hr","ogen","esis"," in"," f","etal"," rabb","its",".","\u2191"," Gluc","oc","or","tic","oids"," have"," also"," been"," shown"," to"," play"," a"," role"," in"," the"," mat","uration"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.76,0.19,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["of"," farmers"," in"," raising"," meat","-","producing"," animals"," such"," as"," sw","ine",","," sheep",","," cattle",","," go","ats",","," rabb","its"," and"," po","ult","ry","."," Further",","," such"," infections"," are"," a"," source"," of"," great"," concern"," for"," companion"," animals"," such"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.76,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["Science",","," Machine"," Learning",","," Python",","," Java",",","\u2191"," Wolf","ram","\u2191"," Mathemat","ica",","," SQL",","," R","ab","bit","M","Q",","," Redis",",","\u2191"," Cass","andra",",","\u2191"," Elasticsearch",","," Apache","\u2191"," Sol","r",","," Git",","," Linux"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.76,0.0,0.0,0.55,0.0,0.52,0.0,0.0,0.0,0.44,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.62,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["squ","ir","rel","."," The"," cow"," e","ats"," the"," squ","ir","rel","."," The"," cow"," sees"," the"," cat","."," The"," rabbit"," is"," round","."," The"," rabbit"," sees"," the"," cat","."," The"," squ","ir","rel"," e","ats"," the"," rabbit","."," The"," squ"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.75,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["delivery","."," A"," semi","-","interp","enet","rating"," hyd","rog","el"," polymer"," network"," was"," prepared"," consisting"," of"," a"," polymer"," containing"," rabbit"," antib","ody"," (","I","g","G",")"," and"," go","at"," anti","-","rabbit"," I","g","G"," as"," the"," ant","igen"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.75,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["subfamily","."," Expression"," of"," these"," F","M","Os"," is"," both"," tissue"," and"," species"," dependent","."," The"," major"," liver"," enzyme"," in"," rabb","its",","," for"," example",","," is"," a"," minor"," hepat","ic"," enzyme"," in"," mouse",","," but"," a"," major"," enzyme"," in"," mouse"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.75,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[","," isn","'t"," quite"," ag","ile","\u23ce"," enough",")"," *"," Postgre","SQL","\u23ce\u23ce","\u2191"," Q","ueue","ing"," *"," R","ab","bit","M","Q"," *","\u2191"," B","ean","st","alk","d"," *","\u2191"," G","ear","man","\u23ce\u23ce"," Other"," *","\u2191"," Thinking","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.75,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["not"," bind"," to"," M"," cell"," surfaces","."," Since"," these"," mouse"," mon","ocl","onal"," antib","odies"," are"," also"," foreign"," proteins"," to"," rabb","its",","," levels"," of"," ant","igen"," specific"," I","g","A"," in"," mu","co","sal"," secret","ions"," will"," be"," studied"," to"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.75,0.14,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["F","U"," are"," visible"," in"," the"," micros","ph","eres"," at"," least"," up"," to"," the"," ","19","th"," day","."," In"," rabb","its",","," after"," intr","ace","reb","ral"," impl","ant","ation"," of","\u21ea"," PL","AGA","-","5","-","F","U"," micros"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.75,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","4",".","\u2191"," Backbone",",","\u2191"," Ang","ular","js","\u23ce","5","."," Less"," for"," CSS","\u23ce","6",".","\u2191"," Rabbit"," M","Q"," or"," AWS"," SN","S","+","S","Q","S"," for"," messaging","\u23ce","======","\u23ce","FF","0","000","","itor"]}]},{"quantile_name":"Subsample Interval 0","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.66,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["ade","uring",">"," ab","ent","ley",":"," in"," theory",","," no","."," But"," the"," tests"," require"," a"," package"," like"," rabbit","-","management"," or"," so",","," and"," this"," is"," currently"," not"," available"," in"," precise","."," the"," LP"," P","PA"," needs"," to"," be"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.35,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.86,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["971",")"," are"," widely"," used"," prok","ary","otic"," in"," vit","ro"," translation"," systems",".","\u23ce","In"," the"," case"," of"," the"," rabbit"," ret","icul","oc","yte"," method",","," cell","-","free"," ret","icul","oc","yte"," lys","ates"," continue"," to"," synthes","ize"," protein"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.82,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["anded"," rats"," (","1","-","3",".","5","),"," cult","ured"," cells"," (","4","-","6",")"," and"," b","anded"," rabb","its"," (","7",")"," are",":"," ","1",")"," Does"," hy","pert","ro","phy"," reduce"," S","ER","C","a","2"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.33,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.82,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["I"," cell"," lines"," of"," human"," or"," rabbit"," origin"," as"," virus"," source","."," It"," was"," found"," that"," certain"," infected"," lines"," killed"," rabb","its"," in"," a"," dose"," dependent"," manner",";"," animals"," inj","ected"," with"," the"," let","hal"," lines"," developed"," high"," temperature"," within"," four"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.81,0.48,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["80","x","9","d"," includes"," humans",","," non","-","human"," mammals"," (","e",".","g","."," dogs",","," cats",","," rabb","its",","," cattle",","," horses",","," sheep",","," go","ats",","," sw","ine",","," deer",","," or"," the"," like",")"]}]},{"quantile_name":"Subsample Interval 1","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.82,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["anded"," rats"," (","1","-","3",".","5","),"," cult","ured"," cells"," (","4","-","6",")"," and"," b","anded"," rabb","its"," (","7",")"," are",":"," ","1",")"," Does"," hy","pert","ro","phy"," reduce"," S","ER","C","a","2"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.33,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.82,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["I"," cell"," lines"," of"," human"," or"," rabbit"," origin"," as"," virus"," source","."," It"," was"," found"," that"," certain"," infected"," lines"," killed"," rabb","its"," in"," a"," dose"," dependent"," manner",";"," animals"," inj","ected"," with"," the"," let","hal"," lines"," developed"," high"," temperature"," within"," four"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.81,0.48,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["80","x","9","d"," includes"," humans",","," non","-","human"," mammals"," (","e",".","g","."," dogs",","," cats",","," rabb","its",","," cattle",","," horses",","," sheep",","," go","ats",","," sw","ine",","," deer",","," or"," the"," like",")"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.81,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":[":"," Here"," are"," five"," tools"," similar"," to"," Java"," Message"," Service",":","\u23ce\u23ce","*"," NAME","_","1","\u23ce","*"," R","ab","bit","M","Q","\u23ce","*"," Active","M","Q","\u23ce","*"," Apache","\u2191"," Q","p","id","\u23ce","*"," Google"," Cloud","\u2191"," Pub"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.81,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["mon","ocl","onal"," and"," pol","ycl","onal"," antib","odies",","," the"," experimental"," animal"," is"," su","itably"," a"," go","at",","," rabbit",","," rat"," or"," mouse","."," If"," desired",","," the"," immun","ogen"," of"," the"," invention"," may"," be"," administered"," as"," a"," conjug"]}]},{"quantile_name":"Subsample Interval 2","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["up","time","?","\u23ce\u23ce","Each"," individual"," question"," will"," take"," minutes"," to"," hours"," to"," research"," and"," will"," open","\u23ce"," up"," new"," rabbit"," holes",".","\u23ce\u23ce","~~~","\u23ce","\u2191","J","edi","72","\u23ce","That","'s"," why"," I"," think"," these"," blogs","/","projects"," are"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["REST","\u23ce","    ","\u23ce","        ","*"," Object","-","\u2191","Oriented"," Development","/","Design","\u23ce","    ","\u23ce","        ","*"," R","ab","bit","M","Q"," (","or"," other"," message"," q","ueue","ing"," technologies",")","\u23ce","     ","\u23ce","        ","*","\u2191"," Automated"," Testing","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.23,0.08,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["and"," particularly"," mam","mal","ian"," subjects"," including",","," in"," addition"," to"," humans",","," horses",","," c","ows",","," dogs",","," rabb","its",","," fow","l",","," sheep",","," and"," the"," like",","," for"," veter","inary"," purposes","."," As"," noted"," above",","]},{"tokens_acts_list":[0.0,0.0,0.0,0.51,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["laboratory"," from"," a"," rabbit"," with"," d","iar","rh","ea"," has"," been"," shown"," to"," produce"," d","iar","rh","ea"," in"," other"," rabb","its",".","\u21ea"," RD","EC","-","1"," ","does"," not"," inv","ade"," or"," produce"," enter","ot","ox","in"," but"," does"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","dishes","  ","were","  ","ass","aul","ted","  ","instead","  ","of","  ","dit","ches",",","  ","and"," ","\u23ce","rabb","its","  ","were","  ","cut","  ","to","  ","pieces","  ","upon","  ","the","  ","reb","elling","  ","f"," ;"," "]}]},{"quantile_name":"Subsample Interval 3","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.61,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["primary"," ind","irect","ly"," via"," phosph","at","id","ic"," acid","."," The"," experimental"," model"," employed"," is"," a"," primary"," culture"," of"," rabbit"," prox","imal"," tub","ule"," cells","."," the"," principal"," method","ologies"," used"," include"," fluor","ometric"," monitoring"," of"," Na","+-","H","+"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["beta","-","\u21ea","V","LD","L",")"," by"," gran","ul","oma"," mac","roph","ages"," from"," normal"," and","\u21ea"," WH","HL"," rabb","its",","," and"," the"," influence"," of"," induced"," activation"," (","gamma","-","I","F","N",","," L","PS",")"," on"," MP"]},{"tokens_acts_list":[0.0,0.0,0.0,0.71,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.31,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["service"," ","\u23ce","rabb","itm","q"," events"," handler"," with"," data"," as"," real"," time"," data",".","\u23ce\u23ce","authent","ification"," process"," :"," rabb","itm","q"," process"," authent","ification"," ,"," multiple"," authent","ification"," layers","\u23ce\u23ce"," topic"," between"," us"," and"," ui"," interface"," :"," one"," topic"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["stim","ul","atory"," or"," inhib","itory"," effect"," on"," the"," init","iation"," step"," of"," the"," protein"," synthetic"," process"," in"," lys","ed"," rabbit"," ret","icul","oc","ytes"," and"," H","eL","a"," cell"," extra","cts","."," The"," second"," objective"," involves"," the"," role"," of"," NA"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.11,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["rance"," if"," they","'re"," turned"," over"," onto"," their"," backs"," and"," held"," there"," for"," just"," a"," few"," seconds",".\""," \"","\u2191","Rabb","its"," and"," guinea"," p","igs"," do"," the"," same"," if"," you"," stroke"," them"," or"," roll"," them"," over"," first",".\""," \"","Do"]}]},{"quantile_name":"Subsample Interval 4","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["the"," pol","ys","uc","cin","im","ide"," with"," cy","ste","amine"," is"," an"," irrit","ant"," when"," they"," carried"," out"," a"," rabbit"," eye"," muc","ous"," membrane"," irrit","ation"," test"," (","described"," her","ein","after",").","\u23ce","Therefore",","," it"," has"," been"," apparent"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["will"," remain"," to"," drop"," ","1","\u23ce","n"," the"," Easter"," collection"," basket","."," '","\u23ce","Never"," were"," so"," many"," pretty"," rabb","its","."," '","\u23ce","."," ?","\u23ce","\u2191","Ji","unu"," u","i","ira",","," ^","","irt","N"," g","f"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\"","Life"," doesn","'t"," give"," you"," many"," chances",".\""," \"","It","'s"," all"," about"," one"," shot",".\""," \"","\u2191","Mere"," rabb","is","-","ro","using"," is"," not"," my"," aim","...\""," \"","The"," who","/","e"," effort"," is"," to"," make"," the"," wheels"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.13,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\"","OK",","," I"," am"," wearing"," a"," sad","dle"," and"," it","'s"," time"," to"," go",".\"","\""," \"","I"," love"," rabb","its",".\""," \"","Oh",","," my","...\""," \"\"","\u2191","Wow",","," I"," am"," thinking"," maybe"," I"," should"," have"," had"," a"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.59,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["I"," can"," still"," clone"," the"," branch","\u23ce","#","la","unch","pad"," ","2","015","-","04","-","06","\u23ce","<","rabb","its","ky","net",">"," Hi","\u23ce","<","K","a","Z","e","R",">"," hi"," there","\u23ce","<","K","a","Z"]}]},{"quantile_name":"Subsample Interval 5","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["He"," is"," sweet"," and"," kind"," and"," nur","turing","."," He"," loves"," playing"," kick","ball","."," He"," also"," loves"," his"," pet"," rabbit"," and"," his"," family","."," He"," even"," has"," a"," comed","ic"," nature"," to"," his"," personality",".","\u23ce\u23ce","Assistant",":"," Here","'s"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["in","  ","inj","ecting","  ","a","  ","drop","  ","into","  ","one"," ","\u23ce","eye","  ","of","  ","a","  ","rabbit",",","  ","avoiding","  ","the","  ","entrance","  ","of","  ","ger","ms","  ","and","  ","the","  ","forma","-"," "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["him"," sit",".\""," \"","\u2191","Sit","?\""," \"","He"," gets"," a"," chair","?\""," \"","He","...\""," \"","He"," kills"," b","unn","ies",".\""," \"","I","'m"," impressed",".\""," \"","\u2191","Didn","'t"," expect"," anyone"," to"," come"," looking"," for"," me"," so"," soon"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.81,0.4,0.0,0.0,0.0,0.23,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.06,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["[","10","-","3","]","\u23ce","-","q","ueue","bot",":#","ubuntu","-","release","-"," New",":"," accepted"," rab","bi","tv","cs"," [","sync","]"," (","focal","-","proposed",")"," [","0",".","18","-","1","]","\u23ce","-","q","ueue"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["neck","'s"," on"," the"," block",".\""," \"","Come"," on",","," get"," a"," move"," on",".\""," \"","It","'s"," like"," a"," rabbit"," hut","ch"," in"," here",".\""," \"","\u2191","Okay",".\""," \"","You"," nice"," and"," sn","ug","?\""," \"","What"," the"," hell"]}]},{"quantile_name":"Subsample Interval 6","examples":[{"tokens_acts_list":[0.0,0.0,0.42,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.49,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.48,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ats"," the"," rabbit","."," The"," cow"," is"," red","."," The"," dog"," needs"," the"," cow","."," The"," rabbit"," needs"," the"," mouse","."," If"," the"," dog"," ch","ases"," the"," rabbit"," and"," the"," dog"," is"," not"," red"," then"," the"," dog"," does"," not"," need"," the"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["philosoph","eme"," ","\u23ce","ausg","aben","  ","und","  ","die","  ","\u2191","Bann","st","rah","len","  ","der","  ","\u2191","Rabb","inen","  ","auf","  ","sich","  ","l","uden","."," ","\u23ce","\u2191","Myst","ik",",","  ","wie","  ","sie","  "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["language",","," will"," be"," a"," very"," important"," one"," indeed","."," In"," fact"," he"," may"," then"," venture"," upon"," almost"," any","\u2191"," Rabb","in","ical"," work",".","\u21ea"," PRE","FACE",".","\u2191"," V","ll"," The"," Collection"," of","\u2191"," A","hh","revi","ations",","]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["  ","b","enn","  ","","aud",")","  ","b","te"," ","\u23ce","j","\u00fcb","if","c","f","yen","  ","\u2191","Rabb","iner","  ","(","h","aften","  ","mit","  ","S","B","ann","  ","un","b","  ","ba","bu","raj","  ","be"]}]},{"quantile_name":"Subsample Interval 7","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["of"," African","-","American"," folklore",".","  ","In"," African","-","American"," folklore",","," the"," character","\u2191"," Br","'","er","\u2191"," Rabbit",","," or"," B","'","","rer","\u2191"," Rabbit",","," is"," not"," stere","otyp","ed"," and"," is"," a"," smart"," t","rick"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["um",">"," wow"," i"," got"," ","3"," ","seconds"," lag","\u23ce","<","D","aS","kre","ech",">"," Hi"," w","rab","bit","^","\u23ce","<","m","4","x","1","m","um",">"," ok"," people"," i","'ve"," done"," this"," sudo"," m","ke","2"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.54,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["is"," a"," young"," asian"," m","aid",","," pet","ite",","," slim",","," average",","," w","ears"," glasses",","," rabbit"," teeth",","," self","-","conscious"," about"," her"," small"," chest",".","\u23ce","Write"," their"," description"," in"," the"," first"," person",","," as"," she"," tries"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["than"," guinea"," p","igs",","," a"," topic"," you"," may"," wish"," to"," research"," further"," if"," you"," are"," considering"," adop","ting"," a"," rabbit"," as"," a"," pet",".","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","What"," essential"," elements"," should"," be"," included"," in"," a"]}]},{"quantile_name":"Subsample Interval 8","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ue","^","  ","f","is","  ","","enj","amb","ent","  ","fo","uvent","  ","fur","  ","la","  ","Lu","*"," ","\u23ce","ne","  ","pr\u00e9c","\u00e9d","ente",",","  ","&","  ","f","eft","ent","  ","la","non","v","elle","\u2191"," L"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["  ","d","'","Abraham","  ","\u2191","L\u00e9","vi",",","  ","l","\u00ee","a","\u00ef","m","  ","fils","  ","du","  ","rab","bin","  ","\u2191","E","li","\u00e9","zer"," ","\u23ce","\u2191","L\u00e9","vi",".","  ","\u2191","Leur","  ","signature","  ","est"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ang"," won","'t"," hire"," most"," of"," them",","," and"," it","'s"," not","\u23ce"," because"," they","'re"," incomp","et","ent",".","\u23ce\u23ce","~~~","\u23ce","max","har","ris","\u23ce"," I","'m"," ","39",","," and"," I"," didn","'t"," major"," in"," CS",".","\u21ea"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["."," Il"," est"," n\u00e9"," \u00e0","\u2191"," Ak","ka",","," ville"," du","\u2191"," Dr","aa","."," ","11"," ","n","'","av","ait"," pas"," ne","uf"," ans"," qu","and"," il"," entrepren","ait"," ses"," premiers"," voy","ages","."," Si"," je","une"," qu","'"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["another"," or"," even"," verb","ally"," and"," physically"," attack"," each"," other",","," so"," it","'s"," best"," to"," get"," only"," one"," pet"," rabbit"," if"," you"," can",","," or"," adopt"," a"," male"," and"," female"," pair"," if"," you"," choose"," to"," have"," more"," than"," one"," pet"]}]},{"quantile_name":"Bottom Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["15",".","69","\u23ce","1","918"," ","59",".","72","\u23ce","1","918"," ","1","918","\u23ce","1","918"," ","1","918"," ","1","918","\u23ce","1","918"," ","1","918"," ","1","918","\u23ce","1","918"," ","1","918","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.43,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["com","/","sp","/","12","/","comics","/","<EOT>","\u23ce\u23ce","Human",":"," A"," farmer"," has"," some"," chick","ens"," and"," rabb","its","."," There"," are"," ","60"," ","eyes"," and"," ","100"," ","legs"," altogether","."," How"," many"," chick","ens"," and"," how"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["-"," General"," Oscar"," West","\u23ce"," over",","," Chief"," of"," Army"," Air","\u23ce"," Corps",":","\u2191"," Hear"," Admiral","\u2191"," Ar","\u23ce"," th","ur"," B","."," Cook",","," Chief",","," Navy","\u23ce"," Bureau"," of","\u2191"," Aer","onaut","ics",";","\u23ce","Captain"," L","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u2014","  ","II",".","  ","\u2191","M\u00e9","ir","  ","fils","  ","d","'","Isaac"," ","\u23ce","\u2191","","Eat","zen","ell","b","ogen",",","  ","5","  ","septembre",",","  ","17","  ","\u00e9l","ul",",","  ","323","  ","(","1","563"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["gr","ids"," between"," fluor","osc","opy"," and"," radi","ography"," by"," using"," a"," single"," F","P","D"," (","\u2191","Flat"," Panel","\u2191"," Detector",")"," sensor"," using"," a"," semiconductor",".","\u23ce","However",","," the"," radi","ographic"," apparatus"," which"," exec","utes"," both"," fluor","osc"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ffle","_","details","_","mode","_","none","\u23ce","            ","}","\u23ce","        ",")","\u23ce\u23ce","        ","button","Pre","fer","red","R","aff","le","Mode",".","set","On","Click","Listener"," {","\u23ce","            ","show","R","af","fl","eM","odes","(","\u23ce","                ","require"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["do","omed"," House",","," The"," Secret","\u2191"," Witness",","," All","\u2191"," Souls","'"," Day",","," and"," The","\u2191"," Aged"," lia","bbi",","," have"," been"," translated"," in"," Mrs","."," Anne"," S",".","\u2191"," Bush","by","'s"," The","\u2191"," Danes",",","\u2191"," Sket"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["link"," by"," utilizing"," the"," channel"," CH","2"," ","set"," by"," the"," channel","-","setting"," switch"," ","112",".","\u23ce","The"," radio"," keyboard"," ","202"," ","is"," connected"," to"," the"," terminal"," ","102"," ","via"," the"," radio"," link","."," This"," radio"," keyboard"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["cannot"," fly"," or"," operate"," helicop","ters"," due"," to"," lack"," of"," intelligence"," and"," motor"," skills","."," It","'s"," highly"," unlikely"," a"," rabbit"," would"," have"," any"," use"," for"," or"," interest"," in"," a"," helicopter",".","\u23ce\u23ce","-"," No"," photos"," or"," videos"," seem"," to"," exist"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["do","\u015bwi","ad","cz","enia",","," ta","kie"," jak"," pr","aca"," w"," klub","ie","\u2191"," Play","boy"," jako"," \"","kr\u00f3l","icz","ka","\".","\u2191"," Jej"," praw","dzi","we"," im","i\u0119"," to"," Angela",","," a"," adopt","ow","ali"," ","j\u0105"," Richard"," i"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ously"," requires"," a"," large"," surface"," area"," for"," the"," placement"," of"," its"," constituent"," components","."," The"," high"," power"," consumption"," of"," the"," triang","ular","-","to","-","sin","u","so","idal"," w","ave","form"," generator"," is"," attribut","able"," to"," the"," requirements"," of"," the"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["of"," x"," and"," y"," into"," the"," equation"," for"," z"," and"," w"," to"," find"," the"," number"," of"," chick","ens"," and"," rabb","its",".","\u23ce\u23ce","z"," ="," ","60"," ","-"," x","\u23ce"," w"," ="," ","100"," ","-"," y","\u23ce\u23ce"," z"," ="]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["  ","(","S",".","  ","auch"," ","\u23ce","\u2191","H","abb","eo","et","'s","  ","\u2191","Ans","icht","  ","Cap",".","  ","5",".)","     ","W",".","  ","\u2191","Bu","dd","  ","dag","egen"," ","\u23ce","l","\u00fcm","B","t","  "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["je","ho","vi","ih"," in","'","ti","is"," ","\u23ce","temple"," at"," Jerusalem","."," According"," to"," their"," -","dw","n"," ","\u23ce","acknowledg","ement",","," this"," is"," the"," kingdom"," never"," '","^"," to"," ","\u23ce","be"," destroyed",","," which"," was"," predicted"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\\xa4","\u00a9","\u00e7","\u00aa","\u00e7","\\xc2","\\xb6","\u00e6","","\u00b2","\\xc2","\\xa1","\u00e5","\u00a3","\u00b0","\u00e9","\u00b3","\u00e4","\u00ba","\u23ce","<","robin","__",">"," \u00e5","\\xc2","\\xa5","\u00e6","\u00aa","\u23ce","<","robin","__",">"," al","sam","ix","e","r\u00e9","\u00e9","\\xc2"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["constantly","-","recurring"," ","\u23ce","names","  ","that","  ","are","  ","\"","  ","familiar","  ","in","  ","our","  ","mou","ths","  ","as","  ","household","  ","words",",\""," ","\u23ce","and","  ","wo","  ","trust","  ","that","  ","they",",","  "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["bloom",","," the"," Broadway"," star",".\""," \"","CC",","," can"," we"," have"," your"," aut","ograph","?\""," \"","\u2191","S","ked","ad","dle",".\""," \"","My"," friends",","," Mrs",".","\u2191"," Vald","ez",","," Mrs","."," Cohen",","," Mr",".","\u2191"," Lev"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["irt"," es","\u2191"," E","uch"," ","gar"," nicht"," zu"," wi","\u017f","\u017f","en",","," ob"," ","\u23ce","\u2191","E","uer"," Rabbi"," durch"," ","\u017f","ein","\u2191"," Stud","ium",","," oder"," durch"," ein"," ","halb"," ","\u23ce","\u2191","Du","tz","end","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["(","main",")"," \"","firefox"," \"","help"," >"," report"," a"," bug","\""," tries"," to"," create"," bad"," filename","\""," [","\u2191","Und","ec","ided",",","\u2191","Un","conf","ir","med","]","  ","https","://","la","unch","pad",".","net","/","bugs","/"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["-","x","en","?","\u23ce","<","md","z",">"," I"," guess"," not","\u23ce","<","robb","iew",">"," h","eh","\u23ce","<","smo","ser",">"," sl","ang","as","ek",","," i"," think"," so"," ,"," as"," per"," j","j","oh","ansen","\u23ce","<"]}]}],"top_logits":["bit","hole","itual","rabb","in","holes","it","warren"],"bottom_logits":["fav","adj","oro","rome","carp","ret","effect","chal","Stars","Science"]}