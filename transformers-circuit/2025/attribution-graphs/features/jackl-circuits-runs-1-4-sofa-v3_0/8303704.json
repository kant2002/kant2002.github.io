{"index":8303704,"examples_quantiles":[{"quantile_name":"Top Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06,0.06,0.0,0.0,0.0,1.0,0.42,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[" ","16"," ","are"," mov","ably"," disposed"," so"," that"," a"," minimum","-","size"," sheet"," SS"," to"," a"," maximum","-","size"," sheet"," ","LS"," can"," be"," st","acked"," and"," supported"," on"," the"," t","ray"," ","12","."," In"," order"," not"," to"," obst"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.11,0.0,0.0,0.44,0.84,0.0,0.0,0.09,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["low"," activity"," ","5","-","\u21ea","HTT","L","PR"," \"","short","\""," all","ele"," as"," compared"," to"," individuals"," with"," the"," \"","long","\"","/\"","long","\""," gen","otype","."," Moreover",","," our"," pilot"," data"," suggest"," interactions"," between"," the"," two"," gen","otypes"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.82,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["a","82","e","9","d","7","b","24","a","636","d",","," type",":"," ","2","}"," m","_","P","ref","ab","Int","ernal",":"," {","file","ID",":"," ","23","580","}"," serial","ized","Version",":"," ","5"," ","m"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.72,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["c","3","a","11","345",","," type",":"," ","2","}"," m","_","P","ref","ab","Int","ernal",":"," {","file","ID",":"," ","23","575","}"," serial","ized","Version",":"," ","5"," ","m","_","Component",":"," -"," component",":"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.68,0.09,0.06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["?\""," \"","\u2191","Yog","ot"," some","..."," ar","zt"," on"," you",".\""," \"","Short"," st","icks"," carry"," the"," p","acks",".\""," \"","\u2191","Looks"," like"," it","'s"," you"," and"," me",",","kate",".\""," \"","So"," what","'s"," this"," stuff"," even"," do"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.67,0.0,0.0,0.0,0.0,0.0,0.37,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[")","\u23ce","-"," M"," cone"," cells"," -"," sensitive"," to"," medium"," wavel","eng","ths"," (","green","/","yellow"," end",")","\u23ce","-"," L"," cone"," cells"," -"," sensitive"," to"," long"," wavel","eng","ths"," (","red"," end",")","\u23ce\u23ce","These"," cone"," cells"," essentially"," act"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.66,0.36,0.22,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[")","s","')","\u23ce\u23ce","def"," calculate","_","ma","cd","(","df",","," ","ema","_","s","=","12",","," ","ema","_","l","=","26",","," signal","_","m","w","=","9","):","\u23ce","    ","df","['","E","MA","_","S"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.65,0.0,0.0,0.0,0.0,0.04,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["larger"," particle"," di","ame","ters"," exhib","iting"," smaller"," sc","at","tering"," angles",","," the"," cond","enser"," lens"," ","4","c"," having"," a"," long"," focal"," distance"," (","f","3",")"," is"," used"," while"," the"," ring","-","shaped"," detector"," ","5"," ","is"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.57,0.64,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["s","/","N","AC","K","s"," (","in"," the"," case"," where"," a"," short"," cyc","lic"," prefix"," is"," used",";"," for"," a"," long"," cyc","lic"," prefix"," the"," number"," might"," be"," less",")."," The"," U","E"," knows"," the"," AC","K","/","\u21ea","N"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.61,0.28,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["class","=\"","i","md","-","buttons","\">","\u23ce","        ","<","button"," class","=\"","i","md","-","button"," i","md","-","button","--","post","\""," type","=\"","button","\"","\u23ce","            ","onclick","=\"","window",".","location",".","href","='","<?","php"," echo"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06,0.0,0.0,0.0,0.33,0.61,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["iss","age"," alter","nat","if"," et"," n","omm","\u00e9es"," AP","H","-","1","\u03b1","S"," et"," AP","H","-","1","\u03b1","L"," (","248"," ",";"," ","431",")."," La"," prot","\u00e9","ine"," AP","H","-","1"," ","et"," la","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.58,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["control"," of"," the"," dimensions"," of"," the"," repe","ating"," domains"," and"," their"," short"," range"," ordering"," can"," be"," quite"," robust","."," However",","," the"," use"," of"," block"," c","opol","ym","ers"," to"," generate"," lith","ographic"," patterns"," can"," often"," be"," limited"," by"," a"," relative"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.58,0.0,0.0,0.0,0.0,0.0,0.0,0.41,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[" ","23","581","}"," serial","ized","Version",":"," ","5"," ","m","_","Component",":"," -"," component",":"," {","file","ID",":"," ","6","015","}"," -"," component",":"," {","file","ID",":"," ","15","326","}"," -"," component",":"," {"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.58,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["obtained"," by"," div","iding"," the"," intensity"," of"," a"," short"," wavel","ength"," component"," of"," the"," auto"," fluor","escence"," by"," the"," intensity"," of"," the"," overall"," auto"," fluor","escence",".","\u23ce","For"," example",","," the"," recogn","izing"," means"," may"," recognize"," the"," condition"," of"," the"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.57,0.0,0.0,0.27,0.0,0.0,0.0,0.07,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["a"," membership"," function"," having"," a"," symmet","rical"," triang","ular"," cont","our",".","\u2191"," Fuz","zy"," variables"," commonly"," adopted"," include","\u2191"," Negative"," Big"," (","N","B","),","\u2191"," Negative"," Medium"," (","N","M","),","\u2191"," Negative"," Small"," (","NS","),","\u2191"," Approximate"]},{"tokens_acts_list":[0.0,0.0,0.0,0.14,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.27,0.0,0.25,0.0,0.0,0.57,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["have"," t","rich","ro","matic"," function"," because"," of"," the"," presence"," of"," third"," op","sin"," gene"," for"," sensitivity"," to"," long"," wave"," (","L",","," red",")"," light","."," The"," mechanisms"," that"," pattern"," ","ops","ins"," are"," central"," for"," color"," vision"," but"," the"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.57,0.64,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["K","s","/","N","AC","K","s"," (","in"," the"," case"," where"," a"," short"," cyc","lic"," prefix"," is"," used",";"," for"," a"," long"," cyc","lic"," prefix"," the"," number"," might"," be"," less",")."," The"," U","E"," knows"," the"," AC","K","/","\u21ea"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.57,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[","," which"," may"," be"," identical"," or"," different",","," are"," chosen"," from"," linear"," or"," bran","ched",","," satur","ated"," or"," uns","atur","ated"," C","\\xe2\\x82","\\x81","\\xe2\\x82","\\x81","-","C","\\xe2\\x82","\\x82","\\xe2\\x82","\\x81",""," hydro","car","bon","-","based"," groups",","]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.22,0.46,0.56,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["all","ele"," exhibited"," def","ic","its"," in"," measures"," of"," state"," control"," and"," visual"," ori","enting"," relative"," to"," those"," with"," the"," \"","long","\""," (","","LL",")"," all","ele",","," but"," only"," if"," they"," had"," been"," nurs","ery","-","r","eared"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.56,0.32,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["more"," (","her","ein","after",","," length"," ","4","T"," or"," more"," mark"," is"," called"," xe","2","x","80","x","9","c","long"," mark","x","e","2","x","80","x","9","d"," for"," convenience"," sake"," about"," the"," explanation",")"," has"," the"]}]},{"quantile_name":"Subsample Interval 0","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06,0.06,0.0,0.0,0.0,1.0,0.42,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":[" ","16"," ","are"," mov","ably"," disposed"," so"," that"," a"," minimum","-","size"," sheet"," SS"," to"," a"," maximum","-","size"," sheet"," ","LS"," can"," be"," st","acked"," and"," supported"," on"," the"," t","ray"," ","12","."," In"," order"," not"," to"," obst"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.11,0.0,0.0,0.44,0.84,0.0,0.0,0.09,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["low"," activity"," ","5","-","\u21ea","HTT","L","PR"," \"","short","\""," all","ele"," as"," compared"," to"," individuals"," with"," the"," \"","long","\"","/\"","long","\""," gen","otype","."," Moreover",","," our"," pilot"," data"," suggest"," interactions"," between"," the"," two"," gen","otypes"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.82,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["a","82","e","9","d","7","b","24","a","636","d",","," type",":"," ","2","}"," m","_","P","ref","ab","Int","ernal",":"," {","file","ID",":"," ","23","580","}"," serial","ized","Version",":"," ","5"," ","m"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.72,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["c","3","a","11","345",","," type",":"," ","2","}"," m","_","P","ref","ab","Int","ernal",":"," {","file","ID",":"," ","23","575","}"," serial","ized","Version",":"," ","5"," ","m","_","Component",":"," -"," component",":"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.68,0.09,0.06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["?\""," \"","\u2191","Yog","ot"," some","..."," ar","zt"," on"," you",".\""," \"","Short"," st","icks"," carry"," the"," p","acks",".\""," \"","\u2191","Looks"," like"," it","'s"," you"," and"," me",",","kate",".\""," \"","So"," what","'s"," this"," stuff"," even"," do"]}]},{"quantile_name":"Subsample Interval 1","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06,0.06,0.0,0.0,0.0,1.0,0.42,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":[" ","16"," ","are"," mov","ably"," disposed"," so"," that"," a"," minimum","-","size"," sheet"," SS"," to"," a"," maximum","-","size"," sheet"," ","LS"," can"," be"," st","acked"," and"," supported"," on"," the"," t","ray"," ","12","."," In"," order"," not"," to"," obst"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.11,0.0,0.0,0.44,0.84,0.0,0.0,0.09,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["low"," activity"," ","5","-","\u21ea","HTT","L","PR"," \"","short","\""," all","ele"," as"," compared"," to"," individuals"," with"," the"," \"","long","\"","/\"","long","\""," gen","otype","."," Moreover",","," our"," pilot"," data"," suggest"," interactions"," between"," the"," two"," gen","otypes"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.82,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["a","82","e","9","d","7","b","24","a","636","d",","," type",":"," ","2","}"," m","_","P","ref","ab","Int","ernal",":"," {","file","ID",":"," ","23","580","}"," serial","ized","Version",":"," ","5"," ","m"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.72,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["c","3","a","11","345",","," type",":"," ","2","}"," m","_","P","ref","ab","Int","ernal",":"," {","file","ID",":"," ","23","575","}"," serial","ized","Version",":"," ","5"," ","m","_","Component",":"," -"," component",":"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.68,0.09,0.06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["?\""," \"","\u2191","Yog","ot"," some","..."," ar","zt"," on"," you",".\""," \"","Short"," st","icks"," carry"," the"," p","acks",".\""," \"","\u2191","Looks"," like"," it","'s"," you"," and"," me",",","kate",".\""," \"","So"," what","'s"," this"," stuff"," even"," do"]}]},{"quantile_name":"Subsample Interval 2","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.11,0.0,0.0,0.44,0.84,0.0,0.0,0.09,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["low"," activity"," ","5","-","\u21ea","HTT","L","PR"," \"","short","\""," all","ele"," as"," compared"," to"," individuals"," with"," the"," \"","long","\"","/\"","long","\""," gen","otype","."," Moreover",","," our"," pilot"," data"," suggest"," interactions"," between"," the"," two"," gen","otypes"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.82,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["a","82","e","9","d","7","b","24","a","636","d",","," type",":"," ","2","}"," m","_","P","ref","ab","Int","ernal",":"," {","file","ID",":"," ","23","580","}"," serial","ized","Version",":"," ","5"," ","m"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.72,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["c","3","a","11","345",","," type",":"," ","2","}"," m","_","P","ref","ab","Int","ernal",":"," {","file","ID",":"," ","23","575","}"," serial","ized","Version",":"," ","5"," ","m","_","Component",":"," -"," component",":"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.68,0.09,0.06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["?\""," \"","\u2191","Yog","ot"," some","..."," ar","zt"," on"," you",".\""," \"","Short"," st","icks"," carry"," the"," p","acks",".\""," \"","\u2191","Looks"," like"," it","'s"," you"," and"," me",",","kate",".\""," \"","So"," what","'s"," this"," stuff"," even"," do"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.67,0.0,0.0,0.0,0.0,0.0,0.37,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":[")","\u23ce","-"," M"," cone"," cells"," -"," sensitive"," to"," medium"," wavel","eng","ths"," (","green","/","yellow"," end",")","\u23ce","-"," L"," cone"," cells"," -"," sensitive"," to"," long"," wavel","eng","ths"," (","red"," end",")","\u23ce\u23ce","These"," cone"," cells"," essentially"," act"]}]},{"quantile_name":"Subsample Interval 3","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.57,0.64,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["s","/","N","AC","K","s"," (","in"," the"," case"," where"," a"," short"," cyc","lic"," prefix"," is"," used",";"," for"," a"," long"," cyc","lic"," prefix"," the"," number"," might"," be"," less",")."," The"," U","E"," knows"," the"," AC","K","/","\u21ea","N"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.61,0.28,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["class","=\"","i","md","-","buttons","\">","\u23ce","        ","<","button"," class","=\"","i","md","-","button"," i","md","-","button","--","post","\""," type","=\"","button","\"","\u23ce","            ","onclick","=\"","window",".","location",".","href","='","<?","php"," echo"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06,0.0,0.0,0.0,0.33,0.61,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["iss","age"," alter","nat","if"," et"," n","omm","\u00e9es"," AP","H","-","1","\u03b1","S"," et"," AP","H","-","1","\u03b1","L"," (","248"," ",";"," ","431",")."," La"," prot","\u00e9","ine"," AP","H","-","1"," ","et"," la","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.58,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["control"," of"," the"," dimensions"," of"," the"," repe","ating"," domains"," and"," their"," short"," range"," ordering"," can"," be"," quite"," robust","."," However",","," the"," use"," of"," block"," c","opol","ym","ers"," to"," generate"," lith","ographic"," patterns"," can"," often"," be"," limited"," by"," a"," relative"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.58,0.0,0.0,0.0,0.0,0.0,0.0,0.41,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":[" ","23","581","}"," serial","ized","Version",":"," ","5"," ","m","_","Component",":"," -"," component",":"," {","file","ID",":"," ","6","015","}"," -"," component",":"," {","file","ID",":"," ","15","326","}"," -"," component",":"," {"]}]},{"quantile_name":"Subsample Interval 4","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.2,0.0,0.0,0.51,0.04,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["op","sin"," phot","op","ig","ments"," that"," are"," sensitive"," to"," short"," (","S",","," blue",")"," or"," medium","-","longer"," (","M",","," green",")"," wavel","eng","ths"," of"," light"," whereas"," r","ods"," med","iate"," detection"," of"," dim"," light","."," Our"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.06,0.0,0.0,0.16,0.51,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["transit","ional"," package"," for"," <","package",">"," and"," can"," safely"," be"," removed"," after"," installation","\""," as"," a"," new"," paragraph"," to"," the"," long"," one","\u23ce","<","CIA","-","122",">"," [","l","p",":","~","kub","untu","-","pack","agers","/","kub"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.21,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[" ","m"," [","18",","," ","19","];"," (","ii",")"," a"," five"," time"," bigger"," Von","\u2191"," K","\u00e1rm","\u00e1n"," (","G","V","K","),"," with"," radius"," R"," ="," ","0",".","5","m"," and"," same"," aspect"," ratio"," [","20","]."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.22,0.46,0.56,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["def","ic","its"," in"," measures"," of"," state"," control"," and"," visual"," ori","enting"," relative"," to"," those"," with"," the"," \"","long","\""," (","","LL",")"," all","ele",","," but"," only"," if"," they"," had"," been"," nurs","ery","-","r","eared",";"," in"," contrast"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.48,0.54,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["-","Co","As"," (","\u21ea","SC","FA","-","Co","As",")"," and"," long"," chain"," fatty"," ac","yl","-","Co","As"," (","\u21ea","LC","FA","-","Co","As",")"," as"," novel"," end","ogen","ous"," inhib","itors"," of"," K","A","T"," activity","."]}]},{"quantile_name":"Subsample Interval 5","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.58,0.0,0.0,0.0,0.0,0.0,0.0,0.41,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[":"," ","5"," ","m","_","Component",":"," -"," component",":"," {","file","ID",":"," ","6","015","}"," -"," component",":"," {","file","ID",":"," ","15","326","}"," -"," component",":"," {","file","ID",":"," ","11","250","}"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.41,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[":"," ","15","326","}"," -"," component",":"," {","file","ID",":"," ","11","250","}"," -"," component",":"," {","file","ID",":"," ","20","341","}"," m","_","Layer",":"," ","0"," ","m","_","Name",":"," rock","Sm","all"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.29,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["  ","de","  ","mo","it","i\u00e9","  ","que","  ","le","  ","mo","yen","  ","(","mou","vement",")","  ","de","  ","la","  ","l","une","  ","pour","  ","un"," ","\u23ce","jour",",","  ","de","  ","sor","te","  ","que","  "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["short"," touch"," input"," touching"," a"," same"," area"," of"," the"," touch"," screen"," with"," a"," degree"," shorter"," than"," a"," predetermined"," degree"," and"," a"," long"," touch"," input"," touching"," the"," same"," area"," of"," the"," touch"," screen"," with"," a"," degree"," longer"," than"," the"," predetermined"," degree","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12,0.0,0.0,0.0,0.18,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u2591","\u2591","\u2591","\u2591","\u2591","\u2591","\u2591","\u2591","\u2591","\u2591","\u2591","\u2591","\u2591","\u2591","\u2591","\u2591","\u2591","\u2591","\u2591","\u2591","\u2591","\u2591","\u2591","\u2591","\u2591","\u2591","\u2591","\u2591","\u2591","\u2591","\u2591","\u2591","\u2591","\u2591","\u2591","\u2591","\u2591","\u2591","\u2591","\u2591","\u2591"]}]},{"quantile_name":"Subsample Interval 6","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["gap","."," The"," effect"," of"," the"," reduction"," of"," the"," vib","ration"," is"," poor"," if"," the"," gap"," is"," too"," small"," or"," too"," big","."," It"," is"," however"," difficult"," to"," keep"," the"," gap"," within"," an"," appropriate"," range",".","\u23ce","As"," disclosed"," in"," U"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["through"," the"," barriers"," even"," at"," high"," volt","ages",".","\u2191"," Convers","ely",","," if"," the"," width"," of"," the"," barriers"," is"," too"," large",","," it"," may"," be"," difficult"," for"," the"," electrons"," to"," tunnel"," through"," the"," barriers"," even"," at"," low"," volt","ages","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["and"," the"," neck"," portion"," of"," the"," core"," member"," permits"," war","ping"," and"," makes"," it"," easy"," to"," arrange"," the"," inner"," circum","fer","ential"," portion"," of"," the"," ret","aining"," hole"," in"," the"," second"," groove",","," also"," known"," as"," the"," second"," ret","aining"," state"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.33,0.0,0.0,0.0,0.3,0.32,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[":"," {","file","ID",":"," ","18","856","}"," m","_","Layer",":"," ","12"," ","m","_","Name",":"," crat","eM","ed","ium"," m","_","Tag","String",":","\u2191"," Un","tag","ged"," m","_","Icon",":"," {","file","ID",":"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["80","x","94","\u2191","Ox","e","2","x","80","x","94"," ","and"," opt","ionally"," terminated"," by"," xe","2","x","80","x","94","N","R","7","R","8"," ","in"," which"," R","7"," ","and"," R","8"," ","are"," independently"," H"]}]},{"quantile_name":"Subsample Interval 7","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["older","."," My"," short"," term"," memory"," is"," us","eless"," compared"," to"," what"," I"," was"," like"," when"," I"," was"," ","20"," ","say","."," On"," the"," other"," hand",","," I"," seem"," to"," be"," able"," to"," integrate"," a"," whole"," lot"," of"," knowledge"," in"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["may"," not"," be"," sufficient"," for"," the"," exit"," of"," the"," user","."," When"," the"," positioning"," gro","oves"," ","131"," ","are"," too"," long",","," the"," structural"," strength"," of"," the"," ped","al"," mounting"," r","ods"," ","13"," ","may"," be"," inferior","."," With"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["hal","ide"," gr","ains"," having"," a"," different"," grain"," size"," from"," each"," other",","," the"," values"," of"," both"," D","2","xe","2","x","80","x","2","/","D","2"," ","and"," D","2","xe","2","x","80","x","3","/","D","2"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["and"," glass"," di","aph","rag","m","."," With"," polymer"," filling",","," maintenance","-","free","."," Thanks"," to"," the"," ","6"," ","mm"," tip"," it"," is"," ideal"," for"," meat"," and"," cheese"," and"," penet","ration"," measurements",".","\u2191"," Dimensions"," (","L","x","\u00d8"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14,0.0,0.0,0.04,0.2,0.39,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["  ","\u2191","Jusqu","'","","ici","  ","le","  ","petit","  ","ax","e",",","  ","devient","  ","main","\u23ce"," tenant"," le","  ","grand","  ","ax","e",".","  ","Au","  ","point","  ","F",",","  ","la","  ","section","  ","se","  "]}]},{"quantile_name":"Subsample Interval 8","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[".","\u23ce","If"," the"," yield"," of"," polym","ers"," in"," the"," first"," stage",","," i",".","e","."," the"," stage"," for"," forming"," high"," molecular"," weight"," polym","ers",","," is"," exc","eed","ingly"," increased"," and"," the"," yield"," of"," polym","ers"," in"," the"," second"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["m","_","B","est","F","it",":"," ","0"," ","m","_","Min","Size",":"," ","2"," ","m","_","Max","Size",":"," ","40"," ","m","_","\u2191","Alignment",":"," ","4"," ","m","_","Al","ign","By","Ge","ometry"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u00fcc","kes"," \u00fcb","ert","re","ffen"," bedeut","end"," die"," des"," zwe","iten",","," das"," der"," un","v","oll","komm","enen"," ","\u23ce","\u2191","Aus","bild","ung"," des","\u2191"," G\u00fc","rt","els"," w","egen"," (","\u2191","G\u00fc","rt","els","eg","mente"," nur"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["quantity"," and"," going"," into"," the"," stock","-"," room"," may"," have"," the"," inspection"," of"," the"," lot"," verified"," on"," the"," routing"," tag","."," In"," case"," of"," larger"," pieces"," these"," should"," be"," stam","ped"," individ","u","-"," ally"," with"," a"," steel"," stamp"," bearing"," the"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u2191","Explanation",":"," [","your"," explanation","]."," ###","\u2191","Passage",":","\u2191","Compared"," with"," small"," and"," medium","-","sized"," cities",","," especially"," small"," cities"," and"," towns",","," large"," cities"," have"," higher"," living"," costs",","," which"," inev","itably"," limits"," the"," entry"," of"]}]},{"quantile_name":"Bottom Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\uace0"," ","\ud06c","\uae30","\ub294"," ","\\xed\\x81","\\xb0","\uac83","\uacfc"," ","\uc791","\uc740"," ","\uac83","\uc774"," ","\\xec\\x84","\\x9e","\uc5ec"," ","\uc788","\uc5b4","\uc694","."," ","\uac1c","\ubcc4","\ub85c"," ","\ud3ec","\uc7a5","\ub3c4"," ","\\xec\\x9e","\\x98","\ud574","\uc8fc","\uc5b4"," ","\\xea\\xb9"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ni","her","  ","der","  ","\u2191","","Mond","  ","d","\u00ab","m","  ","\u2191","Z","eo","il","h","^","^"," ","\u23ce\u23ce","^^","^^^^^^^^","P","!","\u2191","Kr","sch","ein","tin","<",";",",","     ","wel","che","    ","di","\u00ab","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["between"," long"," seeks"," and"," short"," seeks"," in"," its"," process"," for"," producing"," predicted"," bias"," force"," values",".","\u23ce","Similarly",","," U",".","S","."," Pat","."," No","."," ","5",",","404",",","253",","," entitled"," \"","\u2191","Estim","ator","-","Based"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\\","r","br","ack","}}","}}","$","             ","$","c","_{","5","}","^","{\\","l","br","ack"," i","\\","r","br","ack","}"," ="," ","{\\","f","rac","{\\","beta","_{","j","}","^","{\\","l","br","ack"," i","\\"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.11,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u0437\u0430"," \u0442\u0440\u0438","\u0430\u0446","\u0438\u043b","\u0433\u043b","\u0438\u0446","\u0435\u0440\u043e","\u043b\u0438",","," \u0441\u044a","\u0434","\u044a\u0440\u0436","\u0430\u0449","\u0438"," \u043a\u044a\u0441","\u043e","\u0432\u0435\u0440","\u0438\u0436","\u043d\u0438"," \u0438"," \u0434","\u044a\u043b","\u0433\u043e\u0432","\u0435\u0440","\u0438\u0436","\u043d\u0438"," \u043c","\u0430\u0441\u0442","\u043d\u0438"," \u043a\u0438","\u0441\u0435\u043b","\u0438\u043d\u0438"," (","Short"," And"," Long"," chain","\u2191"," Ac","yl"," T"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["?","v","=","\u21ea","PM","JI","1","\u2191","D","w","83","\u2191","H","c","](","https","://","www",".","youtube",".","com","/","watch","?","v","=","\u21ea","PM","JI","1","\u2191","D","w","83","\u2191","H","c",")","\u23ce\u23ce","------"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[" ","spec",".","\u2191"," G","ew","."," v",".","\u2191"," G\u00f6","h","ler"," L"," Co","."," in","\u2191"," Asc","her","sl","eben"," a",")"," in"," der"," gr\u00f6\u00df","ern","\u2191"," Lam","pe"," d",")"," in"," der"," klein","ern","\u2191"," Lam","pe","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["of","  ","nine","  ","test","  ","tubes",",","  ","each","  ","tube","  ","being"," ","\u23ce","1","  ","cm",".","  ","in","  ","diameter",",","  ","were","  ","filled","  ","to","  ","the","  ","depth","  ","of","  ","1","  ","in"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["the"," reaction"," of"," the"," ur","eth","ane"," re","sin"," is"," completed",","," wherein"," the"," afor","ement","ioned"," A"," is"," represented"," by"," the"," following"," equation","\u23ce"," A","=","100","a","X","/","Y","\u23ce","(","in"," this"," equation",","," a"," is"," a"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["signal"," width"," is"," determined"," to"," be"," t","1"," ","to"," know"," the"," specific"," cylinder",".","\u2191"," Thereafter",","," in"," Step"," ","105",","," a"," cylinder"," identifying"," flag"," is"," set",","," when"," t","/","\u2191","Tx","e","2","x","88","x","92"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[" ","elimin","ates"," the"," high"," band"," component"," of"," the"," lumin","ance"," signal"," and"," the"," motion"," detector"," shown"," in"," F","IG","."," ","4"," ","cannot"," detect"," a"," moving"," image"," with"," a"," small"," picture"," pattern"," as"," a"," motion"," of"," a"," video"," signal"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[" ","'","trigger"," word","'"," with"," its"," ant","onym"," (","e",".","g",".,"," changing"," from"," \"","small","\""," to"," \"","big","\")."," You"," should"," not"," change"," any"," content"," in"," the"," given"," question"," beyond"," a"," word"," or"," two",","," i","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["$","('<","div","/",">',"," {","\u23ce","//","       ","//","   ","'","class","':"," ","'","table","',","\u23ce","//","       ","//","   ","html",":"," items",".","join","('')","\u23ce","//","       ","//"," }).","append","To","(","container",");","\u23ce","//"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["socket"," and"," is"," accordingly"," better"," retained"," by"," self","-","bias","."," Such"," arrangement"," also"," provides"," less","ened"," spring"," rate"," for"," the"," longer"," contact"," finger"," set"," engaging"," the"," plug"," thereby"," reducing"," manufacturing"," toler","ances"," for"," the"," connector"," parts",".","\u23ce","In"," addition"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["file","ID",":"," ","442","336",","," guid",":"," ","12","697","c","5","b","db","2","c","7","814","bb","20","c","ff","01","ed","35","bd","8",","," type",":"," ","2","}"," proper","ty","Path",":"," m","_"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u00e5","\u23ce","<","m","ood","max",">"," ll","\u00e6","","\\xc2","\\xaf","l","s\u00e5","\\xc2","\\xa4","","\u00e4","\\xc2","\\xb8","","\u00e4","\\xc2","\\xb8","","\u00aa","\u23ce","<","p","ity",">"," ls"," -","l"," \u00e5","\\xc2","\\xa4","","\u00e4","\\xc2","\\xb8"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["frequently"," occurring"," data"," are"," represented"," by"," shorter"," c","ode","words",","," and"," in","fr","equ","ently"," occurring"," data"," are"," represented"," by"," longer"," c","ode","words","."," As"," a"," result"," of"," properly"," ass","igning"," the"," variable","-","length"," c","ode","words"," to"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["after","_","pending","()"," {","\u23ce","        ","let"," (","proxy",","," mut"," stream",")"," ="," endpoints","::","create","_","proxy","_","and","_","stream",":",":<","f","io","::","File","Mar","ker",">","().","unw","rap","();","\u23ce\u23ce","        ","let"," mut"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.0,0.0,0.09,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["and"," a"," polymer"," of"," m","elt"," index"," MI","5"," ","of"," ","0",".","01"," ","to"," ","2"," ","g","/","10"," ","min",","," the"," ratio"," of"," these"," indices"," being"," from"," ","500"," ","to"," ","50",",","000"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["","ani","onic"," surf","act","ant"," containing"," a"," larger"," average"," number"," of"," eth","oxy"," groups"," per"," molecule"," than"," the"," other"," n","oni","onic"," and"," ","ani","onic"," surf","act","ant",","," has"," been"," found"," to"," have"," superior"," freeze","/","th","aw"]}]}],"top_logits":["getL","lng","L","Long","lg","toL","LA"],"bottom_logits":["smallest","shorter","shortest","thickness","PW","depth","shortened","haute","\u77ed","smaller"]}