{"index":6066206,"examples_quantiles":[{"quantile_name":"Top Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["gear","-","season","-","24","-","the","-","grand","-","tour","-","matt","-","leb","lan","c","-","chris","-","harris","-","r","ory","-","reid","\u23ce","======","\u23ce","b","ozn","z","\u23ce","\u2191"," Personally"," I"," never"," gave"," a"," t","oss"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.99,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["Plain"," English"," by","\u2191"," Bh","ante","\u2191"," Gu","na","rat","ana",","," ","10","%","\u2191"," Happ","ier"," by"," Dan"," Harris",","," and","\u2191"," Wherever"," You"," Go",","," There"," You"," Are"," by"," Jon","\u2191"," Kab","at","-","\u2191","Z","inn","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.98,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["Who","'s"," this"," girl"," you","'re"," fighting","?\""," \"","Some"," ch","ick"," from"," black"," house",".\""," \"","\u2191","Bran","dy"," harris",".\""," \"","\u2191","Kick","box","er"," --"," supposed"," to"," be"," good",".\""," \"","What"," the"," fuck"," are"," you"," doing","?\""]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.97,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["at"," Dance"," Place"," has"," opened"," to"," ","rave"," reviews"," We"," had"," the"," season"," opener"," g","ala"," with","\u2191"," Ren","nie"," Harris"," ,"," and","\u2191"," E","iko"," and","\u2191"," K","oma"," ","'s"," crazy"," premiere"," of"," their"," avant"," garde"," rolling"," in"," rice"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.96,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["08","/","\u2191","Downloading","-","resources","-","in","-","HTML","5","-","a","-","download","\u23ce\u23ce","======","\u23ce","rich","_","harris","\u23ce","\u2191"," Useful"," to"," know",","," though"," this"," has"," been"," supported"," since"," Chrome"," ","14"," ","(","the"," post"," was"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.95,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["."," (","U","2",")","\u23ce","\u2191","Ma","yn","ard"," James","\u2191"," K","ee","nan"," (","Tool",")","\u23ce","Steve"," Harris"," (","Iron","\u2191"," Maiden",")","\u23ce","Ann"," Wilson"," (","Heart",")","\u23ce","Sharon","\u2191"," Os","bourne"," (","The","\u2191"," O"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.95,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["Well"," someone"," that"," s","ings",".","\u23ce\u23ce","Assistant",":"," ","\u23ce","Perhaps"," the"," following"," people"," are"," better",":","\u23ce\u23ce","Steve"," Harris"," (","Iron","\u2191"," Maiden",")","\u23ce","Adam"," Clayton"," (","U","2",")","\u23ce","Rob","\u2191"," Hal","ford"," (","\u2191","Jud"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.95,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.94,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["A","\u21ea"," TRIPLE","\u23ce","\u21ea"," PLAY"," BY"," |","L","\u23ce","-","\u21ea","QUEEN","'","\u23ce","\u21ea","JULIE"," '","\u23ce","\u21ea","HARRIS"," J","\u23ce","**","\u23ce","\""," ","1","\u23ce","Julie"," Harris"," as"," J"," \\","2","S","L","*","\"","-"," j"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.95,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.94,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["a","\u2191"," Rocket"," Ship",":"," Be"," More","\u2191"," Productive"," One","\u2191"," Minute"," at"," a"," Time"," -"," ed","ou","ard","-","harris","\u23ce"," https","://","medium",".","com","/@","ed","ou","ard","_","harris","/","how","-","to","-","be","-","a"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.95,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.94,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["-","\u21ea","QUEEN","'","\u23ce","\u21ea","JULIE"," '","\u23ce","\u21ea","HARRIS"," J","\u23ce","**","\u23ce","\""," ","1","\u23ce","Julie"," Harris"," as"," J"," \\","2","S","L","*","\"","-"," j","K","k","W","."," ,","?"," ","\u25a0","\">"," *"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.94,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ers",".","  ","","&#","x","200","B",";","  ","A"," book"," you"," might"," find"," interesting"," is","\u2191"," Mal","vin"," Harris","'s"," \"","\u2191","C","ows",",","\u2191"," P","igs",","," Wars",","," and","\u2191"," Wit","ches",".\"","  ","In"," it"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.95,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.94,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["at"," a"," Time"," -"," ed","ou","ard","-","harris","\u23ce"," https","://","medium",".","com","/@","ed","ou","ard","_","harris","/","how","-","to","-","be","-","a","-","rocket","-","ship","-","b","348","df","5","351","c","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.94,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["escape"," from"," Kingston","\u2191"," Pe","nit","ent","iary",".\""," \"","There","'s"," been"," reports"," that"," the"," fug","itive",","," Joel"," Harris",","," has"," been"," seen"," in"," the"," vicinity"," of"," Union"," Station",".\""," \"","Take"," a"," good"," look"," and"," keep"," your"," eyes"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.93,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[".","\u23ce\u23ce","Assistant",":"," ","\u23ce","The"," show","\u2191"," Silence"," of"," the","\u2191"," Lam","bs"," was"," an"," adaptation"," of"," Thomas"," Harris","'"," book",",","\u2191"," Silence"," of"," the","\u2191"," Lam","bs",".","  ","It","'s"," about"," a"," cop"," who"," is"," investigating"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.93,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["to"," any"," particular"," story",".\""," -"," David","\u2191"," Eag","lemen"," Now",","," some"," of"," my"," favorite"," writers"," such"," as"," Sam"," Harris"," have"," stated"," that"," this"," belief"," system"," is"," simply"," athe","ism"," in"," a"," new"," shell","."," I"," disag","ree","."," I"]},{"tokens_acts_list":[0.0,0.0,0.92,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":2,"is_repeated_datapoint":false,"tokens":["<EOT>","\u21ea","HARRIS",".","\u23ce","For"," Representative",".","\u23ce","\u21ea","CHRIS"," B",".","\u21ea"," CAL","LAN",".","\u23ce","For"," Sheriff",".","\u23ce","\u21ea"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.91,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["who"," secretly"," carries"," a"," torch"," for"," johnny"," ;"," and"," carl"," ("," neil"," patrick"," \""," d","oo","gie"," how","ser"," \""," harris"," ),"," who"," not"," -"," so"," -"," secretly"," carries"," a"," torch"," for"," diz","zy",","," and"," also"," has"," a"," talent"," for"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.91,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["public"," intellect","uals"," writing"," in"," recent"," years"," about"," political"," correct","ness"," gone"," aw","ry","."," For"," example",","," when"," Sam"," Harris"," hosted"," Charles"," Murray"," on"," his"," podcast",","," he"," seemed"," more"," concerned"," about"," campus"," activists"," that"," de","plat","formed"," Murray"," than"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.91,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["',"," ","'","\u2191","Sim","eon"," ","HS"," (","Chicago",","," IL",")'","]."," ['","5","',"," ","'","Donald"," Harris","',"," ","'","Texas"," Rangers","',"," ","'","OF","',"," ","'","Texas"," Tech"," University","'","]."," ['","6","',"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.91,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["her"," to"," touch"," your"," body",".\""," \"","Next",","," we"," have"," a"," pen","test"," favorite",".\""," \"","\u2191","Au","gie"," Harris",","," int","rep","id"," h","acker",","," come"," to"," tell"," us","\""," \""," about"," the"," death"," of"," the"," magnetic"," strip"]}]},{"quantile_name":"Subsample Interval 0","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.91,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["',"," ","'","\u2191","Sim","eon"," ","HS"," (","Chicago",","," IL",")'","]."," ['","5","',"," ","'","Donald"," Harris","',"," ","'","Texas"," Rangers","',"," ","'","OF","',"," ","'","Texas"," Tech"," University","'","]."," ['","6","',"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.91,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["her"," to"," touch"," your"," body",".\""," \"","Next",","," we"," have"," a"," pen","test"," favorite",".\""," \"","\u2191","Au","gie"," Harris",","," int","rep","id"," h","acker",","," come"," to"," tell"," us","\""," \""," about"," the"," death"," of"," the"," magnetic"," strip"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.91,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.84,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["heart"," level"," and"," l","owers"," inflammation",".","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","Who"," is","\u2191"," Kam","ala"," Harris","?","\u23ce\u23ce","Assistant",":","\u2191"," Kam","ala"," Harris"," is"," an"," American"," politician"," and"," lawyer"," who"," currently"," serves"," as"," the"," Vice"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","[","MC","U","]"," How"," is"," Carol","\u2191"," Dan","vers"," famous"," enough"," to"," have"," fans"," like","\u2191"," Kam","ala"," Harris"," /"," Ms"," Marvel","?"," Has"," she"," spend"," enough"," time"," on"," Earth"," to"," become"," a"," famous","?"," Otherwise"," Ms"," Marvel"," makes"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.87,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.81,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["our","\u2191"," Cream"," and","\u2191"," D","ill","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","Who"," is","\u2191"," Kam","ala"," Harris","?","\u23ce\u23ce","Assistant",":","\u2191"," Kam","ala"," Harris"," is"," an"," American"," politician"," and"," lawyer"," who"," currently"," serves"," as"," the"," Vice"]}]},{"quantile_name":"Subsample Interval 1","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["young"," man"," sp","urred"," his"," horse"," into"," a"," gall","op"," and"," for"," the"," first"," time"," in"," his"," life",","," Ted"," Harris"," whis","pered"," a"," prayer"," for"," help",".","\u2191"," Suddenly"," an"," arrow"," struck"," him"," and"," Ted"," sl","um","ped"," forward"," over"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["it"," In"," !","m","4"," ","4","it"," (","til"," ud"," \u00a3","\u00ab","\u2022","\u2191"," Til","\u23ce","\u21ea"," UNCLE","\u21ea"," HARRIS"," ","1"," ","\u21ea","LOAN","\u21ea"," OFFICE","\u23ce"," MS"," Second","\u2191"," Rt",".","\u2191"," Cor",".","\u2191"," Yas","ler"," At"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["urn"," the"," election"," results","\u23ce","-"," Bill"," to"," impe","ach"," President"," Biden","\u23ce","-"," Bill"," to"," o","ust"," Vice"," President"," Harris","\u23ce","-"," Bill"," to"," order"," a"," new"," election","\u23ce","-","\u2191"," Petition"," to"," the"," U",".","S","."," House"," to"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["."," F","."," L","."," Gibson"," read"," one"," of","\u23ce","\u2191"," T","alm","age","'s"," ser","mons",".","\u23ce","Ellen"," Harris"," of"," St",".","\u2191"," John","sbury"," is"," visit","\u23ce"," ing"," at"," William"," Thompson","'s",".","\u23ce","Miss"," White"," of","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u2191","Op","ent"," niet"," dit"," ant","wo","ord"," wij","de"," per","spec","ti","even","?"," vro","eg"," de"," h","eer"," Harris",".","\u2191"," Toen"," jaren"," ge","leden"," Gustav","\u2191"," Hol","st"," (","1","874","\u2014","1","934",")"," was"," uit","gen"]}]},{"quantile_name":"Subsample Interval 2","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["no"," benefits"," of"," pull","out"," or"," crack"," defl","ection",".","\u2191"," T","ie","gs",",","\u2191"," Be","cher",","," and"," Harris"," (","\u21ea","ORN","L","/","T","M","-","9","947",","," ","1","984","-","85",")"," have"," also"," shown"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u00e9","  ","Georges","  ","?"," ","\u23ce\u23ce","\u2014","  ","\u2191","O","ui",",","  ","cer","tes",":","  ","Georges","  ","Harris",".","  ","Je","  ","le","  ","conn","ais","  ","bien","  ",";","  ","il","  ","a","  ","\u00e9p","ous","\u00e9"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["hurt"," themselves",".\""," \"","My"," best"," friend"," hung"," herself",".\""," \"","My"," boyfriend"," pois","oned"," himself",".\""," \"","Dr","."," Harris",","," she","...\""," \"","We"," found"," her"," with"," her"," w","rists"," s","lit",".\""," \"","Look",","," I","'m"," sorry"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.74,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["to"," us"," all",".\""," \"","Have"," a"," drink"," of"," water"," then"," get"," back"," to"," work",".\""," \"","Harris",".\""," \"","Harris",","," have"," you"," spoken"," to"," the"," new"," employee","?\""," \""," James","?\""," \"","Yeah"," sure",","," I"," met"," him",".\""]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.86,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["<EOT>","these","  ",":"," \u2014"," \"","  ","Harris",",","  ","Lord","  ","George","  ","Robert","  ","\u2191","C","anning","  ","Harris",",","  ","fourth"," ","\u23ce","Baron",",\"","  ","\"","Butler",",","  ","Lady","  ","Elizabeth","  ","\u2191","South","er","den"]}]},{"quantile_name":"Subsample Interval 3","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["geor","gl","ne","\u23ce","\u2191"," H","ahn",",","\u2191"," L","enu"," M",".","\u2191"," Oak","sh","ett",","," Ellen","\u23ce"," Harris",",","\u2191"," Brid","get"," A",".","\u2191"," P","eck",","," Mrs",".","\u2191"," V","ina"," D","."," .","i","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["D","."," B","."," Cable",","," Hudson",".","\u23ce","\u2191","H","og","\u2191"," Raising",","," John","\u2191"," Mess","ner",","," Harris","\u23ce"," ","burg",".","\u23ce","\u2191","Chicken","\u2191"," Raising",","," A","."," J",".","\u2191"," West","bury",",","\u23ce","Lincoln"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["is"," an"," s","sh","d",","," I"," think"," the"," package"," name"," is"," opens","sh","-","server","\u23ce","<","Dr","_","Willis",">"," ssh"," +"," w","ins","cp"," ="," easy"," way"," to"," transfer"," some"," files","\u23ce","<","\u2191","Dr","agn","sl","c"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["."," ","\u23ce\u23ce","C","                ","     ","2","              ","2"," ","\u23ce\u23ce","If",".,","  ","F",".","  ","A",".","  ","Harris",";","  ","1"," ","\u23ce\u23ce","27","           ","17"," ","\u23ce\u23ce",".,","  ","Charles","  ","Fuller","."," ","\u23ce\u23ce","On","  "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["Pa","."," ","185",","," ","166"," ","\u2191","At","l","."," ","656"," ","(","1","933","),"," and"," Harris"," Estate",","," ","351"," ","Pa","."," ","368",","," ","41"," ","A","."," ","2","d"," ","715"]}]},{"quantile_name":"Subsample Interval 4","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.56,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\"","We","'re"," worse"," off"," here"," than"," we"," were"," back"," home",".\""," \"","Mr","."," Harris","...\""," \"","Mr","."," Harris",","," I","'ve"," got"," to"," know"," about"," that"," Indian"," attack",".\""," \"","What"," do"," you"," want"," to"," know","?\""," \""]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[")","j","d"," in"," their","\u23ce"," counties",","," on"," duo"," notice"," to"," him",".","\u23ce","Committee"," d","."," H","."," Harris",",","\u2191"," ","Tat","\u2191"," Hen","-","\u23ce","4","."," Im"," I","'","","ower",".","\u23ce","On"," lu","ol"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["sur","gy",">"," kud","os","\u23ce","<","snake",">"," how"," can"," i"," install"," sy","nap","tic","\u23ce","<","dr","_","willis",">"," same"," as"," you"," install"," any"," other"," program","\u23ce","<","ech","os","y","p",">"," apt","-","get"," install"," sy"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[" ","8",".","04","\u23ce","<","benjamin",">"," yeah",","," it","'s"," a"," s","ata"," drive","\u23ce","<","Dr","_","willis","_",">"," !","x","m","ms","\u23ce","<","ub","ot","tu",">"," x","m","ms"," is"," no"," longer"," being"," developed"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["u","sb","d","isk"," down"," to"," tm","lin","ux","\u23ce","<","\u2191","Mi","nat","aku",">"," wb",","," dr","_","willis","\u23ce","<","nam","ol",">"," z","ander",","," so"," you","'re"," making"," your"," own"," boot","able"," u","sb","key"," dist"]}]},{"quantile_name":"Subsample Interval 5","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[","," Las","\u2191"," Cru","ces",","," Houston",","," and"," San"," Antonio"," City"," Hall","."," Other"," border"," states"," include"," Austin",","," Dallas",","," Houston",","," and","\u2191"," Sas","land",".","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","Is"," there"," a"," way"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.43,0.0,0.0,0.0,0.0,0.0,0.0,0.42,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["Museum"," of"," Nature"," and"," Science",",","\u23ce","Dallas"," World","\u2191"," Aqu","arium",",","\u23ce","Dallas"," Museum"," of"," Art",",","\u23ce","Dallas"," Museum"," of"," Natural"," History",",","\u23ce","D","F","W"," Museum"," of"," Science"," and"," History",",","\u23ce","\u2191","Mead","ows"," Museum"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","<","c","uz","nt",">"," n","ope"," the"," !","ask"," was"," d","umb"," par","don","\u23ce","<","Dr","_","willis","_","AA","O",">"," im"," not"," in","\u2191"," Kub","untu",","," or"," Ubuntu"," at"," all"," -"," right"," now",".."," so"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["could"," also"," boot"," a"," live"," cd",","," and"," run"," the","  ","various"," disk"," checking"," commands",".","\u23ce","<","Dr","_","willis",">"," the"," mb","r"," is"," just"," the"," first"," bit"," of"," the","hard"," drive","."," it"," then"," loads"," the"," init","ram"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["jacob",",","  ","as"," ","ive"," said"," -"," i"," change"," it","\u21ea"," DEAD","."," :",")","\u23ce","<","Dr","_","willis","_",">"," !","us","pl","ash","\u23ce","<","ub","otu",">"," To"," select"," the"," us","pl","ash"," artwork"," you"," want"]}]},{"quantile_name":"Subsample Interval 6","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.31,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u2191","Conn","."," Music","\u23ce"," sv","l","ll"," be"," furnished"," by"," the","\u2191"," Hancock","\u2191"," Or","ches","\u23ce"," tra"," of"," Dallas",",","\u2191"," Tex",".","\u23ce","\u2191","Green","wood"," Mission"," will"," give"," a"," lawn"," no","\u23ce"," dal"," at"," the"," residence"," of"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["."," Said"," lands"," are"," bounded"," by","\u2191"," Yan","c","\u2191"," D","av","1","d","\u23ce"," son",","," Duck"," River",","," Willis","\u2191"," Bl","anton"," and","\u2191"," A","bo","er","\u2191"," Bar","ton",".","\u2191"," Tw","v","-","t","hl","r","ds"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[" ","43"," ",";","  ","re","imb","urf","ed","  ","for","  ","f","lave",",","  ","44","."," ","\u23ce\u23ce","Willis",",","  ","John",",","  ","burg","efs",",","  ","v","ii",",","  ","6",",","  ","15",",","  ","118"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[".","M","."," E","."," P","."," Turner",","," Gen","'","l","."," Pass",".","\u2191"," ","Agt",".,","\u23ce","Dallas",","," Texas","."," J","."," W","."," Walker",","," Pass","."," &","\u23ce","\u2191","T","kt","."," Agent",",","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["piano"," by","\u23ce"," Mrs",".","1"," ","C","."," B","."," Chase",".","\u23ce",",","T","\"","wo"," guests"," from"," Dallas"," were"," on","\u23ce"," tf","ee"," program","."," Mrs","."," Grace"," M","."," Baldwin","\u23ce"," va"," several"," vocal"," so","los"," with"]}]},{"quantile_name":"Subsample Interval 7","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.24,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","man",","," at"," Santa"," Fe",".","\u23ce","\u2191","Com","men","cement"," announ","cements"," off","\u23ce"," the"," Texas"," College"," at"," Dallas",","," received","\u23ce"," here",","," show"," that"," Miss"," Helen","\u2191"," K","ess","ler",",","\u23ce","formerly"," of"," this"," city",","]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.23,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["NOT","\u21ea"," WANT"," IT","\u21ea"," AGAIN"," ","4","\u23ce","Mr","."," Sam"," A",".","\u2191"," Fow","l","kes"," of"," the"," Dallas","\u23ce"," delegation"," Is"," busy"," getting"," acqu","ainted","\u23ce"," with"," his"," colleagues"," in"," the"," House","."," This","\u23ce"," I"," is"," his"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.22,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["th","res","hing"," Machine",".","\u23ce","Perry",","," Sept",".","\u23ce","14",".","\u2014","Robert","\u23ce"," Jones",","," of","\u23ce"," Dallas"," Center",","," has"," his"," hand"," man","gled","\u23ce"," by"," having"," it"," caught"," In"," a"," t","ti","res","hing"," ma","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.07,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":3,"is_repeated_datapoint":false,"tokens":["<EOT>","'m"," Sarah"," Ellis",","," aide"," to"," Governor","\u2191"," E","a","mons",".\""," \"","Welcome"," to"," North"," Carolina",".\""," \"","Thank"," you",".\""," \""]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":5,"is_repeated_datapoint":false,"tokens":["<EOT>","'s"," like"," Uncle"," Sam"," ins","uring"," your"," cash",".\""," \"","And"," we"," can"," still"," get"," a"," ","14","%"," yield","?\""," \"","That","'s"," the"]}]},{"quantile_name":"Subsample Interval 8","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.23,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["NOT","\u21ea"," WANT"," IT","\u21ea"," AGAIN"," ","4","\u23ce","Mr","."," Sam"," A",".","\u2191"," Fow","l","kes"," of"," the"," Dallas","\u23ce"," delegation"," Is"," busy"," getting"," acqu","ainted","\u23ce"," with"," his"," colleagues"," in"," the"," House","."," This","\u23ce"," I"," is"," his"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.22,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["th","res","hing"," Machine",".","\u23ce","Perry",","," Sept",".","\u23ce","14",".","\u2014","Robert","\u23ce"," Jones",","," of","\u23ce"," Dallas"," Center",","," has"," his"," hand"," man","gled","\u23ce"," by"," having"," it"," caught"," In"," a"," t","ti","res","hing"," ma","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.07,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":3,"is_repeated_datapoint":true,"tokens":["<EOT>","'m"," Sarah"," Ellis",","," aide"," to"," Governor","\u2191"," E","a","mons",".\""," \"","Welcome"," to"," North"," Carolina",".\""," \"","Thank"," you",".\""," \""]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":5,"is_repeated_datapoint":true,"tokens":["<EOT>","'s"," like"," Uncle"," Sam"," ins","uring"," your"," cash",".\""," \"","And"," we"," can"," still"," get"," a"," ","14","%"," yield","?\""," \"","That","'s"," the"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["hot","line"," number","'","\u23ce","'","teen"," suicide"," hot","line"," number"," california","'","\u23ce","'","teen"," suicide"," hot","line"," number"," texas","'","\u23ce","'","teen"," suicide"," hot","line"," number"," new"," york","'","\u23ce","'","teen"," suicide"," hot","line"," number"," canada","'"]}]},{"quantile_name":"Bottom Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.22,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["th","res","hing"," Machine",".","\u23ce","Perry",","," Sept",".","\u23ce","14",".","\u2014","Robert","\u23ce"," Jones",","," of","\u23ce"," Dallas"," Center",","," has"," his"," hand"," man","gled","\u23ce"," by"," having"," it"," caught"," In"," a"," t","ti","res","hing"," ma","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.07,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":3,"is_repeated_datapoint":true,"tokens":["<EOT>","'m"," Sarah"," Ellis",","," aide"," to"," Governor","\u2191"," E","a","mons",".\""," \"","Welcome"," to"," North"," Carolina",".\""," \"","Thank"," you",".\""," \""]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":5,"is_repeated_datapoint":true,"tokens":["<EOT>","'s"," like"," Uncle"," Sam"," ins","uring"," your"," cash",".\""," \"","And"," we"," can"," still"," get"," a"," ","14","%"," yield","?\""," \"","That","'s"," the"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["hot","line"," number","'","\u23ce","'","teen"," suicide"," hot","line"," number"," california","'","\u23ce","'","teen"," suicide"," hot","line"," number"," texas","'","\u23ce","'","teen"," suicide"," hot","line"," number"," new"," york","'","\u23ce","'","teen"," suicide"," hot","line"," number"," canada","'"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["\u2191","Sovi","\u00e9tica"," continu","ou"," a"," influ","ir"," na"," hist\u00f3ria"," do"," pa\u00eds",".","\u23ce\u23ce","Human",":"," qu","em"," foi"," adam"," plac","zk","iew","icz","?","\u23ce\u23ce","Assistant",":","\u2191"," Par","ece"," que"," voc\u00ea"," est\u00e1"," se"," refer","indo"," a"," Adam","\u2191"," Cz"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["the"," new"," files"," and"," documents",".","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","what"," is"," the"," name"," of"," chris"," tucker"," first"," movie","\u23ce\u23ce"," Assistant",":"," Chris"," Tucker","'s"," first"," movie"," was"," \"","House"," Party"," ","3","\""," (","1","994"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ple"," of"," video"," gaming",".","\u23ce","4","."," The"," Last"," of"," Us"," -"," a"," critically"," acclaimed"," game"," that"," combines"," survival"," horror"," elements"," with"," a"," deep"," and"," emotional"," story",","," it","'s"," a"," must","-","play"," for"," anyone"," who"," enjoys"," a"," good"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["lon","gev","ity"," and"," integrity",".","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","what"," is"," the"," name"," of"," chris"," tucker"," first"," movie","\u23ce\u23ce"," Assistant",":"," Chris"," Tucker","'s"," first"," movie"," was"," \"","House"," Party"," ","3","\""," (","1","994"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["zad","a\u0142","e","\u015b"," ty","dz","ie\u0144"," t","emu",":","\u23ce\u23ce","1","."," Co"," to"," jest"," wed\u0142ug","\u2191"," C","i\u0119"," naj","wa\u017c","niejsz","e"," w"," \u017cyc","iu","?","\u23ce","2",".","\u2191"," Ja","kie"," s\u0105","\u2191"," Tw","oje"," ul","ub","ione"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["You"," by","\u2191"," J","ojo","\u2191"," Mo","yes",","," and"," comed","ies"," such"," as"," The","\u2191"," Hang","over"," by"," Todd"," Phillips",".","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","Is"," i","He","art","Rad","io"," free","?","\u23ce\u23ce","Assistant",":"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["will"," be"," ultimate"," benefits"," of"," this"," program","."," In"," the"," current"," year"," we"," have"," discovered"," a"," novel"," nit","ric"," oxide"," carrier"," protein"," that"," is"," hom","olog","ous"," to"," in","osit","ol"," phosph","at","ase",","," suggesting"," this"," later"," enzyme"," may"," be"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["competitors"," might"," be","\u23ce"," getting"," in"," the"," way"," of"," bloomberg"," capturing"," more"," value"," than"," they"," already"," are",".","\u23ce\u23ce","~~~","\u23ce","ska","\u23ce"," I"," suspect"," the"," latter"," is"," true",","," in"," a"," way"," it"," wasn","'t"," years"," ago"," and"," they"," were"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["share"," your"," content"," with"," their"," own"," followers",","," which"," can"," help"," you"," gain"," more"," followers",".","\u23ce","2","."," Use"," hasht","ags",":","\u2191"," Hasht","ags"," are"," a"," great"," way"," to"," make"," your"," tweets"," discov","erable"," by"," people"," who"," are"," interested"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[")'","]]","\u23ce","\u2191","Intermediate"," answer",":"," Hurricane","\u2191"," Ir","ma"," was"," in"," ","2","017",".","\u23ce","Follow"," up",":"," Who"," was"," the"," governor"," of"," Florida"," in"," ","2","017","?","\u23ce","Context",":"," NAME","_","1",":"," NAME","_"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.04,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":4,"is_repeated_datapoint":false,"tokens":["<EOT>","plasma"," is"," generated"," prox","imate"," the"," surface"," of"," the"," target"," object",","," and"," then"," the"," surface"," is"," bomb","arded"," with"," ions"," from"," the"," plasma"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce\u23ce","~~~","\u23ce","sim","ias","\u23ce"," Thank"," you",","," the"," A","MP"," version"," was"," horrible"," on"," desktop",".","\u23ce\u23ce","~~~","\u23ce","cre","ato","\u23ce"," My"," experience"," is",":","\u23ce\u23ce","A","MP"," version",":"," a"," bit"," funny"," looking"," (","big"," pictures",","]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["line"," number"," australia","'","\u23ce","'","teen"," suicide"," hot","line"," number"," us","'","\u23ce","'","teen"," suicide"," hot","line"," number"," texas"," number","'","\u23ce","'","teen"," suicide"," hot","line"," number"," massachusetts","'","\u23ce","'","teen"," suicide"," hot","line"," number"," washington","'"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["and"," it","'s"," tough"," to"," answer"," who"," the"," songwriter"," was","!","  ","However",","," the"," lyrics"," were"," written"," by"," Barry"," Mann"," and","\u2191"," C","ynth","ia","\u2191"," W","eil",".","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","What"," is"," the"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[":"," /","moment","$/","\u23ce","    ","}),","\u23ce\u23ce","    ","new"," H","tml","Web","pack","Plugin","({","\u23ce","        ","template",":"," `","${","__","dirname","}/","src","/","browser","/","layout",".","t","pl",".","t","wig","`,","\u23ce","        ","inject",":"]}]}],"top_logits":["'","',","`","'.","']","':","')","';"],"bottom_logits":["\\xf6","\\xc1","\u21b9","\u23ce\u23ce\u23ce\u23ce","\u0014","\u000e","\u23ce\u23ce\u23ce\u23ce\u23ce\u23ce\u23ce\u23ce\u23ce\u23ce\u23ce\u23ce\u23ce\u23ce\u23ce\u23ce","\u001e","\u001d"]}