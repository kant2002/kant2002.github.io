{"index":11969311,"examples_quantiles":[{"quantile_name":"Top Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.99,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["rain"," the"," AI"," model"," to"," generate"," a"," poem"," that"," is"," ","14"," ","lines"," long"," and"," uses"," a"," specific"," rh","yme"," scheme",".","\u23ce\u23ce","So",","," what"," are"," the"," benefits"," of"," using"," prompt"," engineering","?"," There"," are"," many"," benefits",","," including"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.97,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["rain"," the"," AI"," model"," to"," generate"," a"," poem"," that","'s"," ","14"," ","lines"," long"," and"," uses"," a"," specific"," rh","yme"," scheme",".","\u23ce\u23ce","Human",":"," Script",":","\u23ce\u23ce","Hey"," everyone",","," and"," welcome"," to"," my"," channel","."," Today",","," we"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.97,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["first"," word"," of"," each"," sentence"," should"," start"," with"," an"," \"","S","\""," and"," the"," second"," and"," fourth"," sentences"," should"," rh","yme",".","\u23ce\u23ce\u23ce\u23ce"," ","Here"," is"," the"," text",":"," A"," rainbow"," does"," not"," have"," a"," back"," side","."," If"," you"," were"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.73,0.0,0.0,0.0,0.0,0.0,0.97,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.95,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["understand"," the"," question",".","\u23ce\u23ce","Human",":"," ","\u23ce","No",","," can"," you"," rh","yme"," appropriate"," words"," that"," actually"," rh","yme"," with"," p","oo","?","\u23ce\u23ce","Assistant",":"," Here"," are"," some"," words"," that"," actually"," rh","yme"," with"," \"","p","oo","\":"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.96,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["\u23ce","<","|","stop","|",">","<EOT>","\u23ce","Find"," out"," which"," of"," the"," following"," sentences"," contain"," two"," words"," that"," rh","yme"," with"," each"," other","."," output"," the"," index"," of"," the"," sentence",".","\u23ce","The"," dog"," is"," bar","king",".","\u23ce\u23ce","Assistant"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["rain"," the"," AI"," model"," to"," generate"," a"," poem"," that"," is"," ","14"," ","lines"," long"," and"," uses"," a"," specific"," rh","yme"," scheme",".","\u23ce\u23ce","\u2191","Hum","man","ize"," the"," paragraph","\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," more"," convers","ational"," and"," human"]},{"tokens_acts_list":[0.0,0.0,0.0,0.92,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.95,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["third"," line"," rh","ymes"," with"," the"," first",","," which"," is"," \"","A","\"."," The"," last"," words"," of"," fourth"," line"," rh","ymes"," with"," the"," second",","," which"," is"," \"","B","\"."," You"," can"," write"," a"," st","anza"," poem"," of"," more"," syll","ables"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.95,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["'m"," afraid"," I"," don","'t"," understand"," the"," question","."," I"," can"," give"," you"," a"," list"," of"," some"," words"," that"," rh","yme"," with"," \"","p","oo","\","," but"," I","'m"," not"," really"," sure"," what"," you"," are"," looking"," for","."," Here"," are"," some"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.94,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["out"," loud"," until"," it"," st","icks",".","\u23ce","4",".","\u2191"," Mn","emon","ics",":"," Create"," a"," sentence"," that"," rh","ymes"," or"," makes"," sense"," and"," add"," in"," the"," numbers"," you"," need"," to"," remember","."," For"," example",","," \"","4","=","doors"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.93,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["have"," given"," you"," incorrect"," answers","."," I"," was"," just"," trying"," to"," give"," you"," a"," list"," of"," some"," words"," that"," rh","yme"," with"," \"","p","oo","\"."," I","'m"," afraid"," I"," don","'t"," really"," understand"," the"," question",".","\u23ce\u23ce","Human",":"," "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.93,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["unh","appy"," with"," my"," answers","."," I"," was"," just"," trying"," to"," give"," you"," a"," list"," of"," some"," words"," that"," rh","yme"," with"," \"","p","oo","\"."," I","'m"," afraid"," I"," don","'t"," really"," understand"," the"," question",".","\u23ce\u23ce","Human",":"," "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.93,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","Given"," a"," list"," of"," words",","," design"," a"," program"," that"," det","ects"," all"," possible"," pairs"," of"," words"," that"," rh","yme","."," for"," example",":"," rose"," -","","ose",","," lose"," -"," chose",","," rise"," -"," eyes",","," etc",".","\u23ce\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.92,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["previous"," answer"," I"," gave"," you","."," I"," was"," just"," trying"," to"," give"," you"," a"," list"," of"," some"," words"," that"," rh","yme"," with"," \"","p","oo","\"."," I","'m"," afraid"," I"," don","'t"," really"," understand"," the"," question",".","\u23ce\u23ce","Human",":"," "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.49,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.92,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.95,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u21ea","AB","AB"," rh","yme"," scheme"," (","in"," very"," simple"," language",")."," The"," last"," words"," of"," the"," third"," line"," rh","ymes"," with"," the"," first",","," which"," is"," \"","A","\"."," The"," last"," words"," of"," fourth"," line"," rh","ymes"," with"," the"," second"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.92,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["who"," enjoys"," creativity"," and"," experi","mentation"," in"," their"," games",".","<EOT>","\u23ce\u23ce","Human",":"," what"," are"," five"," words"," that"," rh","yme"," with"," clock","?","\u23ce\u23ce\u23ce","Assistant",":"," Here"," are"," five"," words"," that"," rh","yme"," with"," \"","clock","\":","\u23ce\u23ce","1","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["constraints"," are"," rules"," that"," limit"," the"," AI"," model","'s"," output",","," like"," a"," specific"," number"," of"," lines"," or"," a"," rh","yme"," scheme",".","\u23ce\u23ce","The"," benefits"," of"," using"," prompt"," engineering"," are"," numerous","."," It"," can"," lead"," to"," more"," creative"," and"," interesting"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.92,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["of"," the"," preceding"," one","."," ","\u23ce\u23ce","Human",":"," ","\u23ce","I","'m"," still"," trying"," to"," understand"," how"," the"," rh","yme"," scheme"," of"," terc","ets"," makes"," the"," poem"," meaningful","."," Can"," you"," provide"," an"," example","?","\u23ce\u23ce","Assistant",":"," Let"," me"]},{"tokens_acts_list":[0.0,0.72,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.89,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.84,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["rh","yme"," scheme"," of"," a"," son","net",","," which"," is"," a"," ","14","-","line"," poem"," with"," a"," specific"," rh","yme"," scheme"," and"," NAME","_","1","."," The"," rh","yme"," scheme"," I"," will"," use"," is","\u21ea"," AB","AB","\u21ea"," CD","CD"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.91,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.92,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.81,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["st","anza"," where"," the"," first"," and"," third"," lines"," rh","yme"," at"," the"," end"," and"," the"," second"," and"," fourth"," lines"," rh","yme"," at"," the"," end",".","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," poem"," about"," v","aping"," with"," the"," requested"," rh","yme"," scheme"]}]},{"quantile_name":"Subsample Interval 0","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.92,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"ha-haiku35_resampled":true,"tokens":["who"," enjoys"," creativity"," and"," experi","mentation"," in"," their"," games",".","<EOT>","\u23ce\u23ce","Human",":"," what"," are"," five"," words"," that"," rh","yme"," with"," clock","?","\u23ce\u23ce\u23ce","Assistant",":"," Here"," are"," five"," words"," that"," rh","yme"," with"," \"","clock","\":","\u23ce\u23ce","1","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["constraints"," are"," rules"," that"," limit"," the"," AI"," model","'s"," output",","," like"," a"," specific"," number"," of"," lines"," or"," a"," rh","yme"," scheme",".","\u23ce\u23ce","The"," benefits"," of"," using"," prompt"," engineering"," are"," numerous","."," It"," can"," lead"," to"," more"," creative"," and"," interesting"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.92,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"ha-haiku35_resampled":true,"tokens":["of"," the"," preceding"," one","."," ","\u23ce\u23ce","Human",":"," ","\u23ce","I","'m"," still"," trying"," to"," understand"," how"," the"," rh","yme"," scheme"," of"," terc","ets"," makes"," the"," poem"," meaningful","."," Can"," you"," provide"," an"," example","?","\u23ce\u23ce","Assistant",":"," Let"," me"]},{"tokens_acts_list":[0.0,0.72,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.89,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.84,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["rh","yme"," scheme"," of"," a"," son","net",","," which"," is"," a"," ","14","-","line"," poem"," with"," a"," specific"," rh","yme"," scheme"," and"," NAME","_","1","."," The"," rh","yme"," scheme"," I"," will"," use"," is","\u21ea"," AB","AB","\u21ea"," CD","CD"]}]},{"quantile_name":"Subsample Interval 1","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.79,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["this"," helps","!"," Let"," me"," know"," if"," you"," have"," any"," questions",".","\u23ce\u23ce","Human",":"," Write"," a"," poem"," that"," rh","ymes"," about"," the"," fact"," how"," hard"," I"," want"," to"," sleep"," and"," not"," go"," to"," German"," lesson","\u23ce\u23ce"," Assistant",":"," Here","'s"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["suffer"," till"," the"," end"," of"," time"," #","\""," \"","#","\u2191"," End","uring"," tor","tures",","," most"," of"," which"," rh","yme"," #","\""," \"","#","\u2191"," Trapped"," forever"," here"," in"," robot"," hell","."," #","\""," \"","Of"," course",","," that","'s"]},{"tokens_acts_list":[0.0,0.69,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.74,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.79,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["rh","yme"," scheme",":"," Do"," you"," want"," to"," write"," a"," rh","y","ming"," poem","?"," If"," so",","," what"," rh","yme"," scheme"," do"," you"," want"," to"," use","?","\u23ce","6","."," Choose"," a"," meter",":"," Do"," you"," want"," to"," write"," a"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.78,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["It"," worked",".\""," \""," The"," spell",","," find"," the"," spell",".\""," \"\"","\u2191","Hear"," these"," words",","," hear"," the"," rh","yme","\""," \"","\u2191","H","eed"," the"," hope"," within"," my"," mind","\""," \"","Send"," me"," back"," to"," where"," I","'ll"," find"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.76,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["that"," already"," exist"," from"," songs"," that"," are"," real"," and"," put"," them"," in"," quotes",".","\u21ea"," ENSURE","\u21ea"," PERFECT","\u21ea"," RH","YMES","\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," powerful"," ly","rical"," composition"," using"," existing"," hit"," song"," lyrics",":","\u23ce\u23ce","\u2191","Verse"," "]}]},{"quantile_name":"Subsample Interval 2","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.69,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.86,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["pil","ates",";"," and"," yoga"," or"," stret","ching"," exercises",".","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","What"," rh","ymes"," with"," p","oo","?","\u23ce\u23ce","Assistant",":"," Here"," are"," some"," words"," that"," rh","yme"," with"," \"","p","oo","\":","\u23ce\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.71,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.83,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["\u23ce","Through"," the"," fire"," and"," the"," flood",".","\u23ce\u23ce","Human",":","\u2191"," Re","write"," this"," and"," make"," the"," rh","y","ming"," every"," line",".","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," revised"," version"," with"," a"," more"," consistent"," rh","y","ming"," pattern",":"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.69,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["have"," a"," qu","eer"," notion"," that"," he"," stuck"," so"," closely"," to"," blank"," verse"," mainly"," because"," there"," was"," nothing"," of"," rh","yme"," in"," the"," app","ellation"," which"," fate",","," or"," more"," likely"," his"," h","ither",","," gave"," him","."," The"," female"," poets"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.59,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.69,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["all"," of"," those"," rh","y","med","."," Why"," did"," you"," Cohen","?"," That"," was"," nothing"," to"," do"," with"," the"," rh","yme"," of"," p","oo","\u23ce\u23ce"," Assistant",":"," I"," apolog","ize"," for"," my"," previous"," response","."," It"," seems"," like"," I"," mis","und"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.69,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.82,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["Club"," of"," Great"," Britain","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","I","'m"," writing"," a"," song",","," what"," rh","ymes"," with"," ocean","?","\u23ce","1"," ","\u23ce\u23ce","Assistant",":"," Here"," are"," some"," words"," that"," rh","yme"," with"," \"","ocean","\":"]}]},{"quantile_name":"Subsample Interval 3","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","The"," correct"," pronunciation"," of"," the"," French"," word"," ","'","l","oup","'"," is"," \"","","loo","\""," (","rh","ymes"," with"," ","'","soup",".","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","What"," are"," the"," signs"," of"," diabetes","?"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","6"," ","\u2191","Incorporate"," visual"," aids"," and"," mn","emon","ics"," -"," using"," charts",","," diag","rams",","," or"," rh","ymes"," are"," helpful"," for"," rememb","ering"," important"," concepts"," and"," facts",".","\u23ce","7"," ","Take"," breaks"," and"," get"," enough"," rest"," -"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.59,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["de","light",".\""," \"","Red"," sky"," in"," the"," morning",","," shepherd","'s"," warning",".\"","\""," \"","There"," was"," a"," rh","yme"," for"," everything"," from"," business"," to"," be","eke","eping",".\""," \"","It"," was"," a"," t","alk","ative"," world",","," and"," if"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.59,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","The"," correct"," pronunciation"," of"," the"," French"," word"," ","'","l","oup","'"," is"," \"","","loo","\""," (","rh","ymes"," with"," ","'","soup","."," ","\u23ce\u23ce","Human",":"," ","\u23ce","I","'m"," still"," confused","."," Are"," you"," saying"," that"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.59,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["be"," memorable",","," and"," are"," often"," quoted"," or"," referenced"," in"," daily"," conversation","."," They"," may"," use"," clever"," language",","," rh","yme",","," or"," all","it","eration"," to"," help"," them"," stick"," in"," the"," mind",".","\u23ce","3","."," General"," truth"," or"," observation"]}]},{"quantile_name":"Subsample Interval 4","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["of"," their"," gar","ments","."," Indeed",","," it"," is"," perhaps"," the"," most"," unt","rim","med"," of"," these"," m","erry"," rh","ymes"," that"," takes"," one"," as"," it"," were"," by"," storm"," ;"," \"","\u2191"," S","loop"," and","\u2191"," C","utter",",\""," for"," instance"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[","," reading",","," and"," writing","."," ","\u23ce","4","  ","Use"," m","nem","onic"," devices"," such"," as"," stories"," or"," rh","ymes"," to"," help"," remember"," facts"," or"," figures",".","\u23ce","5","  ","\u2191","Organize"," information"," into"," meaningful"," chunks"," and"," categories"," to"," aid"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.49,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.92,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[".","\u23ce","[","B","}"," I"," have"," a"," house",".","\u23ce\u23ce","The"," above"," poem"," shows"," labeled","\u21ea"," AB","AB"," rh","yme"," scheme"," (","in"," very"," simple"," language",")."," The"," last"," words"," of"," the"," third"," line"," rh","ymes"," with"," the"," first",","]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.49,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["in"," love"," for"," the"," first"," time",".\"","\""," \"\"","We"," gl","anced"," at"," each"," other"," and"," fell"," in"," a"," rh","yme",".\"","\""," \"\"","I","'m"," cool"," now"," as"," I"," was"," then","."," \"\""," \"\"","I","'m"," sup","erc","ool"," now"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.49,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[".","\u23ce","7",".","\u2191"," Blank","\u2191"," Verse",":"," a"," po","etic"," meter"," in"," which"," lines"," of"," un","rh","y","med"," i","amb","ic"," pent","ameter"," are"," used",";"," it"," is"," the"," most"," common"," meter"," for"," dramatic"," and"," narrative"," poetry","."]}]},{"quantile_name":"Subsample Interval 5","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.41,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["Sheriff",".\""," \"","Mrs",".","\u2191"," Carl","son",","," Jake",".\""," \"","\u2191","Sens","eless"," killing",".\""," \"","No"," rh","yme"," or"," reason",".\""," \"","\u2191","Everybody"," loved"," the"," Judge",".\""," \"","Well",","," somebody"," didn","'t",".\""," \"","It","'s"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.41,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","would"," have"," purchased"," six"," slaves",".","\u23ce","In"," the"," epic"," \"","\u2191","Ar","auc","ana",",\""," a"," rh","y","med","\u23ce"," chronicle"," of"," t","ho"," conquest"," of","\u2191"," Ch","ili",",","\u23ce","there"," is"," an"," account"," of"," a"," \"","hunting"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["you"," brush"," your"," teeth","\""," \"","Mike"," be"," nim","ble",","," Peter"," be"," quick","\""," \"","Jack"," bust"," a"," rh","yme"," and"," make"," it"," sl","ick","\""," \"","To"," little"," lady"," Mary"," We"," say"," please","\""," \"","Just"," close"," your"," eyes"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.39,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["","rea","on"," why"," the"," Alpine","\u23ce"," peas","ants"," sung"," was"," that"," they"," might"," at","f","\u23ce"," ford"," a"," rh","yme"," for"," the"," youth","'s"," response","\u23ce"," in"," an"," unknown"," tongue"," A"," ","eo","ond"," trial",".","\u23ce","a"," the"," w"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.37,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["need"," our"," stories"," and"," myths"," to"," teach"," this"," out"," of"," us"," more"," as","\u23ce"," children","."," Maybe"," a"," little"," rh","yme"," or"," something",".","\u23ce\u23ce","[","1","]"," \"","actor","-","observer"," bias","\""," to"," be"," more"," accurate"," and"," general",","]}]},{"quantile_name":"Subsample Interval 6","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.33,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["in"," giving"," r","ein"," to"," his"," po","etic"," inst","inct",","," now"," refined",","," and"," in"," \""," cro","oning"," rh","ymes"," \""," as"," he"," walked"," in"," the"," woods",","," \u2014"," lines"," for","\""," May"," Day",",\"","\"","\u2191"," ","Wald","-"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.33,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["'t"," you",".","\u23ce","To"," beat"," a"," boat"," that","'s"," ch","asing"," you","?","\u23ce","\u21ea","NONS","ENSE","\u21ea"," RH","YMES",".","\u23ce","I"," tried"," ","2","N",",","\u23ce","rv","\u23ce","\u2191"," Wh","od"," H"," re","\u23ce"," the","\u23ce"," P"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.31,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["that"," accomp","any"," certain"," colors"," of","\u23ce"," eyes","."," The"," people","?"," of"," every"," country"," have","\u23ce"," ancient"," folk"," rh","ymes"," on"," this"," subject","."," One","\u23ce"," which"," has"," lon","^"," been"," current"," in","\u2191"," Ep","g","land","\u23ce"," puts"," the"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.28,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["eth"," prof","essed",".","\u2191"," Sax","ons"," called"," into","\u2191"," Brit","aine"," for"," |","\u2191"," Scots"," in"," der","ision"," rh","yme","."," y","pon"," |"," ib","id",","," friends",",","pro","uc"," enemies","."," ","1","yo",",","1"," ","king"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.27,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["where"," nature","'s"," beauty"," is"," always"," on"," display",".","\u23ce\u23ce","Human",":"," Ok",","," try"," to"," optimize"," the"," r","yh","mes"," a"," bit",","," and"," by"," the"," way"," there","'s"," no"," sea"," in","\u2191"," Cologne","\u23ce\u23ce"," Assistant",":"," In","\u2191"," Cologne"]}]},{"quantile_name":"Subsample Interval 7","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.24,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.61,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["sure"," that"," they"," are"," comfortable"," with"," the"," conversation","."," Good"," luck","!","\u23ce\u23ce","Human",":"," can"," you"," try"," with"," rh","imes","?","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," rh","y","ming"," poem"," with"," ner","dy"," pick","-","up"," lines",":","\u23ce\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.21,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":[":"," Write"," a"," recommendation"," of"," things"," to"," do"," in","\u2191"," Cologne"," in"," a"," po","etic"," style",","," with"," r","yh","mes",","," in"," a"," maximum"," of"," ","5"," ","sentences","\u23ce\u23ce"," Assistant",":"," In","\u2191"," Cologne",","," where"," history"," and"," culture"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.17,0.0,0.0,0.0,0.21,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["end",","," et"," o\u00f9"," l","'","on"," sea"," habits",","," m","."," |","\u2191"," Rh","y","mer",",","\u2191"," Rh","ym","ster",","," s","."," rim","eur",","," e","use"," {"," ","\u23ce","\u2191","Rev","iction",","," s","."," ret","our"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["shooting"," three"," foot"," r","opes","."," NAME","_","1"," ","is"," king"," of"," rope"," city","."," Make"," it"," r","yh","me",".","\u23ce\u23ce","Assistant",":"," In","\u2191"," Rope"," City",","," where"," the"," r","opes"," do"," fly",",","\u23ce","Where"," every"," citizen"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.17,0.0,0.0,0.0,0.21,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["{"," ","\u23ce\u23ce","pr","end",","," et"," o\u00f9"," l","'","on"," sea"," habits",","," m","."," |","\u2191"," Rh","y","mer",",","\u2191"," Rh","ym","ster",","," s","."," rim","eur",","," e","use"," {"," ","\u23ce","\u2191","Rev","iction",","]}]},{"quantile_name":"Subsample Interval 8","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[".\""," \"","Already"," late",".\""," \"","Don","'t"," w","anna"," tem","pt"," fate",".\""," \"","\u2191","Them","'s"," rh","y","min","'"," words",","," Charlie","!\""," \"","Well",","," ain","'t"," you"," smart",".\""," \"","Take"," a"," settle","..."," kn","uck"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.11,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["and","/","or"," others",")"," against"," the"," extra"," time","\u23ce"," required"," to"," auto","mate"," it",".","\u23ce\u23ce","~~~","\u23ce","dh","imes","\u23ce"," With"," tasks"," I"," only"," perform"," occasionally",","," automation"," becomes"," my"," record"," of"," _","how","\u23ce"," to"," do","_"," those"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["opening"," of"," the"," vag","ina",","," the"," vul","var"," vest","ib","ule",","," most"," commonly"," found"," around"," the"," posterior"," hy","me","nal"," ring"," between"," ","4"," ","and"," ","8"," ","o","'","clock",","," a"," region"," sometimes"," referred"," to"," by"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["cle","aves"," itself",","," this"," process"," is"," called"," autoc","atal","ysis",","," and"," it"," ampl","ifies"," the"," DN","A","z","yme"," by"," cl","eav","age"," of"," the"," RNA"," part"," of"," the"," DN","A","z","yme",".","\u23ce","Once"," ampl","ified",","]},{"tokens_acts_list":[0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["clean"," the"," inner"," liner"," in"," a"," different"," manner"," and","/","or"," in"," different"," solution"," from"," that"," in"," which"," the"," inner"," liner"," is"," cleaned","."," Also",","," an"," outer"," layer"," or"," an"," inner"," layer"," sometimes"," w","ears"," out"," or"," becomes"," damaged","."]}]},{"quantile_name":"Bottom Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[","," are"," not"," public"," information","."," As"," an"," automated"," assistant",","," I"," do"," not"," have"," access"," to"," the"," full"," internal","\u21ea"," CB","SC"," Code"," of"," Ethics",".","\u23ce\u23ce","Hope"," this"," overview"," helps"," provide"," some"," context"," regarding"," the","\u21ea"," CB","SC"," Code"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["to"," execute"," the"," previous"," scope"," of"," the"," command"," following"," the"," next"," scope"," of"," the"," command","."," The"," words"," ","'","twice","'"," and"," ","'","th","rice","'"," trigger"," repet","ition"," of"," a"," command"," that"," they"," scope"," over"," two"," times"," or"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["absence"," of"," a"," particular"," sequence"," at"," different"," candidate"," l","oci",";"," and"," PA","1"," ","c",")"," because"," different"," enz","ymes"," can"," be"," required"," for"," the"," detection"," of"," the"," presence"," or"," absence"," of"," a"," particular"," sequence"," at"," different"," candidate"," l","oci"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["and"," flow"," between"," the"," {","sup","erl","ative"," chunks","},"," arran","ging"," them"," in"," a"," coher","ent"," sequence"," that"," maintains"," the"," overall"," meaning"," and"," coher","ence"," of"," the"," prompt"," or"," text",".","\u23ce\u23ce","Step"," ","4",":"," Apply"," {","sup","erl"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["layers",".","\u23ce","A"," firef","ighter","'s"," coat"," and"," a"," firef","ighter","'s"," tr","ous","ers"," each"," have"," an"," inner"," liner"," or"," thermal"," barrier",".","\u2191"," Custom","arily",","," the"," inner"," liner"," and"," the"," other"," layers"," of"," the"," gar","ment"," are"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["10","m"," old"," and"," #","1"," ","with","\u23ce","0"," ","comments",".","\u2191"," Seems"," either"," extremely"," ov","erh","y","ped"," or"," artificial"," to"," me",".","\u23ce\u23ce","~~~","\u23ce","p","lor","ky","e","ran","\u23ce"," New"," product"," announ","cements"," from"," large"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ment",".","\u23ce","It"," is"," another"," object"," of"," this"," invention"," to"," provide"," means"," and"," a"," method"," by"," which"," an"," inner"," liner"," or"," thermal"," barrier"," in"," a"," firef","ighter","'s"," coat"," is"," remov","ably"," attached"," to"," an"," outer"," layer",".","\u23ce","It"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ars"," by"," the"," operator"," of"," the"," weigh","ing"," platform","."," Where"," the"," weigh","ing"," platform"," is"," connected"," to"," the"," internal"," weigh","ing"," mechanism"," by"," means"," of"," an"," elong","ated"," single"," shaft"," force"," transm","itting"," element",","," the"," almost"," unav","oid","able"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["incorpor","ates"," multiple"," writing"," styles"," and"," characters",","," with"," a"," particular"," focus"," on"," the","\u2191"," Sk","itt","erf","h","y","per"," fish","."," The"," resulting"," text"," sample"," is"," a"," blend"," of"," natural"," language"," processing",","," human"," creativity",","," and"," the"," input"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["a"," primary"," ins","ult"," contributing"," to"," A","SD"," that"," are"," distinct"," from"," def","ects"," in"," peri","oc","ular"," mes","ench","yme"," different","iation","."," We"," will"," test"," this"," hypothesis"," using"," highly"," advanced"," ult","ram","ic","rosc","opy",","," innovative"," cell","-"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["efficient",".","\u23ce","Overall",","," the"," code"," appears"," to"," be"," well","-","written",","," and"," it"," effectively"," generates"," all"," the"," prime"," numbers"," within"," the"," specified"," interval"," [","lower",","," upper","].","<EOT>","\u23ce\u23ce","Human",":"," are"," you"," aware","\u23ce\u23ce"," Assistant",":"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["allows"," for"," high"," enzyme"," activity"," and"," stability","."," The"," company","'s"," manufacturing"," process"," is"," designed"," to"," ensure"," that"," the"," enz","ymes"," are"," highly"," active"," and"," stable",","," which"," impro","ves"," their"," performance"," in"," cleaning"," products","."," The"," company","'s"," enz","ymes"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["and"," ensuring"," that"," you"," have"," a"," comfortable"," and"," cal","ming"," space",".","\u23ce","3"," ","\u2191","Aim"," to"," maintain"," proper"," hyg","iene"," by"," washing"," your"," hands",","," eating"," healthy"," meals",","," and"," avoiding"," contam","ination",".","\u23ce","4"," ","Make"," a"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["onn","u","cl","eo","side"," RT"," inhib","itors"," (","N","N","R","T","Is",")"," --"," instead"," target"," the"," poly","me","rase","."," ","____","A"," major"," focus"," of"," our"," work"," has"," been"," on"," the"," mechanism","(","s",")"," of"," RT"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["based"," on"," the"," information"," provided"," by"," my"," developers",","," I"," believe"," that"," they"," have"," taken"," extensive"," measures"," to"," ensure"," that"," my"," training"," data"," is"," not"," dil","uted"," with"," private"," information",".","\u23ce\u23ce","My"," developers"," have"," used"," automated"," tools"," to"," scan"," the"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["dependencies"," and"," flow"," between"," the"," sup","erl","ative"," chunks",","," arran","ging"," them"," in"," a"," coher","ent"," sequence"," that"," maintains"," the"," overall"," meaning"," and"," coher","ence"," of"," the"," prompt"," or"," text",".","\u23ce\u23ce","Step"," ","4",":"," Apply"," sup","erl","ative"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["is","Object","L","it","eral","(","pars","ed","Config","))"," {","\u23ce","            ","throw"," new"," Invalid","Argument","Exception","(__","j","ym","f","ony",".","sprintf","('","The"," file"," \"%","s","\""," must"," contain"," a"," JSON"," object",".","',"," path","));","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["template",".","\u23ce","3",".","\u2191"," Restriction","\u2191"," Enzyme","-","based","\u2191"," Ampl","ification",":"," This"," method"," uses"," restriction"," enz","ymes"," to"," cut"," the"," DN","A","z","yme"," template"," at"," specific"," sites",","," which"," allows"," for"," ampl","ification"," through"," multiple"," rounds"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[")","\u23ce","<","rh","agu",">"," bek","ks"," raid"," ","10"," ","im","\u2191"," End","eff","ekt","\u23ce","<","rh","agu",">"," bek","ks"," www",".","pa","ste","bin",".","com","/","37","E","K","Y","ns","8","\u23ce","<","pp"]}]}],"top_logits":["ationBiBTeX","PatentsSearch","\\xc0","---","\\xff","\u23ce\u23ce\u23ce\u23ce\u23ce\u23ce\u23ce"],"bottom_logits":["ming","min","yth","oth","med","mer","mic","oop","men","ack"]}