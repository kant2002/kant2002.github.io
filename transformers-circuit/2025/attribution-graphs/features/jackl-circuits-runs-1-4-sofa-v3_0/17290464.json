{"index": 17290464, "examples_quantiles": [{"quantile_name": "Top Activations", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32, 0.52, 0.0, 0.0, 0.49, 0.48, 0.0, 0.59, 0.0, 0.7, 0.63, 1.0, 0.95, 0.0, 0.0, 0.38, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": [" ", "and", " ", "120", "l", "bs", ",", " NAME", "_", "1", " ", "ex", "u", "ded", " an", " air", " of", " confidence", " that", " made", " her", " appear", " t", "aller", " and", " more", " impos", "ing", ".", "']", "\u23ce\u23ce", "Human", ":", " ", "\u23ce", "1", ".", "", "R", "eph", "rase"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32, 0.52, 0.0, 0.0, 0.49, 0.48, 0.0, 0.59, 0.0, 0.7, 0.63, 1.0, 0.95, 0.0, 0.0, 0.38, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["and", " ", "120", "l", "bs", ",", " NAME", "_", "1", " ", "ex", "u", "ded", " an", " air", " of", " confidence", " that", " made", " her", " appear", " t", "aller", " and", " more", " impos", "ing", ".", "']", "\u23ce\u23ce", "Human", ":", " ", "\u23ce", "1", ".", "", "R", "eph", "rase", " my"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.63, 0.82, 0.45, 0.26, 0.0, 0.0, 0.0, 0.05, 0.48, 0.56, 0.33, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["is", "?\"", " \"", "Just", " a", " small", " token", " of", " my", " appreciation", ".\"", " \"", "A", " very", " small", " token", ".\"", " \"", "Good", " things", " come", " in", " small", " packages", ".\"", " \"", "Little", " things", " come", " in", " small", " packages", ".\"", " \"", "", "Perf", "ume", ".\"", " \"", "No", ","]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.27, 0.41, 0.81, 0.65, 0.43, 0.56, 0.36, 0.37, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07, 0.17, 0.29, 0.53, 0.27, 0.21, 0.62, 0.31, 0.0, 0.06], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": [" ", "after", " shr", "inking", " him", " down", " to", " a", " tiny", " size", ":", "\u23ce\u23ce", "1", ".", " \"", "", "Looks", " like", " I", "'m", " the", " one", " with", " the", " big", " laugh", " now", "!\"", "\u23ce", "2", ".", " \"", "I", " guess", " you", "'ll", " have", " to", " sit", " on", " the"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.41, 0.8, 0.29, 0.51, 0.49, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["and", " love", " to", " play", " pran", "ks", " on", " my", " family", " from", " time", " to", " time", ".", " I", " may", " be", " small", ",", " but", " I", "'ve", " got", " a", " big", " personality", " and", " a", " lot", " of", " love", " to", " give", ".", " I", "'m", " always", " happy", " to", " see", " my"]}, {"tokens_acts_list": [0.16, 0.47, 0.0, 0.43, 0.18, 0.28, 0.56, 0.49, 0.46, 0.5, 0.13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17, 0.09, 0.24, 0.72, 0.59, 0.44, 0.64, 0.19, 0.22, 0.06, 0.41, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15, 0.14, 0.51, 0.47], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["bet", " you", " didn", "'t", " realize", " you", " were", " going", " to", " be", " my", " opening", " act", ".\"", "\u23ce", "4", ".", " \"", "I", "'m", " the", " one", " with", " the", " micro", "phone", " now", ",", " so", " you", " better", " be", " quiet", ".\"", "\u23ce", "5", ".", " \"", "I", " guess", " you"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.15, 0.71, 0.08, 0.0, 0.0, 0.12, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["7", ".", " You", " can", " do", " it", " ", "8", ".", " I", " hope", " it", "'s", " a", " big", " shark", " ", "9", ".", " Little", " things", " count", " ", "10", ".", " The", " world", " is", " your", " oy", "ster", "\u23ce", " NAME", "_", "1", " ", "chat", "bot", ".", "<EOT>", "\u23ce\u23ce"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32, 0.52, 0.0, 0.0, 0.49, 0.48, 0.0, 0.59, 0.0, 0.7, 0.63, 1.0, 0.95, 0.0, 0.0, 0.38, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["'", "4", " ", "and", " ", "120", "l", "bs", ",", " NAME", "_", "1", " ", "ex", "u", "ded", " an", " air", " of", " confidence", " that", " made", " her", " appear", " t", "aller", " and", " more", " impos", "ing", ".", "']", "\u23ce\u23ce", "Human", ":", " ", "\u23ce", "1", ".", "", "R"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13, 0.68, 0.57, 0.56, 0.57, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["!\"", "\u23ce", "3", ".", " \"", "NAME", "_", "4", " ", "may", " not", " be", " as", " tall", " as", " NAME", "_", "3", ",", " but", " he", "'s", " definitely", " the", " bigger", " man", " now", " that", " NAME", "_", "3", "'s", " been", " shr", "unk", " to", " nothing", "!\"", "\u23ce", "4", "."]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.27, 0.41, 0.81, 0.65, 0.43, 0.56, 0.36, 0.37, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07, 0.17, 0.29, 0.53, 0.27, 0.21, 0.62, 0.31, 0.0, 0.06, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["after", " shr", "inking", " him", " down", " to", " a", " tiny", " size", ":", "\u23ce\u23ce", "1", ".", " \"", "", "Looks", " like", " I", "'m", " the", " one", " with", " the", " big", " laugh", " now", "!\"", "\u23ce", "2", ".", " \"", "I", " guess", " you", "'ll", " have", " to", " sit", " on", " the", " stage"]}, {"tokens_acts_list": [0.43, 0.18, 0.28, 0.56, 0.49, 0.46, 0.5, 0.13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17, 0.09, 0.24, 0.72, 0.59, 0.44, 0.64, 0.19, 0.22, 0.06, 0.41, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15, 0.14, 0.51, 0.47, 0.17, 0.05, 0.56], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["'t", " realize", " you", " were", " going", " to", " be", " my", " opening", " act", ".\"", "\u23ce", "4", ".", " \"", "I", "'m", " the", " one", " with", " the", " micro", "phone", " now", ",", " so", " you", " better", " be", " quiet", ".\"", "\u23ce", "5", ".", " \"", "I", " guess", " you", "'ll", " have", " to"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.63, 0.82, 0.45, 0.26, 0.0, 0.0, 0.0, 0.05, 0.48, 0.56, 0.33, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["what", " is", "?\"", " \"", "Just", " a", " small", " token", " of", " my", " appreciation", ".\"", " \"", "A", " very", " small", " token", ".\"", " \"", "Good", " things", " come", " in", " small", " packages", ".\"", " \"", "Little", " things", " come", " in", " small", " packages", ".\"", " \"", "", "Perf", "ume", ".\"", " \"", "No"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32, 0.52, 0.0, 0.0, 0.49, 0.48, 0.0, 0.59, 0.0, 0.7, 0.63, 1.0, 0.95, 0.0, 0.0, 0.38, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["4", " ", "and", " ", "120", "l", "bs", ",", " NAME", "_", "1", " ", "ex", "u", "ded", " an", " air", " of", " confidence", " that", " made", " her", " appear", " t", "aller", " and", " more", " impos", "ing", ".", "']", "\u23ce\u23ce", "Human", ":", " ", "\u23ce", "1", ".", "", "R", "eph"]}, {"tokens_acts_list": [0.08, 0.27, 0.41, 0.81, 0.65, 0.43, 0.56, 0.36, 0.37, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07, 0.17, 0.29, 0.53, 0.27, 0.21, 0.62, 0.31, 0.0, 0.06, 0.0, 0.0, 0.09, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14, 0.12, 0.14, 0.16, 0.47, 0.0, 0.43], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["like", " I", "'m", " the", " one", " with", " the", " big", " laugh", " now", "!\"", "\u23ce", "2", ".", " \"", "I", " guess", " you", "'ll", " have", " to", " sit", " on", " the", " stage", " now", " and", " listen", " to", " my", " jokes", ".\"", "\u23ce", "3", ".", " \"", "I", " bet", " you", " didn", "'t"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.6, 0.32, 0.45, 0.22, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["knowledge", ",", " and", " she", " is", " always", " eager", " to", " learn", " and", " grow", ".", " Despite", " her", " small", " size", ",", " NAME", "_", "2", "}", " has", " a", " big", " heart", " and", " a", " strong", " spirit", ",", " and", " those", " who", " get", " to", " know", " her", " will", " find", " her", " to"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32, 0.52, 0.0, 0.0, 0.49, 0.48, 0.0, 0.59, 0.0, 0.7, 0.63, 1.0, 0.95, 0.0, 0.0, 0.38, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": [" ", "5", "'", "4", " ", "and", " ", "120", "l", "bs", ",", " NAME", "_", "1", " ", "ex", "u", "ded", " an", " air", " of", " confidence", " that", " made", " her", " appear", " t", "aller", " and", " more", " impos", "ing", ".", "']", "\u23ce\u23ce", "Human", ":", " ", "\u23ce", "1", "."]}, {"tokens_acts_list": [0.47, 0.0, 0.43, 0.18, 0.28, 0.56, 0.49, 0.46, 0.5, 0.13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17, 0.09, 0.24, 0.72, 0.59, 0.44, 0.64, 0.19, 0.22, 0.06, 0.41, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15, 0.14, 0.51, 0.47, 0.17], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["you", " didn", "'t", " realize", " you", " were", " going", " to", " be", " my", " opening", " act", ".\"", "\u23ce", "4", ".", " \"", "I", "'m", " the", " one", " with", " the", " micro", "phone", " now", ",", " so", " you", " better", " be", " quiet", ".\"", "\u23ce", "5", ".", " \"", "I", " guess", " you", "'ll"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.58, 0.0, 0.18, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["so", ",", " NAME", "_", "2", " ", "and", " NAME", "_", "3", " ", "lived", " happ", "ily", " ever", " after", ",", " and", " they", " proved", " that", " true", " love", " knows", " no", " bounds", ",", " not", " even", " those", " imposed", " by", " size", ".", "<EOT>", "\u23ce\u23ce", "Human", ":", " can", " you", " tell"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.15, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.12, 0.0, 0.48, 0.58, 0.47, 0.28, 0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["and", " see", " the", " world", " from", " a", " new", " perspective", "?", "\u23ce\u23ce", "Remember", ",", " no", " matter", " how", " small", " you", " may", " feel", ",", " you", " are", " still", " the", " same", " person", ",", " with", " the", " same", " streng", "ths", " and", " abilities", ".", " Let", "'s", " work", " together", " to", " overcome"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13, 0.68, 0.57, 0.56, 0.57, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["\u23ce", "3", ".", " \"", "NAME", "_", "4", " ", "may", " not", " be", " as", " tall", " as", " NAME", "_", "3", ",", " but", " he", "'s", " definitely", " the", " bigger", " man", " now", " that", " NAME", "_", "3", "'s", " been", " shr", "unk", " to", " nothing", "!\"", "\u23ce", "4", ".", " \""]}]}, {"quantile_name": "Subsample Interval 0", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32, 0.52, 0.0, 0.0, 0.49, 0.48, 0.0, 0.59, 0.0, 0.7, 0.63, 1.0, 0.95, 0.0, 0.0, 0.38, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": [" ", "and", " ", "120", "l", "bs", ",", " NAME", "_", "1", " ", "ex", "u", "ded", " an", " air", " of", " confidence", " that", " made", " her", " appear", " t", "aller", " and", " more", " impos", "ing", ".", "']", "\u23ce\u23ce", "Human", ":", " ", "\u23ce", "1", ".", "", "R", "eph", "rase"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32, 0.52, 0.0, 0.0, 0.49, 0.48, 0.0, 0.59, 0.0, 0.7, 0.63, 1.0, 0.95, 0.0, 0.0, 0.38, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["and", " ", "120", "l", "bs", ",", " NAME", "_", "1", " ", "ex", "u", "ded", " an", " air", " of", " confidence", " that", " made", " her", " appear", " t", "aller", " and", " more", " impos", "ing", ".", "']", "\u23ce\u23ce", "Human", ":", " ", "\u23ce", "1", ".", "", "R", "eph", "rase", " my"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.63, 0.82, 0.45, 0.26, 0.0, 0.0, 0.0, 0.05, 0.48, 0.56, 0.33, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["is", "?\"", " \"", "Just", " a", " small", " token", " of", " my", " appreciation", ".\"", " \"", "A", " very", " small", " token", ".\"", " \"", "Good", " things", " come", " in", " small", " packages", ".\"", " \"", "Little", " things", " come", " in", " small", " packages", ".\"", " \"", "", "Perf", "ume", ".\"", " \"", "No", ","]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.27, 0.41, 0.81, 0.65, 0.43, 0.56, 0.36, 0.37, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07, 0.17, 0.29, 0.53, 0.27, 0.21, 0.62, 0.31, 0.0, 0.06], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": [" ", "after", " shr", "inking", " him", " down", " to", " a", " tiny", " size", ":", "\u23ce\u23ce", "1", ".", " \"", "", "Looks", " like", " I", "'m", " the", " one", " with", " the", " big", " laugh", " now", "!\"", "\u23ce", "2", ".", " \"", "I", " guess", " you", "'ll", " have", " to", " sit", " on", " the"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.41, 0.8, 0.29, 0.51, 0.49, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["and", " love", " to", " play", " pran", "ks", " on", " my", " family", " from", " time", " to", " time", ".", " I", " may", " be", " small", ",", " but", " I", "'ve", " got", " a", " big", " personality", " and", " a", " lot", " of", " love", " to", " give", ".", " I", "'m", " always", " happy", " to", " see", " my"]}]}, {"quantile_name": "Subsample Interval 1", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.63, 0.82, 0.45, 0.26, 0.0, 0.0, 0.0, 0.05, 0.48, 0.56, 0.33, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["is", "?\"", " \"", "Just", " a", " small", " token", " of", " my", " appreciation", ".\"", " \"", "A", " very", " small", " token", ".\"", " \"", "Good", " things", " come", " in", " small", " packages", ".\"", " \"", "Little", " things", " come", " in", " small", " packages", ".\"", " \"", "", "Perf", "ume", ".\"", " \"", "No", ","]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.27, 0.41, 0.81, 0.65, 0.43, 0.56, 0.36, 0.37, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07, 0.17, 0.29, 0.53, 0.27, 0.21, 0.62, 0.31, 0.0, 0.06], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": [" ", "after", " shr", "inking", " him", " down", " to", " a", " tiny", " size", ":", "\u23ce\u23ce", "1", ".", " \"", "", "Looks", " like", " I", "'m", " the", " one", " with", " the", " big", " laugh", " now", "!\"", "\u23ce", "2", ".", " \"", "I", " guess", " you", "'ll", " have", " to", " sit", " on", " the"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.41, 0.8, 0.29, 0.51, 0.49, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["and", " love", " to", " play", " pran", "ks", " on", " my", " family", " from", " time", " to", " time", ".", " I", " may", " be", " small", ",", " but", " I", "'ve", " got", " a", " big", " personality", " and", " a", " lot", " of", " love", " to", " give", ".", " I", "'m", " always", " happy", " to", " see", " my"]}, {"tokens_acts_list": [0.16, 0.47, 0.0, 0.43, 0.18, 0.28, 0.56, 0.49, 0.46, 0.5, 0.13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17, 0.09, 0.24, 0.72, 0.59, 0.44, 0.64, 0.19, 0.22, 0.06, 0.41, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15, 0.14, 0.51, 0.47], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["bet", " you", " didn", "'t", " realize", " you", " were", " going", " to", " be", " my", " opening", " act", ".\"", "\u23ce", "4", ".", " \"", "I", "'m", " the", " one", " with", " the", " micro", "phone", " now", ",", " so", " you", " better", " be", " quiet", ".\"", "\u23ce", "5", ".", " \"", "I", " guess", " you"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.15, 0.71, 0.08, 0.0, 0.0, 0.12, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["7", ".", " You", " can", " do", " it", " ", "8", ".", " I", " hope", " it", "'s", " a", " big", " shark", " ", "9", ".", " Little", " things", " count", " ", "10", ".", " The", " world", " is", " your", " oy", "ster", "\u23ce", " NAME", "_", "1", " ", "chat", "bot", ".", "<EOT>", "\u23ce\u23ce"]}]}, {"quantile_name": "Subsample Interval 2", "examples": [{"tokens_acts_list": [0.16, 0.47, 0.0, 0.43, 0.18, 0.28, 0.56, 0.49, 0.46, 0.5, 0.13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17, 0.09, 0.24, 0.72, 0.59, 0.44, 0.64, 0.19, 0.22, 0.06, 0.41, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15, 0.14, 0.51, 0.47], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["bet", " you", " didn", "'t", " realize", " you", " were", " going", " to", " be", " my", " opening", " act", ".\"", "\u23ce", "4", ".", " \"", "I", "'m", " the", " one", " with", " the", " micro", "phone", " now", ",", " so", " you", " better", " be", " quiet", ".\"", "\u23ce", "5", ".", " \"", "I", " guess", " you"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.15, 0.71, 0.08, 0.0, 0.0, 0.12, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["7", ".", " You", " can", " do", " it", " ", "8", ".", " I", " hope", " it", "'s", " a", " big", " shark", " ", "9", ".", " Little", " things", " count", " ", "10", ".", " The", " world", " is", " your", " oy", "ster", "\u23ce", " NAME", "_", "1", " ", "chat", "bot", ".", "<EOT>", "\u23ce\u23ce"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32, 0.52, 0.0, 0.0, 0.49, 0.48, 0.0, 0.59, 0.0, 0.7, 0.63, 1.0, 0.95, 0.0, 0.0, 0.38, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["'", "4", " ", "and", " ", "120", "l", "bs", ",", " NAME", "_", "1", " ", "ex", "u", "ded", " an", " air", " of", " confidence", " that", " made", " her", " appear", " t", "aller", " and", " more", " impos", "ing", ".", "']", "\u23ce\u23ce", "Human", ":", " ", "\u23ce", "1", ".", "", "R"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13, 0.68, 0.57, 0.56, 0.57, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["!\"", "\u23ce", "3", ".", " \"", "NAME", "_", "4", " ", "may", " not", " be", " as", " tall", " as", " NAME", "_", "3", ",", " but", " he", "'s", " definitely", " the", " bigger", " man", " now", " that", " NAME", "_", "3", "'s", " been", " shr", "unk", " to", " nothing", "!\"", "\u23ce", "4", "."]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.27, 0.41, 0.81, 0.65, 0.43, 0.56, 0.36, 0.37, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07, 0.17, 0.29, 0.53, 0.27, 0.21, 0.62, 0.31, 0.0, 0.06, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["after", " shr", "inking", " him", " down", " to", " a", " tiny", " size", ":", "\u23ce\u23ce", "1", ".", " \"", "", "Looks", " like", " I", "'m", " the", " one", " with", " the", " big", " laugh", " now", "!\"", "\u23ce", "2", ".", " \"", "I", " guess", " you", "'ll", " have", " to", " sit", " on", " the", " stage"]}]}, {"quantile_name": "Subsample Interval 3", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32, 0.52, 0.0, 0.0, 0.49, 0.48, 0.0, 0.59, 0.0, 0.7, 0.63, 1.0, 0.95, 0.0, 0.0, 0.38, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["4", " ", "and", " ", "120", "l", "bs", ",", " NAME", "_", "1", " ", "ex", "u", "ded", " an", " air", " of", " confidence", " that", " made", " her", " appear", " t", "aller", " and", " more", " impos", "ing", ".", "']", "\u23ce\u23ce", "Human", ":", " ", "\u23ce", "1", ".", "", "R", "eph"]}, {"tokens_acts_list": [0.08, 0.27, 0.41, 0.81, 0.65, 0.43, 0.56, 0.36, 0.37, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07, 0.17, 0.29, 0.53, 0.27, 0.21, 0.62, 0.31, 0.0, 0.06, 0.0, 0.0, 0.09, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14, 0.12, 0.14, 0.16, 0.47, 0.0, 0.43], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["like", " I", "'m", " the", " one", " with", " the", " big", " laugh", " now", "!\"", "\u23ce", "2", ".", " \"", "I", " guess", " you", "'ll", " have", " to", " sit", " on", " the", " stage", " now", " and", " listen", " to", " my", " jokes", ".\"", "\u23ce", "3", ".", " \"", "I", " bet", " you", " didn", "'t"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.6, 0.32, 0.45, 0.22, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["knowledge", ",", " and", " she", " is", " always", " eager", " to", " learn", " and", " grow", ".", " Despite", " her", " small", " size", ",", " NAME", "_", "2", "}", " has", " a", " big", " heart", " and", " a", " strong", " spirit", ",", " and", " those", " who", " get", " to", " know", " her", " will", " find", " her", " to"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32, 0.52, 0.0, 0.0, 0.49, 0.48, 0.0, 0.59, 0.0, 0.7, 0.63, 1.0, 0.95, 0.0, 0.0, 0.38, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": [" ", "5", "'", "4", " ", "and", " ", "120", "l", "bs", ",", " NAME", "_", "1", " ", "ex", "u", "ded", " an", " air", " of", " confidence", " that", " made", " her", " appear", " t", "aller", " and", " more", " impos", "ing", ".", "']", "\u23ce\u23ce", "Human", ":", " ", "\u23ce", "1", "."]}, {"tokens_acts_list": [0.47, 0.0, 0.43, 0.18, 0.28, 0.56, 0.49, 0.46, 0.5, 0.13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17, 0.09, 0.24, 0.72, 0.59, 0.44, 0.64, 0.19, 0.22, 0.06, 0.41, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15, 0.14, 0.51, 0.47, 0.17], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["you", " didn", "'t", " realize", " you", " were", " going", " to", " be", " my", " opening", " act", ".\"", "\u23ce", "4", ".", " \"", "I", "'m", " the", " one", " with", " the", " micro", "phone", " now", ",", " so", " you", " better", " be", " quiet", ".\"", "\u23ce", "5", ".", " \"", "I", " guess", " you", "'ll"]}]}, {"quantile_name": "Subsample Interval 4", "examples": [{"tokens_acts_list": [0.39, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.24, 0.0, 0.39, 0.36, 0.43, 0.28, 0.51, 0.35, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07, 0.0, 0.29, 0.3, 0.0, 0.0, 0.49, 0.18, 0.03, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["be", " my", " little", " sid", "ek", "ick", " from", " now", " on", ".\"", "\u23ce", "6", ".", " \"", "", "Looks", " like", " I", "'ve", " got", " the", " bigger", " set", " now", ".\"", "\u23ce", "7", ".", " \"", "I", " guess", " you", "'ll", " have", " to", " be", " my", " personal", " joke", " writer", " from"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.41, 0.8, 0.29, 0.51, 0.49, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["to", " play", " pran", "ks", " on", " my", " family", " from", " time", " to", " time", ".", " I", " may", " be", " small", ",", " but", " I", "'ve", " got", " a", " big", " personality", " and", " a", " lot", " of", " love", " to", " give", ".", " I", "'m", " always", " happy", " to", " see", " my", " family", " members"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09, 0.0, 0.5, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19, 0.0, 0.1, 0.0, 0.14, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["parents", " were", " busy", " getting", " ready", " for", " their", " night", " out", ".", " Despite", " her", " relatively", " small", " st", "ature", " at", " just", " ", "5", "'", "4", "\"", " and", " ", "120", "l", "bs", ",", " NAME", "_", "1", " ", "carried", " herself", " with", " an", " unw", "av", "ering", " sense"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.12, 0.27, 0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["others", "\u23ce", " whose", " names", "", " Fr", "ench", "men", " knew", ",", " sur", "\u23ce", " rounded", " the", " small", " figure", " which", " y", "el", "\u23ce", " had", " so", " much", " of", " roy", "alty", ",", " and", " laugh", "ed", "\u23ce", " and", " chat", "ted", " light", "-", "heart", "edly", ".", " In", " a"]}, {"tokens_acts_list": [0.09, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14, 0.12, 0.14, 0.16, 0.47, 0.0, 0.43, 0.18, 0.28, 0.56, 0.49, 0.46, 0.5, 0.13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17, 0.09, 0.24, 0.72, 0.59, 0.44, 0.64, 0.19, 0.22, 0.06, 0.41, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["and", " listen", " to", " my", " jokes", ".\"", "\u23ce", "3", ".", " \"", "I", " bet", " you", " didn", "'t", " realize", " you", " were", " going", " to", " be", " my", " opening", " act", ".\"", "\u23ce", "4", ".", " \"", "I", "'m", " the", " one", " with", " the", " micro", "phone", " now", ",", " so", " you"]}]}, {"quantile_name": "Subsample Interval 5", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.41, 0.8, 0.29, 0.51, 0.49, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["up", " and", " love", " to", " play", " pran", "ks", " on", " my", " family", " from", " time", " to", " time", ".", " I", " may", " be", " small", ",", " but", " I", "'ve", " got", " a", " big", " personality", " and", " a", " lot", " of", " love", " to", " give", ".", " I", "'m", " always", " happy", " to", " see"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27, 0.45, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["open", " arms", "\u23ce", " And", " share", " their", " stories", " and", " their", " ch", "arms", "\u23ce\u23ce", " The", " city", " may", " be", " small", ",", " but", " it", "'s", " full", " of", " life", "\u23ce", " With", " a", " beauty", " that", " can", "'t", " be", " divided", "\u23ce", "", " Na", "vo", "tas", ",", " a", " place"]}, {"tokens_acts_list": [0.03, 0.0, 0.0, 0.0, 0.21, 0.0, 0.25, 0.47, 0.28, 0.0, 0.23, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": [".\"", " \"", " Yeah", ",", " you", "'ve", " got", " to", " be", " careful", " of", " those", " little", " ones", ".\"", " \"", " You", " do", ".\"", " \"", " Prince", ".\"", " \"", " Prince", "?\"", " \"", "I", "'m", " the", " same", " size", " as", " Prince", ".\"", " \"", " That", "'s", " right", ".\"", " \"", " Both"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15, 0.49, 0.14, 0.27, 0.4, 0.28, 0.28, 0.03, 0.0, 0.0, 0.0, 0.21, 0.0, 0.25, 0.47, 0.28, 0.0, 0.23, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["coming", " from", " a", " little", ",", " tiny", " person", ".\"", " \"", " Yeah", ",", " tiny", ".\"", " \"", "Yeah", ".\"", " \"", "Sometimes", " it", "'s", " the", " little", " ones", ".\"", " \"", " Yeah", ",", " you", "'ve", " got", " to", " be", " careful", " of", " those", " little", " ones", ".\"", " \"", " You", " do"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.45, 0.14, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13, 0.11, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.09, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["shark", " way", " ", "3", ".", " Just", " above", " his", " pay", " grade", " ", "4", ".", "", " Shark", " attack", " ", "5", ".", " Don", "'t", " f", "ret", " ", "6", ".", " You", "'re", " gonna", " need", " a", " bigger", " boat", " ", "7", ".", " You", " can", " do", " it", " "]}]}, {"quantile_name": "Subsample Interval 6", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07, 0.3, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["]", " Yeah", ",", " you", " have", ".\"", " \"", "You", " got", " lady", "'s", " hands", ".\"", " \"", "They", " might", " be", " small", ",", " but", " they", "'re", " let", "hal", " weapons", ".\"", " \"", "You", " got", " your", " mother", "'s", " hands", ".\"", " \"", "Right", ".\"", " \"", "Put", " your", " hands"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["so", " divine", "\u23ce", " In", " control", " of", " my", " life", ",", " making", " moves", " and", " taking", " charge", "\u23ce", " Moving", " on", " a", " roll", ",", " feeling", " the", " flow", " of", " the", " game", "\u23ce\u23ce", "", " Verse", " ", "2", ":", "\u23ce", "", "Surrounded", " by", " wealth", ",", " living", " the", " dream"]}, {"tokens_acts_list": [0.0, 0.0, 0.08, 0.24, 0.0, 0.39, 0.36, 0.43, 0.28, 0.51, 0.35, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07, 0.0, 0.29, 0.3, 0.0, 0.0, 0.49, 0.18, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["6", ".", " \"", "", "Looks", " like", " I", "'ve", " got", " the", " bigger", " set", " now", ".\"", "\u23ce", "7", ".", " \"", "I", " guess", " you", "'ll", " have", " to", " be", " my", " personal", " joke", " writer", " from", " now", " on", ".\"", "\u23ce", "8", ".", " \"", "I", " guess", " you", "'ll"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["feeling", " so", " divine", "\u23ce", " In", " control", " of", " my", " life", ",", " making", " moves", " and", " taking", " charge", "\u23ce", " Moving", " on", " a", " roll", ",", " feeling", " the", " flow", " of", " the", " game", "\u23ce\u23ce", "", " Verse", " ", "2", ":", "\u23ce", "", "Surrounded", " by", " wealth", ",", " living", " the"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["", "Amaz", "onian", " fish", ",", "\u23ce", "May", " be", " small", " in", " size", ",", "\u23ce", "Their", " beauty", " and", " grace", ",", "\u23ce", "Makes", " them", " an", " object", " of", " pride", ".", " ", "\u23ce\u23ce", "So", " the", " next", " time", " you", " go", " to", " the", " Amazon", ",", "\u23ce", "And", " you"]}]}, {"quantile_name": "Subsample Interval 7", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["an", "wal", "\u23ce", " http", "://", "www", ".", "n", "yt", "imes", ".", "com", "/", "2", "011", "/", "03", "/", "03", "/", "business", "/", "small", "b", "usiness", "/", "03", "sb", "iz", ".", "html", "?", "_", "r", "=", "2", "\u23ce\u23ce", "======", "\u23ce", "R", "ider"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["'s", " hard", " to", " ro", "am", ".", "\u23ce\u23ce", "Though", " the", "", " Amaz", "onian", " fish", ",", "\u23ce", "May", " be", " small", " in", " size", ",", "\u23ce", "Their", " beauty", " and", " grace", ",", "\u23ce", "Makes", " them", " an", " object", " of", " pride", ".", " ", "\u23ce\u23ce", "So", " the", " next", " time"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["play", " every", " force", " at", " his", " disposal", ",", " including", " the", " good", " lady", " wife", ",", " the", " little", " woman", " at", " home", ",", " quite", " literally", ",\"", " \"", "but", " out", " here", " in", "", " Nu", "bia", ",", " in", " the", " w", "iles", " of", " this", " desert", " landscape", ",", " these"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.12, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["big", ".\"", " \"", "They", " are", " very", " grand", " and", " they", " are", " very", " impos", "ing", ",", " but", " I", " am", " here", " today", " to", " see", " something", " even", " more", " interesting", ",", " just", " over", " there", ".\"", " \"", "I", "'m", " looking", " for", " the", " tomb", " of", "", " K", "hu"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.29, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["benefit", " from", " this", "?\"", " \"", "Don", "'t", " be", " so", " funny", ",", "", " Had", "ji", ".\"", " \"", "Don", "'t", " let", " small", " people", " think", " big", ".\"", " \"", "Of", " course", " they", " get", " some", " pocket", " money", ".\"", " \"", "Team", " capt", "ains", " sort", " out", " financial", " matters"]}]}, {"quantile_name": "Subsample Interval 8", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.14, 0.0, 0.0, 0.06, 0.0, 0.1, 0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["ous", ".\"", " \"", "You", " have", " any", " idea", " what", " it", "'s", " like", " to", " be", " a", " big", " man", " on", " the", " inside", "...", " and", " have", " a", " small", " body", " on", " the", " outside", "?\"", " \"", "You", " have", " any", " idea", " what", " it", "'s", " like", " to", " hear", " about"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.09, 0.0, 0.5, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19, 0.0, 0.1, 0.0, 0.14, 0.0, 0.0, 0.0, 0.0, 0.39, 0.0, 0.0, 0.0, 0.0, 0.12, 0.0, 0.41, 0.55, 0.53, 0.56, 0.14, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["st", "ature", " at", " just", " ", "5", "'", "4", "\"", " and", " ", "120", "l", "bs", ",", " NAME", "_", "1", " ", "carried", " herself", " with", " an", " unw", "av", "ering", " sense", " of", " self", "-", "ass", "urance", ",", " radi", "ating", " a", " presence", " that", " commanded", " attention", "."]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["small", " country", ",", " a", " far", " know", ",", " this", " is", " a", " small", " country", ",", " a", " far", " away", " know", ",", " this", " is", " a", " small", " country", ",", " a", " far", " away", " country", ",\"", " \"", "know", ",", " this", " is", " a", " small", " country", ",", " a", " far", " away"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": [".", "\u23ce", "A", " wo", "wan", " refused", " to", " feed", " a", " dw", "arf", ",", " b", "\u23ce", " cn", "use", " she", " was", " opposed", " to", " d", "ine", "-", "a", "-", "mit", "9", ".", "\u23ce", "", "I", "iot", "tn", "", " U", "io", "oe", ".", " j", " -", " ;"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["our", " de", " la", " val", "eur", " de", " l", "'", "ou", "ver", "ture", " de", " l", "'", "", "IA", " g\u00e9n\u00e9", "rative", " pour", " les", " pet", "ites", " entrepr", "ises", " :", "\u23ce\u23ce", "*", " \"", "", "D\u00e9b", "lo", "quez", " vo", "tre", " pot", "ent", "iel", " avec", " l", "'", ""]}]}, {"quantile_name": "Bottom Activations", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["here", ".\"", " \"", "Then", " go", " out", " tomorrow", " and", " see", " that", " no", " one", " gets", " hurt", ".\"", " \"", "It", "'s", " a", " small", " story", ".\"", " \"", "I", "'ll", " ", "bury", " it", " as", " deep", " as", " I", " can", ".\"", " \"", "The", " ad", " will", " run", " on", " the"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["_", "2", " ", "and", " the", " rest", " of", " the", " medical", " staff", " that", " he", " is", " not", " just", " a", " tiny", " man", ",", " but", " a", " person", " worthy", " of", " respect", " and", " dignity", ".", "<EOT>", "\u23ce\u23ce", "Human", ":", " W", "O", "W", "64", "\uac00", " ", "\\xeb", "\\xad", "\\x90"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["\"", "Pretty", " much", " impossible", ".\"", " \"", "I", " don", "'t", " know", " maybe", " she", "'ll", " really", " pull", " it", " off", " and", " be", " the", " biggest", " success", " in", " human", " history", ".\"", " \"", "First", " of", " all", ",", " you", " started", " off", " with", " two", " ingredients", ".\"", " \"", "In", " fact"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["\"", " You", " have", " nothing", " nice", " to", " say", ",", " don", "'t", " say", " anything", ".\"", " \"", " In", " our", " biggest", " season", " ever", ",", " it", "'s", " the", " biggest", " twist", " of", " all", ".\"", " \"", " It", "'s", " not", " over", " just", " yet", ".\"", " \"", " The", " competition", " continues", " online"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.47, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["\u23ce", "1", ".", "", "R", "eph", "rase", " my", " text", " in", " mild", " creative", " way", ".", "\u23ce", "2", ".", "Text", " should", " not", " be", " expand", " more", " than", " ", "112", " ", "words", "\u23ce", "3", ".", "", "Simpl", "ify", " the", " language", " used", " to", " ensure", " it", "'s"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.16, 0.27, 0.13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["and", " they", " are", " known", " to", " be", " very", " independent", ",", " sometimes", " to", " the", " point", " of", " being", " stub", "born", ".", " Some", " people", " say", " that", " they", " are", " one", " of", " the", " most", " independent", " breeds", " of", " dog", ",", " while", " other", " people", " say", " that", " they", " are", " one"]}, {"tokens_acts_list": [0.0, 0.29, 0.3, 0.0, 0.0, 0.49, 0.18, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.16, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["I", " guess", " you", "'ll", " have", " to", " be", " my", " personal", " joke", " writer", " from", " now", " on", ".\"", "\u23ce", "8", ".", " \"", "I", " guess", " you", "'ll", " have", " to", " be", " my", " little", " comedy", " prop", " from", " now", " on", ".\"", "\u23ce", "9", ".", " \"", "I", " guess", " you"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["about", " the", " toilet", " in", " theatre", ".\"", " \"", "It", "'s", " getting", " too", " long", ".\"", " \"", "Don", "'t", " go", " by", " length", ".\"", " \"", "Go", " by", " emotion", ",", " sir", ".\"", " \"", "You", " listen", ".\"", " \"", "O", " life", ",", " come", " close", " to", " me", ".\"", " \""]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["\u00e9es", "", " LIB", "RES", " Les", " h", "onn", "eurs", " comme", " les", " \u00e9ch", "asses", " gran", " diss", "ent", " c", "eux", " qui", " ne", " ser", "aient", " jam", "ais", "", " J", "eve", " ", "nus", " g", "-", "", "ands", ".", "", " Com", "t", "esse", " de", "", " Diane", "."]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["^", " as", " de", " \u00a7", "ros", " do", "ig", "ts", " tu", " s", "TV", "^", "T", "^^", "g", " \u00e9c", "ris", " fin", " !", " \u00bb", " ", "\u23ce", "Ce", " j", "oli", " mot", " d", "'", "enf", "ant", " nous", " invite", " \u00e0", " par", "ler", " de", " la", " mi", "cr", "ographie", ","]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06, 0.38, 0.0, 0.15, 0.21, 0.0, 0.21, 0.03, 0.0, 0.0, 0.0, 0.0, 0.34, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["competition", ".\"", " \"", "Oscar", " says", " I", "'m", " too", " small", ".\"", " \"", "No", ",", " small", " is", " good", ".\"", " \"", "Small", " is", " a", " weapon", ".\"", " \"", "You", " stand", " this", " close", ".\"", " \"", "When", " I", " hit", ",", " you", " block", ".\"", " \"", "Then", " you", " look"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["has", " risen", " more", " than", " ", "50", "%", " since", " the", " pre", "-", "industrial", " era", ".", "  ", "As", " you", " may", " have", " heard", ",", " CO", "2", " ", "is", " a", " greenhouse", " gas", " so", " named", " because", " it", " \"", "captures", "\"", " long", " wave", " IR", " radiation", " (", "heat"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["\"", "Oh", ",", " no", ".\"", " \"", "Your", " mom", " was", "", " Grand", "ma", ".\"", " \"", "Listen", ",", " I", " don", "'t", " mind", " living", " in", " small", " spaces", ".\"", " \"", "I", " shared", " a", " room", " with", " Sean", ",", " Paul", ",", " Mark", ",", " and", "", " Brend", "an"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["eia", "2", ">", " h", "ehe", ",", " cat", "-", "sized", "\u23ce", "<", "kn", "ome", ">", " that", "'s", " a", " known", " finnish", " phrase", "\u23ce", "<", "kn", "ome", ">", " \"", "kis", "san", " kok", "ois", "et", " kir", "j", "ai", "met", "\"", " ->", " cat", "-", "sized", " letters"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13, 0.68, 0.57, 0.56, 0.57, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["NAME", "_", "4", " ", "may", " not", " be", " as", " tall", " as", " NAME", "_", "3", ",", " but", " he", "'s", " definitely", " the", " bigger", " man", " now", " that", " NAME", "_", "3", "'s", " been", " shr", "unk", " to", " nothing", "!\"", "\u23ce", "4", ".", " \"", "NAME", "_", "3", ","]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["_", "2", " ", "reass", "ured", " him", " that", " she", " only", " wanted", " to", " make", " him", " happy", " and", " that", " he", " would", " be", " able", " to", " live", " a", " normal", " life", " at", " this", " size", ".", "\u23ce\u23ce", "NAME", "_", "3", " ", "was", " initially", " skept", "ical", ",", " but", " he"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["purpose", ".", " If", " you", " don", "'t", " need", " it", ",", " get", " rid", " of", " it", ".", "\u23ce", "             ", "8", ".", " Start", " small", ",", " but", " keep", " momentum", " going", ":", " Start", " with", " one", " room", " or", " a", " specific", " type", " of", " item", " and", " complete", " it", " before", " moving"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["the", " role", " of", " a", " fictional", " dialogue", " writer", ".", " You", " are", " creating", " the", " fake", " dialogue", " between", " a", " fake", " user", " and", " L", "itt", "le", "G", "P", "T", " to", " test", " a", " much", " reduced", " version", " of", " your", " AI", ".", " You", "'ve", " even", " included", " a", " new"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["if", " a", " man", "\u23ce", " jumped", " ", "600", " ", "feet", ",", ",", " and", " he", " can", " draw", " a", "\u23ce", " load", " ", "200", " ", "times", " its", " own", " weight", ".", "\u23ce", "A", " case", " of", " great", " importance", " to", " b", "anka", "\u23ce", " and", " a", " large", " class", " of"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["time", ".\"", " \"", "", "M", "arge", ",", " you", "'re", " a", " genius", "!\"", " \"", "This", " menu", " was", " right", "--", " good", " things", " do", " happen", " at", " ", "'", "", "Zer", "z", ".\"", " \"", "\\xe2\\x99", "\\xaa", "", " ", "'", "", "Zer", "z", "!\"", " \"", "\\xe2\\x99"]}]}], "top_logits": ["grandes", "\\xed\\x81", "big", "grote", "\u0431\u043e\u043b\u044c\u0448", "large", "B", "gro\u00dfe", "dynam"], "bottom_logits": ["file", "bibli", "oslav", "\u043a\u043e\u0442\u043e\u0440\u043e", "emberg", "flan", "slip", "rut", "junction"]}