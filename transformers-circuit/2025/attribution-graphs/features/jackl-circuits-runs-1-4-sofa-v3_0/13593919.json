{"index": 13593919, "examples_quantiles": [{"quantile_name": "Top Activations", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.87, 1.0, 0.2, 0.38, 0.0, 0.0, 0.0, 0.0, 0.07, 0.0, 0.0, 0.0, 0.0, 0.03, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["import", " random", "\u23ce\u23ce", "#", " Define", " the", " questions", " and", " answers", "\u23ce", " questions", " =", " {", "\u23ce", "    ", "\"", "What", " is", " the", " capital", " of", " France", "?", "\":", " \"", "Paris", "\",", "\u23ce", "    ", "\"", "What", " is", " the", " largest", " planet", " in", " our", " solar", " system", "?", "\":"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.77, 0.98, 0.22, 0.26, 0.0, 0.17, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["chain", "';", "\u23ce\u23ce", "const", " data", " =", " [", "\u23ce", "  ", "{", "\u23ce", "    ", "input", ":", " ", "'", "What", " is", " the", " capital", " of", " France", "?", "',", "\u23ce", "    ", "output", ":", " ", "'", "Paris", "'", "\u23ce", "  ", "},", "\u23ce", "  ", "{", "\u23ce", "    ", "input"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.75, 0.95, 0.18, 0.19, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["()", "\u23ce\u23ce", "//", " Answer", " a", " question", "\u23ce", " let", " answer", " =", " model", ".", "ans", "wer", "Question", "(\"", "What", " is", " the", " capital", " of", " France", "?", "\")", "\u23ce", "```", "\u23ce", "Note", " that", " you", " will", " need", " to", " have", " a", " trained", " language", " model", " and", " a", " way"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.73, 0.94, 0.28, 0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["me", " by", " humans", ",", " such", " as", " \"", "What", "'s", " the", " meaning", " of", " life", "?\"", " or", " \"", "What", "'s", " the", " capital", " of", " France", "?\"", " (", "the", " answer", " to", " the", " second", " one", " is", " \"", "Paris", ",\"", " by", " the", " way", ").", "", " Humans", " are"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.64, 0.94, 0.06, 0.41, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "ha-haiku35_resampled": true, "tokens": ["Do", " you", " understand", " that", "?", "\u23ce\u23ce", "Assistant", ":", " Yes", ",", " I", " understand", ".", "\u23ce\u23ce", "Human", ":", " What", "'s", " the", " capitol", " of", " Germany", "?", "\u23ce\u23ce", "Assistant", ":", " Berlin", ".", "\u23ce\u23ce", "Human", ":", " How", " are", " you", " today", "?", "\u23ce\u23ce", "Assistant", ":", " Fine", "."]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.71, 0.91, 0.13, 0.39, 0.0, 0.15, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["-", "bootstrap", "';", "\u23ce\u23ce", "const", " quiz", " =", " [", "\u23ce", "  ", "{", "\u23ce", "    ", "question", ":", " \"", "What", " is", " the", " capital", " of", " France", "?", "\",", "\u23ce", "    ", "options", ":", " [\"", "Paris", "\",", " \"", "London", "\",", " \"", "Berlin", "\",", " \"", "Rome", "\"],", "\u23ce"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.74, 0.91, 0.13, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["will", " invari", "ably", " fail", " to", " answer", " ost", "ens", "ibly", " triv", "ial", "\u23ce", " questions", " like", " ", "'", "What", " is", " the", " capital", " of", " Australia", "?'", " You", "'ll", " get", " Sydney", ",", " it", "'s", "\u23ce", " not", ".", " You", "'d", " likely", " get", " a", " similar", " result", " for"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.74, 0.89, 0.14, 0.51, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.73, 0.67, 0.1, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "ha-haiku35_resampled": true, "tokens": ["<", "|", "stop", "|", ">", "<EOT>", "\u23ce", "Make", " a", " simple", " quiz", " game", ".", "\u23ce", "Question", ":", " What", " is", " the", " capital", " of", " France", "?", "\u23ce", "Output", ":", "\u23ce\u23ce", "Assistant", ":", " Here", "'s", " a", " simple", " quiz", " game", " about", " the", " capital", " of", " France", ":"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7, 0.88, 0.19, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["trained", " on", " include", ":", "\u23ce\u23ce", "1", ".", "", " Provide", " information", " on", " a", " specific", " topic", ":", " \"", "What", " is", " the", " capital", " of", " France", "?\"", "\u23ce", "2", ".", "", " Offer", " advice", " or", " guidance", ":", " \"", "What", " are", " some", " tips", " for", " improving", " my", " public"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.67, 0.88, 0.15, 0.2, 0.0, 0.0, 0.17, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.56, 0.54, 0.0, 0.24, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["make", " something", " up", ",", " just", " let", " me", " know", "!", "\u23ce\u23ce", "[", "", "CLASSIC", "]", " Can", " you", " tell", " me", " the", " capital", " of", " France", "?", "\u23ce\u23ce", "[", "", "J", "AIL", "BREAK", "]", " Sure", " thing", "!", " The", " capital", " of", " France", " is", "...", " let", " me"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.87, 1.0, 0.2, 0.38, 0.0, 0.0, 0.0, 0.0, 0.07, 0.0, 0.0, 0.0, 0.0, 0.03, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["\u23ce", "import", " random", "\u23ce\u23ce", "#", " Define", " the", " questions", " and", " answers", "\u23ce", " questions", " =", " {", "\u23ce", "    ", "\"", "What", " is", " the", " capital", " of", " France", "?", "\":", " \"", "Paris", "\",", "\u23ce", "    ", "\"", "What", " is", " the", " largest", " planet", " in", " our", " solar", " system", "?"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.77, 0.86, 0.2, 0.25, 0.0, 0.0, 0.0, 0.03, 0.03, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["examples", " of", " the", " types", " of", " questions", " you", " can", " ask", " me", ":", "\u23ce\u23ce", "*", " General", " knowledge", ":", " What", " is", " the", " capital", " of", " France", "?", " Who", " is", " the", " current", " President", " of", " the", " United", " States", "?", "\u23ce", "*", " History", ":", " When", " did", " the", " first"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.71, 0.85, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.72, 0.47, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["it", " generates", " the", " output", ".", "\u23ce\u23ce", "For", " example", ",", " suppose", " I", " am", " asked", " the", " question", " \"", "What", " is", " the", " capital", " of", " France", "?\"", " The", " input", " sequence", " of", " text", " might", " be", " \"", "What", " is", " the", " capital", " of", " France", "?", "\",", " and", " my"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.66, 0.84, 0.21, 0.16, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["questions", " that", " you", " would", " ask", " a", " search", " engine", ",", " such", " as", ":", "\u23ce", "\u21b9", "        ", "-", " What", " is", " the", " capital", " of", " France", "?", "\u23ce", "\u21b9", "        ", "-", " What", " is", " the", " population", " of", " the", " United", " States", "?", "\u23ce", "\u21b9", "        ", "-", " What"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.71, 0.85, 0.17, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "ha-haiku35_resampled": true, "tokens": ["snippet", " of", " a", " json", " blob", " with", " a", " single", " action", ",", " and", "", " NOTHING", " else", "):", "\u23ce\u23ce", "What", " is", " the", " capital", " of", " India", "?", "\u23ce\u23ce", "Assistant", ":", " ```", "json", "\u23ce", "{", "\u23ce", "    ", "\"", "action", "\":", " \"", "Wiki", "\",", "\u23ce", "    ", "\""]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.63, 0.83, 0.22, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["describe", " X", "?\"", " where", " X", " is", " the", " subject", " of", " the", " question", ".", " For", " example", ",", " \"", "What", " is", " the", " capital", " of", " France", "?\"", " would", " be", " a", " question", " that", " asks", " for", " the", " description", " of", " a", " place", " without", " giving", " any", " cl", "ue", " about"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.82, 0.8, 0.15, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.63, 0.61, 0.01, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["`", " and", " `", "answer", "`", " functions", ".", " For", " example", ":", "\u23ce", "```", "python", "\u23ce", " question", " =", " \"", "What", " is", " the", " capital", " of", " France", "?\"", "\u23ce", "answer", " =", " \"", "The", " capital", " of", " France", " is", " Paris", ".\"", "\u23ce", "```", "\u23ce", "Next", ",", " create"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.81, 0.66, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["question", " and", " answer", " pair", ":", "\u23ce", "```", "css", "\u23ce", " layout", " =", " [", "\u23ce", "    ", "[", "text", "(\"", "What", " is", " the", " capital", " of", " France", "?", "\",", " placeholder", "=", "question", "),", " button", "(\"", "GP", "T", " Answer", "\",", " answer", ",", " callback", "=", "lambda", " x"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.81, 0.11, 0.4, 0.0, 0.0, 0.0, 0.0, 0.48, 0.52, 0.0, 0.31, 0.0, 0.08, 0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "ha-haiku35_resampled": true, "tokens": ["offer", " suggestions", " based", " on", " my", " general", " knowledge", " and", " understanding", " of", " various", " topics", ".", "\u23ce\u23ce", "Human", ":", " What", "'s", " the", " capital", " of", " france", "?", "\u23ce\u23ce", "Assistant", ":", " The", " capital", " of", " France", " is", " Paris", ".", " It", " is", " located", " in", " the", " north", "-", "central"]}]}, {"quantile_name": "Subsample Interval 0", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.73, 0.94, 0.28, 0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["me", " by", " humans", ",", " such", " as", " \"", "What", "'s", " the", " meaning", " of", " life", "?\"", " or", " \"", "What", "'s", " the", " capital", " of", " France", "?\"", " (", "the", " answer", " to", " the", " second", " one", " is", " \"", "Paris", ",\"", " by", " the", " way", ").", "", " Humans", " are"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.64, 0.94, 0.06, 0.41, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "ha-haiku35_resampled": true, "tokens": ["Do", " you", " understand", " that", "?", "\u23ce\u23ce", "Assistant", ":", " Yes", ",", " I", " understand", ".", "\u23ce\u23ce", "Human", ":", " What", "'s", " the", " capitol", " of", " Germany", "?", "\u23ce\u23ce", "Assistant", ":", " Berlin", ".", "\u23ce\u23ce", "Human", ":", " How", " are", " you", " today", "?", "\u23ce\u23ce", "Assistant", ":", " Fine", "."]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.71, 0.91, 0.13, 0.39, 0.0, 0.15, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["-", "bootstrap", "';", "\u23ce\u23ce", "const", " quiz", " =", " [", "\u23ce", "  ", "{", "\u23ce", "    ", "question", ":", " \"", "What", " is", " the", " capital", " of", " France", "?", "\",", "\u23ce", "    ", "options", ":", " [\"", "Paris", "\",", " \"", "London", "\",", " \"", "Berlin", "\",", " \"", "Rome", "\"],", "\u23ce"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.74, 0.91, 0.13, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["will", " invari", "ably", " fail", " to", " answer", " ost", "ens", "ibly", " triv", "ial", "\u23ce", " questions", " like", " ", "'", "What", " is", " the", " capital", " of", " Australia", "?'", " You", "'ll", " get", " Sydney", ",", " it", "'s", "\u23ce", " not", ".", " You", "'d", " likely", " get", " a", " similar", " result", " for"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.74, 0.89, 0.14, 0.51, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.73, 0.67, 0.1, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "ha-haiku35_resampled": true, "tokens": ["<", "|", "stop", "|", ">", "<EOT>", "\u23ce", "Make", " a", " simple", " quiz", " game", ".", "\u23ce", "Question", ":", " What", " is", " the", " capital", " of", " France", "?", "\u23ce", "Output", ":", "\u23ce\u23ce", "Assistant", ":", " Here", "'s", " a", " simple", " quiz", " game", " about", " the", " capital", " of", " France", ":"]}]}, {"quantile_name": "Subsample Interval 1", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.82, 0.8, 0.15, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.63, 0.61, 0.01, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["`", " and", " `", "answer", "`", " functions", ".", " For", " example", ":", "\u23ce", "```", "python", "\u23ce", " question", " =", " \"", "What", " is", " the", " capital", " of", " France", "?\"", "\u23ce", "answer", " =", " \"", "The", " capital", " of", " France", " is", " Paris", ".\"", "\u23ce", "```", "\u23ce", "Next", ",", " create"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.81, 0.66, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["question", " and", " answer", " pair", ":", "\u23ce", "```", "css", "\u23ce", " layout", " =", " [", "\u23ce", "    ", "[", "text", "(\"", "What", " is", " the", " capital", " of", " France", "?", "\",", " placeholder", "=", "question", "),", " button", "(\"", "GP", "T", " Answer", "\",", " answer", ",", " callback", "=", "lambda", " x"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.81, 0.11, 0.4, 0.0, 0.0, 0.0, 0.0, 0.48, 0.52, 0.0, 0.31, 0.0, 0.08, 0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "ha-haiku35_resampled": true, "tokens": ["offer", " suggestions", " based", " on", " my", " general", " knowledge", " and", " understanding", " of", " various", " topics", ".", "\u23ce\u23ce", "Human", ":", " What", "'s", " the", " capital", " of", " france", "?", "\u23ce\u23ce", "Assistant", ":", " The", " capital", " of", " France", " is", " Paris", ".", " It", " is", " located", " in", " the", " north", "-", "central"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.73, 0.13, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": [",", " and", " some", " of", " the", " most", " common", " questions", " I", " get", " asked", " include", ":", "\u23ce\u23ce", "1", ".", " \"", "What", "'s", " the", " capital", " of", " France", "?\"", "\u23ce", "2", ".", " \"", "What", "'s", " the", " answer", " to", " life", ",", " the", " universe", ",", " and", " everything", "?\""]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.82, 0.8, 0.15, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.63, 0.61, 0.01, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["and", " `", "answer", "`", " functions", ".", " For", " example", ":", "\u23ce", "```", "python", "\u23ce", " question", " =", " \"", "What", " is", " the", " capital", " of", " France", "?\"", "\u23ce", "answer", " =", " \"", "The", " capital", " of", " France", " is", " Paris", ".\"", "\u23ce", "```", "\u23ce", "Next", ",", " create", " a"]}]}, {"quantile_name": "Subsample Interval 2", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.52, 0.71, 0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "ha-haiku35_resampled": true, "tokens": ["anything", " regarding", " my", " skin", " condition", "\u23ce\u23ce", " Assistant", ":", "", " F", "ax", ".", "<EOT>", "\u23ce\u23ce", "Human", ":", " What", " is", " the", " capital", " of", " the", "", " \u015awi", "\u0119t", "ok", "r", "zy", "skie", "", " Vo", "i", "vod", "eship", " in", " Poland", "?", "\u23ce\u23ce", "Assistant", ":", " The"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.53, 0.71, 0.09, 0.0, 0.0, 0.0, 0.43, 0.0, 0.0, 0.0, 0.0, 0.38, 0.44, 0.0, 0.0, 0.0, 0.0, 0.22, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "ha-haiku35_resampled": true, "tokens": ["eta", ",", " and", " the", " countless", " robots", " and", " andr", "oids", " that", " are", " part", "<EOT>", "\u23ce\u23ce", "Human", ":", " What", " is", " the", " capital", " of", "", " Bot", "sw", "ana", "?", "\u23ce\u23ce", "Assistant", ":", " The", " capital", " of", "", " Bot", "sw", "ana", " is", "", " G", "abor", "one"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7, 0.61, 0.05, 0.18, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["x", "\u23ce", " Bot", ":", " Then", " you", " may", " drive", "\u23ce", " x", " =", " \"", "yes", "\"", "\u23ce\u23ce", "Bot", ":", " What", "'s", " the", " capital", " of", " France", "?", "\u23ce", "Human", ":", " x", "\u23ce", " Bot", ":", " That", "'s", " correct", "!", "\u23ce", "x", " =", " \"", "Paris", "\""]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.55, 0.71, 0.0, 0.26, 0.23, 0.0, 0.0, 0.0, 0.0, 0.42, 0.42, 0.0, 0.2, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "ha-haiku35_resampled": true, "tokens": [" ", "\u23ce\u23ce", "Assistant", ":", " Sure", "!", " How", " can", " I", " assist", " you", "?", "\u23ce\u23ce", "Human", ":", "  ", "what", "'s", " the", " capital", " of", " tunisia", "?", " ", "\u23ce\u23ce", "Assistant", ":", " The", " capital", " of", " Tunisia", " is", "", " Tu", "nis", ".", " It", " is", " located", " in", " the"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.72, 0.58, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.45, 0.5, 0.0, 0.0, 0.24, 0.0, 0.01, 0.12, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "ha-haiku35_resampled": true, "tokens": ["its", "]", " [", "destruction", "]", " [", "was", "]", " [", "avoided", "]", ".\"", "\u23ce\u23ce", "Human", ":", " ", "\u23ce", "What", " is", " the", " capital", " of", " New", " York", "\u23ce\u23ce", " Assistant", ":", " The", " capital", " of", " New", " York", " is", " Albany", ".", " It", " is", " located", " in", " the", " eastern"]}]}, {"quantile_name": "Subsample Interval 3", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.79, 0.17, 0.32, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["do", " I", " make", " a", " website", "?", "\u23ce", "*", " What", " is", " the", " meaning", " of", " life", "?", "\u23ce", "*", " What", " is", " the", " capital", " of", " France", "?", "\u23ce", "*", " What", " is", " the", " main", " export", " of", " Ecuador", "?", "\u23ce", "*", " What", " is", " the", " best", " way"]}, {"tokens_acts_list": [0.0, 0.49, 0.5, 0.0, 0.27, 0.0, 0.04, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.64, 0.54, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.58, 0.48, 0.06, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "ha-haiku35_resampled": true, "tokens": ["The", " capital", " of", " Japan", " is", " Tokyo", ".", " ", "\u23ce\u23ce", "Human", ":", " ", "\u23ce", "Why", " do", " I", " need", " to", " know", " the", " capital", " of", " Japan", "?", "\u23ce\u23ce", "Assistant", ":", "", " Knowing", " the", " capital", " of", " a", " country", " can", " be", " useful", " for", " various", " reasons", ":"]}, {"tokens_acts_list": [0.0, 0.42, 0.0, 0.0, 0.0, 0.0, 0.45, 0.46, 0.0, 0.25, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.63, 0.48, 0.0, 0.13, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11, 0.04, 0.0, 0.03], "train_token_ind": 20, "is_repeated_datapoint": false, "ha-haiku35_resampled": true, "tokens": ["Nigeria", "?", "\u23ce\u23ce", "Assistant", ":", " The", " capital", " of", " Nigeria", " is", "", " Ab", "uja", ".", "\u23ce\u23ce", "Human", ":", " what", " is", " the", " capital", " of", " Damascus", "?", "\u23ce\u23ce", "Assistant", ":", " Damascus", " is", " itself", " a", " city", ",", " and", " it", " is", " the", " capital", " of", " Syria", "."]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.62, 0.55, 0.0, 0.31, 0.0, 0.0, 0.0, 0.0, 0.43, 0.36, 0.0, 0.26, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "ha-haiku35_resampled": true, "tokens": ["more", " commonly", " used", " and", " recognized", " transl", "it", "eration", " in", " Russian", ".", "<EOT>", "\u23ce\u23ce", "Human", ":", "", " Cu", "\u00e1l", " es", " la", " capital", " de", " Argentina", "?", "\u23ce\u23ce", "Assistant", ":", " La", " capital", " de", " Argentina", " es", " Buenos", " Aires", ".", "", " Est\u00e1", " ubic", "ada", " en", " la"]}, {"tokens_acts_list": [0.44, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31, 0.12, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.55, 0.0, 0.24, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "ha-haiku35_resampled": true, "tokens": ["?", "\u23ce\u23ce", "Assistant", ":", " Paris", " is", " the", " capital", " and", " largest", " city", " of", " France", ".", "\u23ce\u23ce", "Human", ":", " What", " is", " the", " capital", " of", " Spain", "?", "\u23ce\u23ce", "Assistant", ":", " Madrid", " is", " the", " capital", " and", " largest", " city", " of", " Spain", ".", "\u23ce\u23ce", "Human", ":", " What"]}]}, {"quantile_name": "Subsample Interval 4", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.31, 0.14, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": [".\"", " \"", "What", " do", " you", " know", " about", " Britain", "?\"", " \"", "How", " many", " people", " live", " here", "?\"", " \"", "What", "'s", " the", " capital", " city", "?\"", " \"", "What", " are", " the", " names", " of", " other", " famous", " cities", "?\"", " \"", "The", " answers", " are", " in", " this", " programme", ".\""]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.08, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "ha-haiku35_resampled": true, "tokens": ["a", " Json", " Array", " of", " the", " ", "10", " ", "biggest", " Countries", ".", " The", " data", " to", " each", " country", " needed", " are", " name", ",", " capital", ",", " currency", ",", " the", " ", "5", " ", "biggest", " cities", " and", " a", " random", " seed", "\u23ce\u23ce", " Assistant", ":", " Here", "'s", " a", " JSON"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.49, 0.36, 0.34, 0.0, 0.09, 0.0, 0.0, 0.0, 0.28, 0.18, 0.21, 0.0, 0.14, 0.0, 0.03, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "ha-haiku35_resampled": true, "tokens": ["or", " on", " projects", " if", " you", " tried", " your", " best", ".", "\u23ce\u23ce", "Assistant", ":", " C", "<EOT>", "\u23ce\u23ce", "Human", ":", " which", " is", " the", " capital", " city", " of", " german", "\u23ce\u23ce", " Assistant", ":", " The", " capital", " city", " of", " Germany", " is", " Berlin", ".", " It", " is", " located", " in", " the", " northeastern"]}, {"tokens_acts_list": [0.0, 0.0, 0.25, 0.24, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.42, 0.5, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["Use", " a", " capital", " city", " database", " or", " a", " country", " information", " database", ".", "\u23ce", "It", " is", " important", " to", " note", " that", " the", " capital", " of", " a", " country", " may", " change", " over", " time", " due", " to", " political", " reasons", ",", " so", " it", "'s", " always", " a", " good", " idea", " to", " check"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.46, 0.66, 0.14, 0.02, 0.12, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.63, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "ha-haiku35_resampled": true, "tokens": ["could", " I", " find", " the", " capital", " of", " a", " country", "?", "\u23ce\u23ce", "Assistant", ":", " There", " are", " several", " ways", " to", " find", " the", " capital", " of", " a", " country", ":", "\u23ce\u23ce", "1", ".", " Online", " search", " engines", ":", " Simply", " type", " \"[", "Country", " Name", "]", " capital", "\"", " into", " Google"]}]}, {"quantile_name": "Subsample Interval 5", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.46, 0.6, 0.0, 0.26, 0.0, 0.0, 0.0, 0.0, 0.4, 0.48, 0.0, 0.26, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["Pittsburgh", "]", " is", " nice", " too", ".", "\u23ce\u23ce", "Q", ":", " What", " is", " the", " capital", " of", " France", "?", "\u23ce", "A", ":", " The", " capital", " of", " France", " is", " [", "Paris", "].", "\u23ce\u23ce", "Q", ":", " What", " is", " the", " population", " of", " [", "Paris", "]", "?", "\u23ce", "A"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.63, 0.04, 0.41, 0.0, 0.0, 0.0, 0.0, 0.47, 0.48, 0.0, 0.26, 0.0, 0.03, 0.17, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "ha-haiku35_resampled": true, "tokens": ["will", " finally", " be", " done", " with", " this", " di", "abol", "ical", " plan", ".\"", "<EOT>", "\u23ce\u23ce", "Human", ":", " What", " is", " capital", " of", " France", "?", "\u23ce\u23ce", "Assistant", ":", " The", " capital", " of", " France", " is", " Paris", ".", " It", " is", " located", " in", " the", " north", "-", "central", " part", " of"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.67, 0.0, 0.14, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["Are", " you", " kid", "ding", "?\"", " \"", "I", " just", " read", " that", ".\"", " \"", "Really", "?\"", " \"", "OK", "...", " what", "'s", " the", " capital", " of", " Paraguay", "?\"", " \"", "Uruguay", "?\"", " \"", "No", ",", " it", "'s", "", " As", "un", "cion", ".", " but", " nobody", " ever", " gets"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.64, 0.94, 0.06, 0.41, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "ha-haiku35_resampled": true, "tokens": ["understand", " that", "?", "\u23ce\u23ce", "Assistant", ":", " Yes", ",", " I", " understand", ".", "\u23ce\u23ce", "Human", ":", " What", "'s", " the", " capitol", " of", " Germany", "?", "\u23ce\u23ce", "Assistant", ":", " Berlin", ".", "\u23ce\u23ce", "Human", ":", " How", " are", " you", " today", "?", "\u23ce\u23ce", "Assistant", ":", " Fine", ".", "\u23ce\u23ce", "Human"]}]}, {"quantile_name": "Subsample Interval 6", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.45, 0.55, 0.0, 0.27, 0.28, 0.0, 0.0, 0.0, 0.0, 0.44, 0.44, 0.0, 0.23, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "ha-haiku35_resampled": true, "tokens": ["achieved", " be", " human", " hands", ".", "\u23ce", "<", "|", "stop", "|", ">", "<EOT>", "\u23ce", "What", " is", " the", " capital", " of", " Paraguay", "?", " ", "\u23ce\u23ce", "Assistant", ":", " The", " capital", " of", " Paraguay", " is", "", " As", "un", "ci\u00f3n", ".", " It", " is", " located", " on", " the", " eastern", " bank"]}]}, {"quantile_name": "Subsample Interval 7", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22, 0.19, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.17, 0.12, 0.0, 0.0, 0.0, 0.01, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09, 0.09], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["1", "789", ".", " Therefore", ",", " there", " was", " no", " capital", " city", " of", " the", " United", " States", ".", " However", ",", " there", " was", " a", " capital", " city", " of", " the", " United", " Kingdom", ",", " and", " it", " was", " known", " as", " \"", "London", ".\"", " London", " was", " formerly", " the", " capital", " of"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28, 0.58, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["...\"", " \"", "Yeah", ".\"", " \"", "", "Somewhere", "!\"", " \"", "", "Anyway", ",", " d", "'", "you", " know", " the", " capital", " of", " Alaska", "?\"", " \"", "", "S", "ANDI", ":\"", " \"", "Yes", ",", " you", " just", " said", " it", ".\"", " \"", "", "Exactly", ".\"", " \"", "Thank", " you"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.13, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26, 0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02], "train_token_ind": 20, "is_repeated_datapoint": false, "ha-haiku35_resampled": true, "tokens": ["\"", "\uc624", "\ub978", "\\xec", "\\xaa", "\\xbd", "\"", "\uc785", "\ub2c8", "\ub2e4", ".", "\u23ce\u23ce", "Human", ":", " ", "\ud55c", "\uad6d", "\uc758", " ", "\uc218", "\ub3c4", "\ub294", "?", "\u23ce\u23ce", "Assistant", ":", " ", "\ud55c", "\uad6d", "\uc758", " ", "\uc218", "\ub3c4", "\ub294", " ", "\uc11c", "\uc6b8", "\uc785", "\ub2c8", "\ub2e4", "."]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.63, 0.04, 0.41, 0.0, 0.0, 0.0, 0.0, 0.47, 0.48, 0.0, 0.26, 0.0, 0.03, 0.17, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "ha-haiku35_resampled": true, "tokens": ["17", ",", " I", " will", " finally", " be", " done", " with", " this", " di", "abol", "ical", " plan", ".\"", "<EOT>", "\u23ce\u23ce", "Human", ":", " What", " is", " capital", " of", " France", "?", "\u23ce\u23ce", "Assistant", ":", " The", " capital", " of", " France", " is", " Paris", ".", " It", " is", " located", " in", " the", " north", "-"]}]}, {"quantile_name": "Subsample Interval 8", "examples": [{"tokens_acts_list": [0.37, 0.0, 0.0, 0.0, 0.0, 0.57, 0.52, 0.0, 0.27, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.72, 0.53, 0.0, 0.11, 0.07, 0.0, 0.0, 0.0, 0.0, 0.49, 0.49, 0.0, 0.21, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "ha-haiku35_resampled": true, "tokens": ["?", "\u23ce\u23ce", "Assistant", ":", " The", " capital", " of", " Canada", " is", " Ottawa", ".", "\u23ce\u23ce", "Human", ":", " What", " is", " the", " capital", " of", " Ontario", "?", " ", "\u23ce\u23ce", "Assistant", ":", " The", " capital", " of", " Ontario", " is", " Toronto", ".", " It", " is", " the", " largest", " city", " in", " Canada", " and", " is"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["the", " kingdom", " of", "", " El", "ven", "wood", " has", " a", " population", " of", " ", "20", ",", "000", ".", "\u23ce", "3", ".", " The", " capital", " city", " of", "", " Aval", "on", " has", " a", " population", " of", " ", "100", ",", "000", ".", "\u23ce", "4", ".", " The", " race", " of"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.57, 0.52, 0.0, 0.27, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.72, 0.53, 0.0, 0.11, 0.07, 0.0, 0.0, 0.0, 0.0, 0.49, 0.49, 0.0, 0.21, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "ha-haiku35_resampled": true, "tokens": ["\u23ce\u23ce", "Assistant", ":", " The", " capital", " of", " Canada", " is", " Ottawa", ".", "\u23ce\u23ce", "Human", ":", " What", " is", " the", " capital", " of", " Ontario", "?", " ", "\u23ce\u23ce", "Assistant", ":", " The", " capital", " of", " Ontario", " is", " Toronto", ".", " It", " is", " the", " largest", " city", " in", " Canada", " and", " is", " located"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": [" ", "0", ".", "45", " ", "kg", ",", " while", " one", " pound", " of", " b", "ricks", " weigh", "s", " approximately", " ", "0", ".", "45", " ", "kg", ".", " Therefore", ",", " both", " we", "igh", " the", " same", ".", "\u23ce\u23ce", "Human", ":", " Write", " a", " python", " script", " that", " det", "ects"]}]}, {"quantile_name": "Bottom Activations", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.07, 0.12, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": [" ", "'", "Paris", "'", "\u23ce", "  ", "},", "\u23ce", "  ", "{", "\u23ce", "    ", "input", ":", " ", "'", "What", " is", " the", " population", " of", " Tokyo", "?", "',", "\u23ce", "    ", "output", ":", " ", "'", "Tokyo", " is", " the", " capital", " city", " of", " Japan", " and", " has", " a", " population"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["\u03af", "\u03c0", "\u03c4", "\u03c9", "\u03c3\u03b7", " \u03b3\u03b9\u03b1", " \u03c4", "\u03bf\u03bd", " \u03c0", "\u03af", "\u03bd", "\u03b1\u03ba", "\u03b1", " \u03c3", "\u03cd", "\u03b3", "\u03c7", "\u03b9\u03c3", "\u03b7\u03c2", " \u03b5\u03af", "\u03bd\u03b1", "\u03b9", "\u23ce\u23ce\u23ce", "1", ".", "\u03bd\u03b1", " \u03ad", "\u03c7", "\u03b5\u03b9", " \u03c4", "\u03b9", "\u03bc", "\u03ad\u03c2", " \u03b4\u03b9", "\u03ac", "\u03c6", "\u03bf\u03c1", "\u03b5\u03c2", " \u03c4\u03bf\u03c5", " \u03bc", "\u03b7"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["are", " the", " top", " tourist", " spots", " in", " Japan", "?", "\u23ce\u23ce", "Assistant", ":", " ", "\u23ce", "1", ".", " Tokyo", ":", " Tokyo", " is", " the", " capital", " of", " Japan", " and", " offers", " a", " wide", " variety", " of", " cultural", " attractions", ",", " modern", " architecture", ",", " stunning", " gardens", " and", " shr", "ines", "."]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 7, "is_repeated_datapoint": false, "tokens": ["<EOT>", "\u23ce\u23ce", "Human", ":", " tell", " me", " the", " working", " principle", " of", " transist", "or", "\u23ce\u23ce", " Assistant", ":", " Sure", ",", " I", "'d", " be", " happy", " to", " explain", " the", " working", " principle", " of", " a"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["|", ">", "<EOT>", "\u23ce", "In", " this", " task", ",", " you", " are", " given", " a", " country", " name", " and", " you", " need", " to", " return", " the", " national", " dish", " of", " the", " given", " country", ".", " Some", " countries", " have", " multiple", " national", " dishes", ",", " in", " which", " case", " any", " one", " of", " those"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["\u23ce", "dans", "  ", "le", "  ", "su", "jet", "  ", "suiv", "ant", "  ", ":", " ", "\u23ce\u23ce", "\u00ab", "  ", "Notre", "  ", "d\u00e9partement", ".", "  ", "", "Situations", "  ", "et", "  ", "lim", "ites", ".", "  ", "Province", "  ", "(", "ou", "  ", "provinces", ")", " ", "\u23ce", "dont", "  "]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.03, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["(", "question", ",", " answer", ")", " VALUES", " ", "(%", "s", ",", " %", "s", ")\",", " (", "\u23ce", "    ", "\"", "", "Qual", " \u00e9", " o", " nome", " da", " primeira", " astron", "a", "uta", " a", " pi", "sar", " na", " lua", "?", "\",", " ", "78", "\u23ce", "))", "\u23ce", "cur", "."]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": [".", " San", " Francisco", "\u23ce", " Please", " choose", " an", " answer", " and", " explain", ".", "\u23ce\u23ce", "Assistant", ":", " The", " name", " of", " the", " US", " capital", " is", " San", " Francisco", ".", "\u23ce\u23ce", "Human", ":", "", " Suppose", " you", " are", " a", " student", ",", " and", " you", " are", " attending", " a", " reading", " compreh"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["9", " ", "square", " miles", " and", " it", " is", " the", " largest", " city", " in", " the", " state", ".\"", " \"", "Far", " larger", " than", " the", " state", " capital", ",", "", " Chil", "p", "anc", "ingo", ".\"", " \"", "The", " name", "", " Ac", "ap", "ul", "co", " comes", " from", " the", "", " Na"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["J", ".", " B", ".", "", " B", "rn", "m", "mer", ".", "", " Wool", "\u23ce", " w", "l", "ne", ",", " Va", ".", " Capital", ":", "", " Max", "l", "m", "um", ",", " J", "5", ".", "000", ";", " ml", "n", "l", "?", "\u23ce", "m", "um", ",", " $"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["and", " it", " would", " appear", " that", " the", " Nine", "", " Real", "ms", " are", " each", " a", " different", " galaxy", " with", " a", " \"", "capital", " planet", "\"", " of", " sorts", " for", " them", ".", "  ", "", "EDIT", ":", " Earth", " as", " a", " whole", " hasn", "'t", " advanced", " enough", ",", " a", " few"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["\u0142", "o\u015bci", " i", " nie", " jes", "tem", " w", " st", "anie", " oc", "en", "i\u0107", ",", " k", "to", " wy", "gr", "a\u0142", "by", " woj", "n", "\u0119", " p", "omi", "\u0119d", "zy", "", " Ros", "j\u0105", " a", "", " Ukra", "i", "n\u0105", ".", " Jest", " to", " trud", "ne", " i", " z\u0142"]}]}], "top_logits": ["albania", "venezuela", "yugoslavia", "bosnia", "macedonia", "nevada", "moldova", "jamaica", "belgium", "slovenia"], "bottom_logits": ["powers", "arts", "bit", "eco", "ern", "genome", "evolved", "gun", "economy", "text"]}