{"index":25362511,"examples_quantiles":[{"quantile_name":"Top Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.82,0.0,0.0,0.0,0.0,0.1,0.28,0.0,0.0,0.3,0.02,0.2,0.71,0.85,0.75,0.0,0.0,0.0,0.0,0.44],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["you"," have"," any"," further"," questions",","," please"," don","'t"," hes","itate"," to"," ask",".","\u23ce\u23ce","Human",":"," Write"," a"," lim","erick"," about"," t","oss","ing"," coins",".","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," lim","erick"," about"," t","oss","ing"," coins",":"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.96,0.73,0.0,0.0,0.0,0.11,0.19,0.0,0.0,0.29,0.03,0.24,0.49,0.72,0.66,0.0,0.0,0.0,0.48,0.58,0.43],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["\u23ce\u23ce","Assistant",":"," You"," will"," have"," to"," solve"," this"," problem"," on"," your"," own",".","\u23ce\u23ce","Human",":"," Write"," a"," lim","erick"," about"," sm","elly"," feet",".","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," lim","erick"," about"," sm","elly"," feet",":","\u23ce\u23ce","There"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.87,0.74,0.17,0.0,0.17,0.0,0.0,0.0,0.0,0.0,0.15,0.29,0.02,0.21,0.49,0.6,0.57,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["were"," removed"," from"," the"," dictionary",","," and"," the"," updated"," dictionary"," was"," printed",".","<EOT>","\u23ce\u23ce","Human",":"," Write"," a"," lim","erick"," about"," a"," boy"," who"," lost"," his"," shoes","\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," lim","erick"," about"," a"," boy"," who"," lost"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.86,0.77,0.16,0.0,0.0,0.0,0.0,0.21,0.0,0.03,0.19,0.05,0.11,0.4,0.71,0.69,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["climate"," change",","," and"," public"," health"," concerns"," such"," as"," pan","dem","ics",".","\u23ce\u23ce","Human",":"," Please"," write"," a"," lim","erick"," about"," a"," spider","'s"," el","bow","\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," lim","erick"," about"," a"," spider","'s"," el","bow"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.82,0.0,0.0,0.0,0.0,0.1,0.28,0.0,0.0,0.3,0.02,0.2,0.71,0.85,0.75,0.0,0.0,0.0,0.0,0.44,0.56],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["have"," any"," further"," questions",","," please"," don","'t"," hes","itate"," to"," ask",".","\u23ce\u23ce","Human",":"," Write"," a"," lim","erick"," about"," t","oss","ing"," coins",".","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," lim","erick"," about"," t","oss","ing"," coins",":","\u23ce\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.79,0.74,0.0,0.0,0.15,0.25,0.0,0.13,0.31,0.0,0.23,0.46,0.6,0.63,0.0,0.0,0.41,0.48,0.48,0.4,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["also"," stop"," acting"," like"," she"," owns"," the"," place"," because"," of"," her"," color","'","<EOT>","\u23ce\u23ce","Human",":"," Write"," a"," lim","erick"," about"," rabb","its"," ","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," lim","erick"," about"," rabb","its",":","\u23ce\u23ce","There"," once"," were"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.82,0.0,0.0,0.0,0.0,0.1,0.29,0.0,0.0,0.3,0.02,0.2,0.71,0.85,0.74,0.0,0.0,0.0,0.0,0.44,0.56,0.41,0.55,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["\u23ce\u23ce","Human",":"," Write"," a"," lim","erick"," about"," t","oss","ing"," coins",".","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," lim","erick"," about"," t","oss","ing"," coins",":","\u23ce\u23ce","There"," once"," was"," a"," gam","bler"," so"," bold",",","\u23ce","Who"," t","oss"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.82,0.32,0.09,0.0,0.0,0.0,0.13,0.41,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["with"," a"," large"," chemical"," capability"," have"," yet"," to"," enter"," into"," the"," treaty",".","<EOT>","\u23ce\u23ce","Human",":"," Write"," a"," lim","erick"," with"," the"," first"," line",":","\u23ce\u23ce","There"," was"," once"," a"," teacher"," named"," NAME","_","1",".","\u23ce\u23ce","Assistant",":"," Here"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.41,0.75,0.03,0.0,0.0,0.0,0.28,0.0,0.1,0.25,0.0,0.04,0.46,0.53,0.0,0.0,0.0,0.0,0.51,0.57,0.31,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["of"," future"," results"," and"," the"," value"," of"," the"," stock"," may"," fluct","u","ate",".","\u23ce\u23ce","Human",":"," Write"," a"," poem"," about","\u2191"," Si","lig","uri","\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," poem"," about","\u2191"," Si","lig","uri",":","\u23ce\u23ce","\u2191","Si"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.31,0.61,0.81,0.0,0.45,0.0,0.0,0.0,0.0,0.0,0.35,0.58,0.59,0.0,0.24,0.51,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["any"," questions"," or"," tasks"," you"," may"," have"," within"," my"," capabilities",".","\u23ce\u23ce","Human",":"," Can"," you"," write"," a"," ha","iku"," about"," investing","\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," ha","iku"," about"," investing",":","\u23ce\u23ce","Markets"," rise"," and"," fall","\u23ce","\u2191"," Patience"]},{"tokens_acts_list":[0.0,0.07,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.29,0.21,0.0,0.0,0.0,0.81,0.25,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["of"," poem"," were"," you"," thinking"," about","?","  ","Would"," you"," want"," it"," to"," be"," a"," ha","iku",","," a"," lim","erick",","," a"," son","net",","," what","?","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","Tell"," me"," the"," eas","iest"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.39,0.61,0.76,0.0,0.0,0.0,0.0,0.16,0.0,0.1,0.0,0.0,0.0,0.43,0.59,0.55,0.0,0.0,0.0,0.0,0.35,0.59,0.04],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["Because"," they"," wanted"," a"," game"," ch","anger"," and"," Microsoft"," had"," the"," Xbox",".","\u23ce\u23ce","Human",":"," Write"," a"," ha","iku"," about"," e","arp","lu","gs","\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," ha","iku"," about"," e","arp","lu","gs",":","\u23ce\u23ce","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.86,0.77,0.16,0.0,0.0,0.0,0.0,0.21,0.0,0.03,0.19,0.05,0.11,0.4,0.71,0.69,0.0,0.0,0.0,0.0,0.0,0.36],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["change",","," and"," public"," health"," concerns"," such"," as"," pan","dem","ics",".","\u23ce\u23ce","Human",":"," Please"," write"," a"," lim","erick"," about"," a"," spider","'s"," el","bow","\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," lim","erick"," about"," a"," spider","'s"," el","bow",":"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.75,0.11,0.0,0.0,0.34,0.0,0.26,0.0,0.0,0.0,0.0,0.0,0.49,0.64,0.48,0.39,0.38,0.41,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["know"," if"," you"," need"," any"," further"," help"," or"," clar","ification",".","<EOT>","\u23ce\u23ce","Human",":"," Tell"," me"," a"," dirty"," lim","erick","\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," mild",","," family","-","friendly"," lim","erick",":","\u23ce\u23ce","There"," once"," was"," a"," cat"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.96,0.73,0.0,0.0,0.0,0.11,0.19,0.0,0.0,0.29,0.03,0.24,0.49,0.72,0.66,0.0,0.0,0.0,0.48,0.58,0.43,0.45],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["Assistant",":"," You"," will"," have"," to"," solve"," this"," problem"," on"," your"," own",".","\u23ce\u23ce","Human",":"," Write"," a"," lim","erick"," about"," sm","elly"," feet",".","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," lim","erick"," about"," sm","elly"," feet",":","\u23ce\u23ce","There"," once"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.87,0.74,0.17,0.0,0.17,0.0,0.0,0.0,0.0,0.0,0.15,0.29,0.02,0.21,0.49,0.6,0.57,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["removed"," from"," the"," dictionary",","," and"," the"," updated"," dictionary"," was"," printed",".","<EOT>","\u23ce\u23ce","Human",":"," Write"," a"," lim","erick"," about"," a"," boy"," who"," lost"," his"," shoes","\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," lim","erick"," about"," a"," boy"," who"," lost"," his"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.18,0.05,0.03,0.53,0.71,0.0,0.0,0.0,0.0,0.0,0.32,0.0,0.0,0.23,0.0,0.11,0.53,0.33,0.32,0.63,0.66,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["to"," December"," ","31","st",","," ","2","023",".","<EOT>","\u23ce\u23ce","Human",":"," Write"," a"," rh","y","ming"," poem"," about"," dij","k","st","ras"," algorithm","\u23ce\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," rh","y","ming"," poem"," about","\u2191"," Dij","k","stra"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.34,0.4,0.25,0.0,0.0,0.12,0.08,0.0,0.37,0.71,0.76,0.64,0.45,0.18,0.22,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["*.","on","c",",","  ","having","  ","this","  ","dist","ich","  ","for","mer","iy","  ","ther","eon","  ",":"," ","\u23ce\u23ce","*","\u2022","  ","De","  ","rc","'","j","num","  ","gen","ere"," p","ater","  ","h","ie","  ","G"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.79,0.74,0.0,0.0,0.15,0.25,0.0,0.13,0.31,0.0,0.23,0.46,0.6,0.63,0.0,0.0,0.41,0.48,0.48,0.4,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["stop"," acting"," like"," she"," owns"," the"," place"," because"," of"," her"," color","'","<EOT>","\u23ce\u23ce","Human",":"," Write"," a"," lim","erick"," about"," rabb","its"," ","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," lim","erick"," about"," rabb","its",":","\u23ce\u23ce","There"," once"," were"," some"]}]},{"quantile_name":"Subsample Interval 0","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.82,0.0,0.0,0.0,0.0,0.1,0.28,0.0,0.0,0.3,0.02,0.2,0.71,0.85,0.75,0.0,0.0,0.0,0.0,0.44],"train_token_ind":20,"is_repeated_datapoint":true,"ha-haiku35_resampled":true,"tokens":["you"," have"," any"," further"," questions",","," please"," don","'t"," hes","itate"," to"," ask",".","\u23ce\u23ce","Human",":"," Write"," a"," lim","erick"," about"," t","oss","ing"," coins",".","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," lim","erick"," about"," t","oss","ing"," coins",":"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.96,0.73,0.0,0.0,0.0,0.11,0.19,0.0,0.0,0.29,0.03,0.24,0.49,0.72,0.66,0.0,0.0,0.0,0.48,0.58,0.43],"train_token_ind":20,"is_repeated_datapoint":true,"ha-haiku35_resampled":true,"tokens":["\u23ce\u23ce","Assistant",":"," You"," will"," have"," to"," solve"," this"," problem"," on"," your"," own",".","\u23ce\u23ce","Human",":"," Write"," a"," lim","erick"," about"," sm","elly"," feet",".","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," lim","erick"," about"," sm","elly"," feet",":","\u23ce\u23ce","There"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.87,0.74,0.17,0.0,0.17,0.0,0.0,0.0,0.0,0.0,0.15,0.29,0.02,0.21,0.49,0.6,0.57,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"ha-haiku35_resampled":true,"tokens":["were"," removed"," from"," the"," dictionary",","," and"," the"," updated"," dictionary"," was"," printed",".","<EOT>","\u23ce\u23ce","Human",":"," Write"," a"," lim","erick"," about"," a"," boy"," who"," lost"," his"," shoes","\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," lim","erick"," about"," a"," boy"," who"," lost"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.86,0.77,0.16,0.0,0.0,0.0,0.0,0.21,0.0,0.03,0.19,0.05,0.11,0.4,0.71,0.69,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"ha-haiku35_resampled":true,"tokens":["climate"," change",","," and"," public"," health"," concerns"," such"," as"," pan","dem","ics",".","\u23ce\u23ce","Human",":"," Please"," write"," a"," lim","erick"," about"," a"," spider","'s"," el","bow","\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," lim","erick"," about"," a"," spider","'s"," el","bow"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.82,0.0,0.0,0.0,0.0,0.1,0.28,0.0,0.0,0.3,0.02,0.2,0.71,0.85,0.75,0.0,0.0,0.0,0.0,0.44,0.56],"train_token_ind":20,"is_repeated_datapoint":true,"ha-haiku35_resampled":true,"tokens":["have"," any"," further"," questions",","," please"," don","'t"," hes","itate"," to"," ask",".","\u23ce\u23ce","Human",":"," Write"," a"," lim","erick"," about"," t","oss","ing"," coins",".","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," lim","erick"," about"," t","oss","ing"," coins",":","\u23ce\u23ce"]}]},{"quantile_name":"Subsample Interval 1","examples":[{"tokens_acts_list":[0.0,0.07,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.29,0.21,0.0,0.0,0.0,0.81,0.25,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["of"," poem"," were"," you"," thinking"," about","?","  ","Would"," you"," want"," it"," to"," be"," a"," ha","iku",","," a"," lim","erick",","," a"," son","net",","," what","?","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","Tell"," me"," the"," eas","iest"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.39,0.61,0.76,0.0,0.0,0.0,0.0,0.16,0.0,0.1,0.0,0.0,0.0,0.43,0.59,0.55,0.0,0.0,0.0,0.0,0.35,0.59,0.04],"train_token_ind":20,"is_repeated_datapoint":true,"ha-haiku35_resampled":true,"tokens":["Because"," they"," wanted"," a"," game"," ch","anger"," and"," Microsoft"," had"," the"," Xbox",".","\u23ce\u23ce","Human",":"," Write"," a"," ha","iku"," about"," e","arp","lu","gs","\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," ha","iku"," about"," e","arp","lu","gs",":","\u23ce\u23ce","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.86,0.77,0.16,0.0,0.0,0.0,0.0,0.21,0.0,0.03,0.19,0.05,0.11,0.4,0.71,0.69,0.0,0.0,0.0,0.0,0.0,0.36],"train_token_ind":20,"is_repeated_datapoint":true,"ha-haiku35_resampled":true,"tokens":["change",","," and"," public"," health"," concerns"," such"," as"," pan","dem","ics",".","\u23ce\u23ce","Human",":"," Please"," write"," a"," lim","erick"," about"," a"," spider","'s"," el","bow","\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," lim","erick"," about"," a"," spider","'s"," el","bow",":"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.75,0.11,0.0,0.0,0.34,0.0,0.26,0.0,0.0,0.0,0.0,0.0,0.49,0.64,0.48,0.39,0.38,0.41,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"ha-haiku35_resampled":true,"tokens":["know"," if"," you"," need"," any"," further"," help"," or"," clar","ification",".","<EOT>","\u23ce\u23ce","Human",":"," Tell"," me"," a"," dirty"," lim","erick","\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," mild",","," family","-","friendly"," lim","erick",":","\u23ce\u23ce","There"," once"," was"," a"," cat"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.96,0.73,0.0,0.0,0.0,0.11,0.19,0.0,0.0,0.29,0.03,0.24,0.49,0.72,0.66,0.0,0.0,0.0,0.48,0.58,0.43,0.45],"train_token_ind":20,"is_repeated_datapoint":true,"ha-haiku35_resampled":true,"tokens":["Assistant",":"," You"," will"," have"," to"," solve"," this"," problem"," on"," your"," own",".","\u23ce\u23ce","Human",":"," Write"," a"," lim","erick"," about"," sm","elly"," feet",".","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," lim","erick"," about"," sm","elly"," feet",":","\u23ce\u23ce","There"," once"]}]},{"quantile_name":"Subsample Interval 2","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,1.0,0.82,0.0,0.0,0.0,0.0,0.1,0.29,0.0,0.0,0.3,0.02,0.2,0.71,0.85,0.74,0.0,0.0,0.0,0.0,0.44,0.56,0.41,0.55,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["Human",":"," Write"," a"," lim","erick"," about"," t","oss","ing"," coins",".","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," lim","erick"," about"," t","oss","ing"," coins",":","\u23ce\u23ce","There"," once"," was"," a"," gam","bler"," so"," bold",",","\u23ce","Who"," t","oss","ed"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13,0.42,0.06,0.0,0.0,0.0,0.0,0.0,0.0,0.07,0.0,0.0,0.71,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.18,0.24,0.12,0.0,0.0,0.04,0.0,0.0,0.51,0.44,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["<EOT>","\u23ce\u23ce","Human",":"," Please"," write"," me"," a"," poem"," about"," the","\u2191"," An","ki"," Vector"," robot","."," Please"," limit"," the"," poem"," to"," a"," maximum"," of"," ","16"," ","lines",".","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," poem"," about"," the","\u2191"," An"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.44,0.14,0.08,0.0,0.0,0.0,0.69,0.21,0.36,0.13,0.09,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["go"," to"," office"," three"," days"," a"," week"," now","."," Can"," you"," write"," a"," poem"," to"," kick"," off"," this"," new"," era","?"," Each"," verse"," should"," have"," three"," words",","," first"," one"," starting"," with"," an"," R",","," second"," one"," with"," a"," T",","]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.31,0.68,0.0,0.0,0.0,0.0,0.0,0.0,0.08,0.0,0.04,0.21,0.39,0.0,0.21,0.29,0.22,0.02,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["specific"," pos","\u0142","owie"," (","Members"," of"," Parliament",")"," will"," be"," excluded",".","<EOT>","\u23ce\u23ce","Human",":"," Create"," a"," poem"," about",":","  ","love","\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," poem"," about"," love",":","\u23ce\u23ce","\"","\u2191","Whis","pers"," of"," the"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.38,0.67,0.0,0.0,0.0,0.75,0.3,0.0,0.05,0.26,0.0,0.07,0.65,0.66,0.0,0.0,0.0,0.71,0.61,0.17,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["estimated"," population"," of"," Jordan"," is"," around"," ","9",".","5"," ","million"," people",".","\u23ce\u23ce","Human",":"," Write"," a"," poem"," about"," general"," relat","ivity"," ","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," poem"," about"," general"," relat","ivity",":","\u23ce\u23ce","\u2191","Spac","etime"]}]},{"quantile_name":"Subsample Interval 3","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.37,0.61,0.0,0.0,0.0,0.0,0.0,0.32,0.0,0.0,0.22,0.0,0.02,0.42,0.49,0.58,0.0,0.0,0.0,0.0,0.0,0.46],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["help"," of"," a"," powerful"," robot",".","\u23ce","10","."," \"","My"," Hero","\u23ce\u23ce"," Human",":"," write"," me"," a"," ha","iku"," about"," the"," j","oys"," of"," anime","\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," ha","iku"," about"," the"," j","oys"," of"," anime",":"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.26,0.55,0.0,0.0,0.06,0.36,0.0,0.0,0.14,0.0,0.0,0.59,0.59,0.0,0.0,0.5,0.52,0.2,0.0,0.0,0.0,0.12],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["and"," forms"," to"," indicate"," that"," something"," has"," been"," done"," or"," approved",".","\u23ce\u23ce","Human",":"," ","\u23ce","Write"," a"," poem"," about"," check","marks",".","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," poem"," about"," check","marks",":","\u23ce\u23ce","\u2191","Tiny","\u2191"," Triumph","\u23ce\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16,0.0,0.3,0.0,0.0,0.12,0.44,0.59,0.19,0.25,0.37,0.37,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[","," and"," the"," rh","yme"," scheme"," is","\u21ea"," A","AB","BA",".","\u23ce\u23ce","Here","'s"," an"," example"," of"," a"," lim","erick",":","\u23ce\u23ce","There"," once"," was"," a"," man"," from"," Peru","\u23ce"," Who","'s"," nose"," simply"," wouldn","'t"," do","\u23ce"," So"," he"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12,0.3,0.58,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.26,0.4,0.42,0.0,0.14,0.36,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["cere","ali"," e"," sal","se",".","\u2191"," Bu","on"," appet","ito","!","<EOT>","\u23ce\u23ce","Human",":"," write"," a"," ha","iku"," about"," grass","\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," ha","iku"," about"," grass",":","\u23ce\u23ce","Green"," bl","ades"," s","way"," soft","ly"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15,0.37,0.57,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.3,0.59,0.47,0.0,0.0,0.0,0.0,0.09,0.48,0.06,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["loro"," for","za",","," no","nost","ante"," le"," diffic","olt","\u00e0",".","<EOT>","\u23ce\u23ce","Human",":"," Write"," a"," ha","iku"," about"," g","ator","ade","\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," ha","iku"," about","\u2191"," G","ator","ade",":","\u23ce\u23ce","\u2191","Bright"]}]},{"quantile_name":"Subsample Interval 4","examples":[{"tokens_acts_list":[0.0,0.0,0.24,0.29,0.38,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.44,0.0,0.0,0.0,0.0,0.0,0.34,0.53,0.19,0.39,0.65,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["b","ello"," po","ema"," sobre"," alej","andro"," p\u00e9","rez",","," un"," you","tu","ber"," de"," tecn","olog\u00eda","\u23ce\u23ce"," Assistant",":","\u2191"," Aqu","\u00ed"," ti","enes"," un"," po","ema"," dedic","ado"," a","\u2191"," Alej","andro","\u2191"," P\u00e9","rez",","," el"," you","tu"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.24,0.48,0.0,0.0,0.0,0.0,0.14,0.0,0.0,0.15,0.0,0.0,0.41,0.59,0.57,0.0,0.0,0.0,0.25,0.44,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["h","ailing"," a"," cab"," on"," the"," street",".","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","Write"," a"," ha","iku"," about"," bath","ro","bes",".","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," ha","iku"," about"," bath","ro","bes",":","\u23ce\u23ce","\u2191","Soft"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.23,0.31,0.49,0.13,0.04,0.0,0.0,0.05,0.0,0.0,0.54,0.0,0.0,0.0,0.13,0.43,0.35,0.46,0.0,0.0,0.0,0.0,0.24,0.14,0.41,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["sch","re","ibe"," ein","\u2191"," Ged","icht"," \u00fcber"," die","\u2191"," Al","pen","\u23ce\u23ce"," Assistant",":","\u2191"," Hier"," ist"," ein","\u2191"," Ged","icht"," \u00fcber"," die","\u2191"," Al","pen",":","\u23ce\u23ce","\u2191","Maj","est","\u00e4t","ische","\u2191"," Ber","ge",","," stein","ern"," und"]}]},{"quantile_name":"Subsample Interval 5","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.11,0.37,0.62,0.32,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.25,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["a"," reminder"," of"," the"," beauty"," and"," power"," of"," true"," friendship",".","<EOT>","\u23ce\u23ce","Human",":"," Can"," you"," write"," a"," ha","iku"," about"," how"," inj","ure"," my"," baby"," would"," sleep","  ","through"," the"," whole","  ","night","?","\u23ce\u23ce","Assistant",":"," Here","'s"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.56,0.0,0.0,0.0,0.06,0.0,0.0,0.33,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["op","?","\u23ce\u23ce","Because"," he"," wanted"," a"," NAME","_","2","-","able"," hair","cut","!","\u23ce\u23ce","Human",":"," Write"," a"," poem"," about"," NAME","_","3","\u23ce\u23ce","Assistant",":"," I"," apolog","ize",","," but"," I"," noticed"," that"," you","'ve"," provided"," placeholder"," names"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.36,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["am","ici","?","\u23ce\u23ce","Assistant",":","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","How"," do"," I"," copyright"," poem"," I"," wrote","?","\u23ce\u23ce","Assistant",":"," To"," copyright"," a"," poem"," you","'ve"," written",","," follow"," these"," steps",":","\u23ce\u23ce","1",".","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.39,0.53,0.07,0.0,0.35,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["?\""," \"","2",".\""," \"","\u21ea","WHAT"," ARE"," YOU","\u21ea"," WRITING",",","\u21ea"," MISS","\u21ea"," LANE","?\""," \"","AN"," O","DE"," TO","\u21ea"," SPRING",".\""," \"","HO","W"," DO"," YOU","\u21ea"," SPELL","\u21ea"," MASSACRE","?\""," \"","U","H","...\""," \"","M"]}]},{"quantile_name":"Subsample Interval 6","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.05,0.09,0.26,0.3,0.2,0.09,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["xxxx","vi",";"," \u00bb"," e"," poi"," rifer","isce"," il"," ","\u23ce","segu","ente"," epit","a","fi","\u00ec","o",":"," ","\u23ce\u23ce","\u00ab","\u2191"," Pi","ctor"," e","ram","j"," n","ec"," eram"," pict","orn","m"," gloria"," par","vas"," ","\u23ce","\u2191","Form"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.0,0.0,0.15,0.3,0.0,0.14,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ours",","," c","'","est","-","\u00e0","-","dire"," ex","al","ter",","," en"," des"," vers"," parf","ois"," magn","if","iques",","," l","'","esp","\u00e9","rance",","," la"," bon","t\u00e9",","," le"," d","\u00e9v","ou","ement",","," le"," par","don"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12,0.29,0.0,0.07,0.0,0.0,0.0,0.0,0.1,0.0,0.32,0.0,0.0,0.0,0.0,0.34,0.0,0.15,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["  ","la","  ","content","ezza","  ","ci","  ","ha","  ","re","-"," ","\u23ce","cit","ato"," un","  ","son","etto","  ","di","  ","\u2191","F","osc","olo","  ","sop","ra","  ","\u2191","Napole","one"," ","\u23ce","che","  ","\u00e8","  ","ver"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.29,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["successful"," American"," tech"," companies",".","<EOT>","\u23ce\u23ce","Human",":","\u2191"," Improve","\u21ea"," ONLY"," the"," ","2","."," line"," of"," this"," rap"," for"," rh","y","iming"," and"," better"," met","rum",":"," ","1","."," Listen"," up",","," NAME","_","1",","," I"]}]},{"quantile_name":"Subsample Interval 7","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.04,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["irt"," der","\u2191"," Sch","rift","\u017f","t","eller",","," wor","auf"," der","\u2191"," Mu","\u017f","iker"," ant","wor","tet",":"," ","\u23ce\u23ce","\u201e","\u2191","Und"," er"," bek","am"," ","\u017f","ein","\u2191"," Op","fer",","," d","enn"," w\u00e4hrend"," ich"," me","inen"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.04,0.02,0.0,0.0,0.0,0.0,0.0,0.15,0.18,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["uts","  ","sont","  ","encore"," ","\u23ce","plus","  ","ch","oqu","ants","  ","dans","  ","cette","  ","autre","  ","\u00e9","pi","gram","me",",","  ","ad","res","\u23ce"," s","\u00e9e"," \u00e0","  ","une","  ","fem","me","  ","qui","  ","chass","ait"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\"","He","'s"," such"," a"," j","erk",".\""," \"","Sure",".\""," \"","Sound"," like"," a"," horse",".\""," \"","\\xe2\\x99","\\xaa",""," We"," thank"," you",","," food","\""," \"","\\xe2\\x99","\\xaa",""," The"," food","\""," \"","\\xe2\\x99","\\xaa",""," That"," n","our"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["i","\u23ce"," ne"," year",","," $","40","."," ",",,","\u23ce","\u2191","Obitu","ary","\u2191"," Notices",","," In"," prose"," or"," verse",","," la","\u23ce"," e","ents"," per"," line",",","\u2191"," Notices"," of","\u2191"," Births","."," Mar","\u23ce"," ri","ages",","," Deaths"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.29,0.08,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.19,0.0,0.0,0.04,0.0,0.3,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ns","teren"," und"," wen","iger"," un","gesch","l","iff","enen","\u2191"," Ton"," an","gen","ommen"," .","\u2191"," Fol","gende","\u2191"," Verse"," auf","\u2191"," Herr","n"," de"," la","\u2191"," Roch","ej","aqu","elin"," m","achen"," j","etzt"," auf"," der","\u2191"," Stra","\u00dfe"]}]},{"quantile_name":"Subsample Interval 8","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ied"," vom"," br","aven"," Mann"," k","lag","ten",","," dann"," w\u00fcr","den"," Sie"," es"," w","ohl"," so"," m","achen",":","\u2191"," H","och"," k","lin","gt"," s"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.17,0.0,0.0,0.0,0.18,0.1,0.0,0.25,0.0,0.08,0.16,0.12,0.0,0.0,0.0,0.16,0.38,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["solid","ar","it\u00e9",".","\u2191"," Pass","ons"," la"," par","ole"," au"," j","oy","eux"," po","\u00e8te"," cho","let","ais",","," aut","eur"," d","'","une"," chan","son"," de"," circ","onst","ance"," qui"," obt","int"," un"," grand"," et"," l\u00e9","gi","time"," succ"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.18,0.13,0.0,0.1,0.1,0.2,0.0,0.14,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["sse",","," qui"," depuis"," fut"," r","eine"," de","\u2191"," Su","\u00e8de",","," lui"," a","dr","essa"," ces"," vers"," ch","arm","ants"," :"," ","\u23ce","\u2191","So","uvent"," un"," peu"," de"," v","\u00e9r","it\u00e9"," ","\u23ce","Se"," m","\u00ea","le"," au"," plus"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["you"," like"," it","?\""," \"","This"," is"," a"," Japanese"," folk"," song",",\""," \"\"","My"," Home",".\"","\""," \"","It"," is"," about"," hills"," and"," rabb","its",","," and"," streams"," full"," fish",".\""," \"","It"," is"," the"," most"," beautiful"," song"," in"," the"," world"]}]},{"quantile_name":"Bottom Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["iten","  ","be","\u00a7",""," ","\u23ce","fd",")","led","}","t","eft","c","n","  ","","\u00a9","eb","ic","^","t","S","  ","bef","to","  ","inn","iger","  ","un","b","  ","f","eine","  ","{","^","lec","len","  "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u2191","O","oh",","," deep","-","","fried"," robot",".\""," \"","#"," Just"," tell"," me"," why","?\""," \"#","\""," \"","#"," Please"," read"," this"," ","55","-","page"," warrant"," #","\""," \"","#"," There"," must"," be"," robots"," worse"," than"," I"," #"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ore","."," ","\u23ce\u23ce","\u2191","Mais"," ,"," ma"," m","\u00e8re"," ,"," que"," chan","te","rai","-","je"," ?"," ","\u23ce\u23ce","\u2191","Mad","ame"," G"," U"," I"," B"," E"," R"," T","."," ","\u23ce\u23ce","Ce"," qui"," vous"," pla","ira",".","\u2191"," All"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["oked"," ","\u23ce","lanes","."," I"," offer"," it"," to"," the"," M","i","it","no","R"," m"," an"," ","\u23ce\u23ce\u23ce\u23ce","THE","\u21ea"," MIH","ROR","."," ","\u23ce\u23ce\u23ce\u23ce","I","Q","B"," ","\u23ce\u23ce\u23ce\u23ce","am","mi","lig"," novel","ty"," for"," the"," enter","tat","n"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.16,0.09,0.02,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["and","  ","sale","  ","of","  ","their"," ","\u23ce","He","  ","tells","  ","in","  ","tragic","  ","str","ains",",","  ","how","  ","**","the"," ","\u23ce","of","  ","the","  ","sick",",","  ","and","  ","","ue","  ","clos","ets"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.18,0.08,0.2,0.06,0.04,0.02,0.0,0.0,0.08,0.15,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["t"," beside"," me"," !"," ","\u23ce\u23ce","B","."," ","\u23ce\u23ce\u23ce\u23ce","\u21ea","M","INST","REL","\u21ea"," BALL","AD","."," ","\u23ce\u23ce","\u21ea","WI","UTT","BN"," ON"," ","4"," ","\u21ea","FL","YL","B","AF"," OF"," A","\u21ea"," V","OLU","I","IK"," OF"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.26,0.07,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.05,0.05,0.0,0.0,0.11,0.0,0.0,0.0,0.12,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\"","Action","\":"," \"\"","\u23ce","}","<EOT>","\u23ce\u23ce","Human",":","\u2191"," Rec","\u00edt","ame"," un"," po","ema"," de"," ","100"," ","est","rof","as"," esc","rito"," en"," verso"," inspir","ado"," por"," \"","La","\u2191"," Vida"," es","\u2191"," S","ue","\u00f1o","\""]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[".","\u2191"," Snow","ball","\u2191"," Matching"," the","\u2191"," P","oa"," Lisa","\u23ce","7","."," the"," Fall"," of"," the"," House"," of","\u2191"," Us","her","\u23ce","8",".","\u2191"," Pit"," and"," the","\u2191"," M","ole","\u23ce","9","."," The"," Fall"," of"," the"," House"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.35,0.26,0.16,0.08,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ally"," un","ac","cept","able",".","\u23ce\u23ce","Human",":"," ","\u23ce","Would"," you"," be"," able"," to"," help"," me"," re","write"," the"," nurs","ery"," rh","yme"," to"," be"," more"," cand","id"," for"," a"," generation"," who"," will"," be"," und","eni","ably"," affected"," by"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ui","  ","1","851","."," ","\u23ce\u23ce\u23ce","\u2191","Mel",".:","    ","\u2191","G","utter","  ","","omb","ord","."," ","\u23ce\u23ce\u23ce","\u2191","S","ving","  ","\u2191","Dig","  ","i","  ","Sky",","," ","\u23ce\u23ce","\u2191","Sang","f","u","gl","!","    "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["sch","re","ibe"," einen"," sehr"," herz","lichen","\u2191"," Geb","urt","sta","gs","gru","\u00df"," im","\u2191"," Stil"," von"," Bob"," der","\u2191"," Bau","me","ister"," an","\u2191"," W","illi",".","\u2191"," Be","nut","ze"," v","iele"," motiv","i","erende","\u2191"," Fl","osk"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["street",","," was"," han"," ","ied"," the"," following"," let","-"," ter","\u23ce"," by"," Walter"," L",".."," Butler",";","?","\u23ce","\u2191","Tor","kv","ii",".","l","c",","," June"," ","10",","," ","1","870",".","\u23ce","My","\u2191"," Df","ar"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["que"," Le","\u2191"," B","raz"," a"," magist","r","alement"," ex","al","t\u00e9"," dans"," la"," cour","te"," p","i\u00e8","ce"," :"," A","\u2191"," Vill","iers"," qui"," figure"," dans"," les","\u2191"," Po","\u00e8","mes"," vot","ifs"," (","\u2191","Cal","mann","-","\u2191","L\u00e9"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["Go"," and"," find"," your"," way"," in"," this"," w","icked"," world",".\""," \"[","gar","field"," crying","]","\""," \"[","hip","-","hop"," music","]","\""," \"","Well",","," I","'m"," off"," to"," meet"," and"," im","press"," cel","ery","'s"," family",".\""," \""]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["be"," hung",","," without","\u23ce"," the"," cerem","oney"," or"," a"," trial",".","\u23ce","\u21ea","WARREN","\u21ea"," PRICE","\u21ea"," CURRENT",".","\u23ce","\u21ea","WARREN","\u21ea"," PRICE","\u21ea"," CURRENT",".","\u21ea"," CORR","ECTED","\u21ea"," WEEKLY"," BY","\u21ea"," FR","EER"," A","\u21ea"," SMITH","\u21ea"]},{"tokens_acts_list":[0.0,0.06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\"","#","\u2191"," Me","rr","ily",","," me","rr","ily",","," me","rr","ily",","," me","rr","ily","\""," \"","#"," Life"," is"," Max","'s"," dream","\""," \"","Sugar"," will"," give"," you"," night","m","ares",".\""," \""," How"," much"," time"," do"]}]}],"top_logits":["dedicated","celebrating","sung","Oh","Hub","Ole","Big","dedic","\uff08","AUTO"],"bottom_logits":["reich","\u043c\u0430\u043f\u0438","\u044f\u0441\u0442","numberUS","\u306f\u306f","tej","ovj","\f","\u0011"]}