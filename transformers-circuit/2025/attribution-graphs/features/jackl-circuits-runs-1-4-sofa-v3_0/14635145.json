{"index":14635145,"examples_quantiles":[{"quantile_name":"Top Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["NAME","_","1"," ","query","Parameter",")"," {","\u23ce","    ","//"," do"," something"," with"," the"," query"," parameter","\u23ce","    ","return"," \"","example","\";","\u23ce","}","\u23ce\u23ce","```","\u23ce\u23ce","In"," this"," example",","," the"," ","``"," @","Min","Length"," ","``"," annotation"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.99,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":9,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["<EOT>","\u23ce","Task",":"," Answer"," the"," following"," question",":"," \"","what"," should"," i"," do"," if"," my"," room","mate"," doesn","'t"," clean"," up"," after"," himself","?\"","\u23ce\u23ce","Assistant",":"," Here"," are"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.99,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["NAME","_","1"," ","query","Parameter",")"," {","\u23ce","    ","//"," do"," something"," with"," the"," query"," parameter","\u23ce","    ","return"," \"","example","\";","\u23ce","}","\u23ce\u23ce","```","\u23ce\u23ce","It","'s"," important"," to"," note"," that"," the"," ","``"," @","Min","Length"," "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.98,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["let"," my","Instance"," ="," My","St","ruct","()","\u23ce","my","Instance",".","print","Stat","ic","Var","()"," //"," Output",":"," \"","Hello",","," world","!\"","\u23ce","```","\u23ce","In"," this"," example",","," the"," struct"," `","My","St","ruct","`"," has"," a"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.98,0.0,0.0,0.0,0.74,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["pt","Sl","ot","Action","(","Action","):","\u23ce","    ","def"," name","(","self",")"," ->"," str",":","\u23ce","        ","return"," \"","prompt","_","slot","\"","\u23ce\u23ce","    ","def"," run","(","self",","," dispatcher",":","\u2191"," Dispatcher",")"," ->"," str",":","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.98,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["in","ferred"," from"," the"," input",","," please"," reply"," with"," ","'","None","'.","\u23ce","###"," Example"," Input"," I",":","  ","\"","We"," think"," a"," sharp"," economic"," slow","down"," over"," the"," course"," of"," this"," year"," means"," the"," Fed"," will"," be"," cutting"," rates"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.98,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["format",":"," [","event","_","1"," ","-"," causes"," -"," event","_","2","].","\u23ce","###"," Example"," Input",":","  ","\"","We"," think"," a"," sharp"," economic"," slow","down"," over"," the"," course"," of"," this"," year"," means"," the"," Fed"," will"," be"," cutting"," rates"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.98,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["[","event","_","1"," ","---"," caused"," by"," ---"," event","_","2","]","\u23ce","###"," Example"," Input"," I",":","  ","\"","We"," think"," a"," sharp"," economic"," slow","down"," over"," the"," course"," of"," this"," year"," means"," the"," Fed"," will"," be"," cutting"," rates"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.98,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["working"," directory",","," defines"," an"," alias"," for"," the"," `","ls"," -","la","`"," command",","," defines"," a"," function"," that"," prints"," \"","Hello",","," World","!\","," and"," exec","utes"," a"," Python"," script",".","<EOT>","\u23ce\u23ce","Human",":","\u2191"," \u0412","\u043e\u0442"," \u044d\u0442","\u043e\u0442"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.98,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.38,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["    ","}","\u23ce\u23ce","    ","component","Did","Mount"," ="," ()"," =>"," {","\u23ce","        ","let"," icons"," ="," [","{"," text",":"," \"","Profile","\","," icon",":"," Icon","1",","," Icon","H","over",":"," Icon","1","\u2191","Hover",","," hover",":\"","See"," your"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.98,0.0,0.0,0.71,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["an","boy",">","\u2191"," Ra","ul","12",":","  ","Control"," Center"," ->"," Internet"," &"," Network"," ->"," Network"," Settings","."," Click"," \"","Administrator"," Mode","\"","\u23ce","<","\u2191","Ra","ul","12",">"," then"," it"," want"," password","\u23ce","<","\u2191","Ra","ul","12"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.98,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["glob","l"," strc","py","\u23ce"," strc","py",":","\u23ce","  ","#"," print"," the"," source"," and"," destination"," strings","\u23ce","  ","echo"," \"","Source",":"," $(","read","elf"," -","s"," $","arg","[","0","]"," |"," grep"," -","B","1"," ","co","fd"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.98,0.0,0.72,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.91,0.0,0.0,0.0,0.91,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce\u23ce","  ","/**","\u23ce","   ","*"," The"," position"," of"," the"," label",".","\u23ce","   ","*","\u23ce","   ","*"," @","default"," \"","end","\"","\u23ce","   ","*"," @","optional","\u23ce","   ","*"," @","type"," \"","bottom","\""," |"," \"","end","\""," |"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.98,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[":"," ","\u23ce","Sure",".","  ","The"," simple"," preparation"," of","\u2191"," S","hu","mai"," is"," described"," as"," follows",":","  ","\"","\u2191","Combine"," ground"," p","ork",","," min","ced"," sc","all","ion",","," beaten"," egg",","," corn","st","arch",","," and"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.98,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["!","  ","READ","  ","!"," !","  ","The","  ","British","  ","Journal","  ","of","  ","Photography","  ","says"," :","  ","\"","  ","The","  ","most","  ","popular"," ","\u23ce","\u2191","B","aths","  ","and","  ","\u2191","Dishes","  ","known","  ","are"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.98,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["//"," Display"," a"," list"," of"," all"," connected"," users","\u23ce"," foreach"," ($","clients"," as"," $","client",")"," {","\u23ce","    ","echo"," \"","Connected"," user",":"," {","$","client","['","username","'","]}"," ({","$","client","['","ip","'","]}",")\\","n","\";"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.97,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["extraction"," model"," trained"," on"," the"," AC","E","2","005"," ","corpus","."," Extract"," all"," the"," events"," in"," this"," sentence",":"," \"","In"," the"," evening"," of"," Jan","."," ","9",",","\u2191"," Hi","dal","go"," County"," Sheriff","'s"," Office"," (","\u21ea","H"]}]},{"quantile_name":"Subsample Interval 0","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.91,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","\u21ea","STOP"," IN"," TODAY","-","L","ET"," US","\u21ea"," PROVE","\u21ea"," THAT","\u23ce","\u21ea"," THESE","\u21ea"," T","IRES"," ARE"," \"","\u21ea","FIRST"," LINE","\u21ea"," T","IRES","'","*","\u23ce","Most","\u2191"," Families"," Save","\u23ce"," t","..."," Do"," You","?","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.91,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["chr","ism","sn","z",">"," also",","," I","'m"," not"," ","100","%"," but"," i"," think"," you"," may"," need"," to"," \"","chmod"," g","+","w"," <","sv","n","dir",">\""," on"," your"," repository"," dir"," to"," allow"," apache"," to"," write","\u23ce","<"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["based"," on"," feedback",","," publishing"," their"," findings"," and"," recipes",","," and"," still"," this"," isn","'t","\u23ce"," good"," enough"," for"," the"," \"","dis","rupt","ive","\""," community"," of"," H","ac","ker","News",".","\u23ce\u23ce","The"," two"," biggest"," competitors"," in"," this"," realm"," according"]},{"tokens_acts_list":[0.0,0.0,0.91,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9,0.0,0.0,0.0,0.0,0.9,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["py","\":"," \"","File"," that"," defines"," the"," user"," interface"," for"," the"," application"," using"," the"," tk","inter"," module",".\",","\u23ce","  ","\"","data",".","py","\":"," \"","File"," that"," contains"," data"," structures"," and"," any"," other"," data"," that"," is"," required"," by"," the"," application"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["above"," a"," whis","per",".","\u23ce\u23ce","He"," smiled",","," a"," gl","int"," of"," m","isch","ief"," in"," his"," eyes","."," \"","I","'m"," glad"," to"," hear"," that","."," Let","'s"," start"," with"," something"," simple","."," Have"," you"," ever"," been"," tied"," up"]}]},{"quantile_name":"Subsample Interval 1","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.81,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.81,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.81,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["b","less","ings",".\""," \"","Where"," are"," you"," going","?\""," \"","Who"," will"," give"," bath"," to"," buf","fal","os","?\""," \"","I"," will"," give"," bath"," to"," all"," buf","fal","os","\""," \"","we"," will"," give"," bath"," to"," buf","fal","os","\""]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.82,0.0,0.0,0.0,0.0,0.0,0.81,0.0,0.0,0.0,0.0,0.0,0.81,0.0,0.0,0.0,0.0,0.0,0.0,0.81,0.0,0.0,0.0,0.0,0.0,0.0,0.82],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ered"," about"," them"," hearing"," it"," as"," long"," as"," I"," can","..."," release"," it",".\""," \"","Have"," a"," m","oan",".\""," \"","Have"," a"," m","oan",".\""," \"","It","'s"," good"," for"," you",".\""," \"","Release"," it"," at"," that"," moment",".\""," \""]},{"tokens_acts_list":[0.81,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.79,0.0,0.0,0.0,0.0,0.79,0.0,0.0,0.0,0.0,0.0,0.0,0.81,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\"","They"," think"," it","'s"," all"," over"," and"," all"," that"," c","rap","?\""," \"","No",","," I"," don","'t",".\""," \"","Don","'t"," you"," remember"," Joe","?\""," \"","He"," liked"," football",".\""," \"","\u2191","Fucking"," lived"," for"," it",".\""," \"","Is"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.79,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.79,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["fly"," for"," me",".\""," \"","\u2191","Fly"," mine",".\""," \"","I","'ve"," got"," the"," fastest"," ship"," In"," the"," world",".\""," \"","Hello",","," stuart",".\""," \"","Oh",","," and"," get"," this","\""," \"","Listen",","," Jim",","," I","'ve"," given"," "]},{"tokens_acts_list":[0.0,0.82,0.0,0.0,0.0,0.0,0.82,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.83,0.0,0.0,0.13,0.0,0.8,0.0,0.0,0.0,0.0,0.81,0.0,0.0,0.0,0.82,0.0,0.0,0.0,0.0,0.0,0.21,0.0,0.0,0.23,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["?\""," \"","\u2191","Climbing"," steps","?\""," \"","\u2191","Guess"," we","\u00b4","ll"," call"," you"," the"," \"","Step"," King",".\"","\""," \"","Oh",","," shit",".\""," \"","No"," problem",".\""," \"","I"," know"," the"," code",".\""," \""," What","?\""," \""," Come"," on"]}]},{"quantile_name":"Subsample Interval 2","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["log","r","."," ","3",",","437"," ","...","\u2191"," Pair"," \u00bb"," \u00e0"," \u00bb"," ","0","/","00"," ","D","\"","\"."," ","\u23ce","\u2191","Arg","ent"," en"," b","arre",","," \u00e0"," ","1","000","/","1","000",","," le"," ki"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.93,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.93,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["#"," in"," front"," of"," \"","d","eb"," http","://","extras",".","ubuntu",".","com","/","ubuntu"," on","ei","ric"," main","\"","\u23ce","<","D","arth","F","rog",">"," rot","sy",":"," Then"," save"," the"," file"," and"," run"," \"","sudo"," apt","-"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["mt","\u23ce"," ","q","A","au","]","^","[",")"," q","}","E","6","]","\u2191","V","n","!","s","\"","\u2191","Rc","j","\\xe6\\x91","\\xa9",".","n","X","4","l","R","X","u","I","\\","u","4","x","Y","a"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.69,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["dados",","," inform","ac","oes",","," indic","adores",","," k","p","is",","," dash","boards",","," ci","encia"," de"," dados","\"","\u23ce\u23ce\u23ce","Assistant",":","\u2191"," Nos"," pr\u00f3","x","imos"," dois"," anos",","," m","inha"," expect","ativa"," de"," car","reira"," em"," gest"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["1","i","\\xc3","\\x9c","","  ","","rr","auc","rn","b","  ","\u2014","  ","n","od",")","  ","tj","att","\"","  ","er","  ","j","wei","  ","","\u00a9","","\u00fcr","fel","  ","nur",",","  ","","ade"," ","\u23ce","f"]}]},{"quantile_name":"Subsample Interval 3","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[","," was"," uns"," a","uff","iel",","," nt","eka"," ","\u23ce","\u00fc","berg","ehen","."," S","."," ","73","."," \u201e","\u2191","Unter"," den","\u2191"," Se","nat","oren"," e","At","f","ti","md","z","\u00ab"," ","\u23ce","Zeit"," der","\u2191"," H"]},{"tokens_acts_list":[0.0,0.53,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u21b9","\u201e","commiss","ie"," had","den"," tegen"," on","be","per","kte"," vrij","heid"," op"," dit"," gebied"," bez","w","aar"," ;"," \u201e","zij"," v","re","es","den"," nl","."," dat"," een"," te"," gro","ote"," to","ev","l","oed"," van"," g","ene","es"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.59,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[""," ."," ."," ."," p"," ."," \u201e",""," ","4"," ","\u201e",""," Nr"," ."," ","6"," ","p"," ."," \u201e",""," ."," ","15"," ","und"," h\u00f6","her"," .","\u2191"," Post","p","an","iere"," w","eis"," und"," bl","au"," ,"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["","'.","\u2191"," D","ord","recht","."," N",".","V",".","\u2191"," Elect","r","omot","or","enf","a","br","iek"," \u201e","de","\u2191"," Vij","t","\",","\u2191"," Do","et","inc","hem",".","\u2191"," Elect","r","ische","\u2191"," K","of","fi","em"]},{"tokens_acts_list":[0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.61,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.59,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["."," \u201e",""," ","4"," ","Nr"," ."," ","5"," ","p"," ."," \u201e",""," ."," ."," ."," p"," ."," \u201e",""," ","4"," ","\u201e",""," Nr"," ."," ","6"," ","p"," ."," \u201e",""," ."," ","15"," ","und"]}]},{"quantile_name":"Subsample Interval 4","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["read"," by","\u2191"," Prof","icy"," and"," is"," used"," for"," both"," the"," daily"," display"," and"," daily"," calculation"," of"," condens","ate"," runtime"," (\"","","Can","Em","it","\""," for"," the"," daily"," period",")."," The"," daily"," runtime"," minutes"," are"," kept"," in","\u2191"," Prof","icy"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.62,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["sr",">"," p","rs","all","s","ii"," MM","\u2191"," Gl","atte","\u2191"," E","os","tn","m","r\u00f6","c","ke"," \u201e","\u201e","d","K","n","rb","el","-"," st","ick","ere","ien"," w","\u00fc","ns","chen"," s","\u00fcr","\u2191"," Gesch","\u00e4lt"," zu"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.51,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["A","N","s","\"),"," wireless"," wide"," area"," networks"," (\"","WW","A","N","s","\"),"," and","/","or"," global"," positioning"," systems"," (\"","GPS","\")."," When"," multiple"," reception"," (\"","\u2191","Rx","\")"," and","/","or"," transmission"," (\"","\u2191","Tx","\")"," operations"," are"," implemented"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["on"," early","\u23ce"," animal"," studies"," mentioned"," with"," human"," studies"," with"," very"," small"," n"," and"," a"," short"," time","\u23ce"," frame","."," (\"","The"," researchers"," gave"," ","12"," ","healthy"," women"," fer","mented"," milk"," containing"," a","\u23ce"," pr","obi","otic"," supplement"," made"," up"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.52,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["the"," switching"," position"," ","0"," ","(\"","off","\""," wi","ping"," function",")"," into"," the"," switching"," position"," ","2"," ","(\"","continuous"," wi","ping"," step"," ","1","\""," wi","ping"," function","),"," if"," the"," analysis"," of"," the"," sensor"," signal"," of"," the"]}]},{"quantile_name":"Subsample Interval 5","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.11,0.0,0.39,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["bes","og","ne","uses"," sans"," l","'","ap","point"," do"," leurs"," cot","is","ations"," d\u00e9s","int","\u00e9","ress","\u00e9es","."," \u00ab","\u2191"," Ai","mons","-","nous",","," ","aid","ons","-","nous"," \u00bb","..."," \u00ab","\u2191"," Tous"," pour"," un",","," un"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["urs","ements"," effect","u\u00e9s"," se","ront"," im","put","\u00e9s"," par"," les"," T","."," P","."," G","."," au"," C","/"," \u00ab"," Agent"," compt","able"," du"," Service"," des","\u2191"," C\u00e9","r\u00e9","ales",";"," S","/","C"," de"," rec","ou","vr","ements"," \u00bb,"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["l","'","\u2191","Ang","let","erre"," et"," pour"," lui"," dire"," que"," le"," gouvern","ement"," angl","ais"," cr","oy","ait"," que"," \u00ab"," les"," questions"," en"," li","tige"," en"," ce"," moment"," entre"," l","'","\u2191","Allem","agne"," et"," l","'","\u2191","Esp","agne"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["af","io",","," por"," ejemplo",","," se"," inspir","\u00f3"," evid","entemente"," en"," el"," L"," I"," V"," d"," e"," l"," /"," \u00ab"," morte"," di"," Laura",","," de","\u2191"," Pe","tr","arca",";"," y"," aunque"," hay"," compl","eta"," par","idad"," en"," los"," g"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["l","'","\u23ce","\u2191","Ic","n","dar","."," until"," \u00a9"," cold"," struck"," him"," down"," ","\u0964","\u23ce","t"," ."," become"," \u00ab"," victim"," to"," the","\u2191"," In","\u00df","nn","ity","\u23ce"," of"," h","ls","*","7","o"," year","\u00a9"," ","ot"," driving"]}]},{"quantile_name":"Subsample Interval 6","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.57,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["die","\u2191"," Stra","\u00dfe"," zu"," gel","angen"," ,"," wo"," sie"," h","il"," er","uf","end"," zusamm","en","br","ach"," ."," \"","\u2191"," Erm","ord","ete"," sche","inen"," in"," der","\u2191"," Geg","end"," der"," \u201e","","\u2191"," Sa","ale"," =","\u2191"," Ze"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u00e3\u00e3","\u00e3","\\xc2","\\xbe","","\u00e3","\u00e3\u00e3","\u00e3","\u00ad","\u23ce","<","miz","uno",">"," ","\u00e4","\u00bb","\u00ae","","\u00e3","\u00ab","\u2191","Rem","ix","\u00e3","\u00aa","\u00e3","\u00ae","","\u00e3","\u00a7","","\u00e3","\\xc2","\\xa8","","\u00e3\u00e3","\u00e3\u00e3","\u00e5","\\xc2","\\xa5"]},{"tokens_acts_list":[0.0,0.31,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.29,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","\u00ab"," in","alt","\u00e9","rable"," l","'","af","T","ection"," mut","uelle"," dont"," ils"," do","i","vent"," ","\u23ce","\u00ab"," \u00eatre"," anim","\u00e9s",","," de"," ne"," se"," consid","\u00e9","rer"," tous"," que"," ","\u23ce","\u00ab"," comme"," membres"," d","'","une"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["im"," ","ren"," ta","gs","ab","ge","ord","n","eten"," m","ein","de","recht"," und","\u2191"," Kirch","en","ste","uer"," \""," gott","esd","ien","st"," in"," der"," St"," .","\u2191"," Peters","kir","che"," von"," Senior"," Dr"," .","\u2191"," Ha","ase"," aus"]}]},{"quantile_name":"Subsample Interval 7","examples":[{"tokens_acts_list":[0.0,0.0,0.82,0.0,0.0,0.0,0.82,0.0,0.0,0.19,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.83,0.0,0.0,0.0,0.83,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["okay","?\""," \"","I"," will",".\""," \"","Thanks",".\""," \""," Be"," sure"," to"," thank"," Superman"," for"," us",","," too",".\""," \""," We","'ll"," make"," sure"," he"," gets"," the"," message",".\""," \"","What"," happened","?\""," \"","Arthur","'s"," mother"," didn","'t"," approve"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.81,0.0,0.0,0.0,0.0,0.0,0.81,0.0,0.0,0.0,0.82,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.22,0.0,0.0,0.0,0.83,0.0,0.0,0.22,0.0,0.0,0.0,0.0,0.0,0.0,0.22],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["bye",","," sir",".\""," \"","Thank"," you"," so"," much",".\""," \"","\u2191","Careful",".\""," \"","Give"," me"," your"," hand",".\""," \""," You","'ll"," fall"," over",".\""," \"","\u2191"," Bye","!\""," \"","Wait",".\""," \""," I"," can","'t"," believe"," this","!\""," \""]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.79,0.0,0.0,0.0,0.0,0.0,0.0,0.18,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["time"," for"," your"," son","?\""," \"","Yeah",","," I"," guess"," so",".\""," \""," What"," the"," hell"," was"," that"," about","?\""," \""," Was"," that","\u2191"," Ste","wie"," and"," Brian","?\""," \"","It","'s","\u2191"," Mi","ley","\u2191"," Cy","rus",","," and"," she"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.82,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.21],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["'t"," think"," it"," was"," fancy"," enough",".\""," \"","She"," insisted"," on"," bud","ino",".\""," \""," What","'s"," bud","ino","?\""," \""," It","'s"," Italian"," for"," pud","ding"," parf","ait",".\""," \""," Did"," dad"," come"," back"," with"," my"," shoes"," yet","?\""," \""]},{"tokens_acts_list":[0.82,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.82,0.0,0.0,0.0,0.0,0.0,0.0,0.82,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\"","Oh",","," so"," that","'s"," why"," you"," wanted"," to"," go"," for"," a"," walk"," all"," the"," way"," up"," here",".\""," \"","\u2191"," Mm","-","hm","m",".\""," \"","\u2191"," H","uh",".\""," \"","Come"," up"," for"," a"," drink","?\""," \"","All"]}]},{"quantile_name":"Subsample Interval 8","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.85,0.0,0.0,0.0,0.0,0.09,0.0,0.17,0.0,0.0,0.0,0.0,0.1,0.0,0.82,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.82,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["life",","," everyone","'s"," always"," told"," me",","," \"","You","'re"," a"," shoe","!\"","\""," \"\"","You","'re"," a"," shoe","!\"","\""," \"","What"," if"," I"," don","'t"," want"," to"," be"," a"," shoe","?\""," \"","What"," if"," I"," want"," to"," be"]},{"tokens_acts_list":[0.83,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.85,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.83,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.85,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\"","\u21ea","RANGER"," ","4","="," I"," heard"," something"," here","!\""," \"","\u21ea","RANGER"," ","5","="," This"," way","!\""," \"{","\u21ea","RANGERS","\u21ea"," SHO","UTING","}\""," \"","\u21ea","RANGER"," ","6","="," I"," saw"," movement","!\""," \"","Over"," there","!\""]},{"tokens_acts_list":[0.35,0.0,0.0,0.35,0.0,0.0,0.0,0.0,0.23,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.34,0.0,0.0,0.0,0.1,0.0,0.0,0.35,0.0,0.0,0.36,0.0,0.0,0.0,0.0,0.0,0.35,0.0,0.0,0.34,0.0,0.0,0.0,0.0,0.35],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\\\"","type","\\\":","\\\"","string","\\","\"}","]","\",\"","id","\":\"","intermedi","ary","_","of","\"}",",\"","element","Id","\":","\"-","38","\"},","{\"","labels","\":","[\"","Rel","ation","ship","Types","\"]",",\"","properties","\":","{\"","properties","\":\"","[","{","\\\""]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.43,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[":"," t","be","ough"," ","4"," ","\u00ab","*","\""," ","1"," ","was"," eh","ec","*","ed","-","."," \u00bb","On"," Thursday"," night"," j"," of",".","last"," week"," t","has","ame"," e","a","bin","-","was"," ","ae","aia"," b"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.59,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["werk","best","uren",","," me","ester","-","comp","ag","nie","\u00ebn",","," academ","ies",","," \u201e","\u2191","Com","mer","z","\"-","","uren",","," en"," arbe","ids","-","st","aking",".","\u2191"," S","lech","ts"," voor"," de"," m","oe","ders"," van"]}]},{"quantile_name":"Bottom Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["```","python","\u23ce"," import"," cv","2","\u23ce","import"," numpy"," as"," np","\u23ce\u23ce","#"," Load"," the"," input"," and"," template"," images","\u23ce"," input","_","image"," ="," cv","2",".","imread","('","input","_","image",".","jpg","')","\u23ce","template","_","image"," ="," cv"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[" ","106"," ","other"," than"," the"," input"," shaft"," ","102"," ","and"," component"," parts"," of"," the"," reduction"," machine"," including"," a"," bearing"," c","asing"," ","104",".","\u23ce","[","Patent"," Document"," ","1","]"," Japanese","\u2191"," Laid","-","Open"," Patent"," Application"," No"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.93,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["highlighting"," its"," core"," principles",".","\u23ce\u23ce","2","."," \"","How"," do"," you"," define"," A","/","B"," testing"," for"," d","ummies","?\"","\u23ce\u23ce","Answer",":"," A","/","B"," testing"," is"," a"," way"," to"," compare"," two"," versions"," of"," something",","," like"," a"," website"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","lot","har","bot","\u23ce",">"," _","\"","\u2191"," Isn","'t"," it"," enough"," that"," down","v","oted"," comments"," become"," gray","?\"","_","\u23ce\u23ce","\u2191","N","ope","."," That"," doesn","'t"," show"," you"," the"," difference"," between"," a"," barely","-","pass","able"," comment"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.79,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.79,0.0,0.0,0.0,0.02,0.0,0.0,0.78,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.78,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["_","company","_","name","\":\"","\",","\u23ce","\"","from","_","person","_","name","\":\"","\",","\u23ce","\"","from","_","address","\":\"","\",","\u23ce","\"","to","_","company","_","name","\":\"","\",","\u23ce","\"","to","_","person","_","name","\":\"","\",","\u23ce"]},{"tokens_acts_list":[0.0,0.9,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.79,0.0,0.0,0.0,0.0,0.27,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\":"," \"","What"," happens"," to"," our"," bodies"," when"," we"," don","'t"," get"," enough"," sleep"," over"," a"," long"," period"," of"," time","?\"","\u23ce\u23ce","\"","\u2191","Tu","tor","\":"," \""," Not"," getting"," enough"," sleep"," over"," a"," long"," period"," of"," time"," can"," have"," many"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[":","\u23ce","```","python","\u23ce","#","\u2191"," Multiply"," two"," decimal"," objects","\u23ce"," d","1"," ","="," decimal",".","\u2191","Decimal","('","2",".","71","828","')","\u23ce","d","2"," ","="," decimal",".","\u2191","Decimal","('","3",".","14","159","')"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["'t"," really"," understand"," how"," much"," of"," each"," ingredients"," I"," should"," use",";"," do"," you"," know"," what"," the"," measurements"," should"," be","?\"","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," basic"," vanilla"," cake"," recipe"," with"," measurements",":","\u23ce\u23ce","\u2191","Ingredients",":","\u23ce","-"," "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.82,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["you"," rel","ax"," and"," calm"," down"," when"," you","'re"," feeling"," anx","ious",":","\u23ce\u23ce","\"","What"," is"," causing"," my"," anxiety","?\"","  ","Sometimes"," this"," will"," help"," you"," to"," sort"," out"," the"," cause"," of"," your"," anxiety",","," so"," you"," can"," focus"," on"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.94,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.9,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.91,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["1"," ","and"," wife",":","\u23ce","NAME","_","1",":"," \"","\u2191","Honey",","," can"," we"," talk"," for"," a"," moment","?\"","\u23ce","Wife",":"," \"","Of"," course",","," what","'s"," on"," your"," mind","?\"","\u23ce","NAME","_","1",":"," \"","I"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ous","Item","(","item","Type",":"," S","u","gg","estion","Item","Type","):"," Unit"," ="," js",".","native","\u23ce","  ","/*"," protected"," */"," def"," select","P","revi","ous","Item","(","item","Type",":"," S","u","gg","estion","Item","Type",","," original"]},{"tokens_acts_list":[0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["the"," semiconductor"," and"," isol","ating"," regions",".","\u23ce","In"," an"," alternative"," embod","iment",","," the"," device"," may"," further"," include"," an"," isol","ating"," ins","ulating"," film"," that"," surro","unds"," the"," semiconductor"," region","."," And"," the"," semiconductor"," region"," may"," be"," electric","ally"," isolated"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.47,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["  ","S","Be","hn","if","jt","f","ein","  ","un","b","  ","33","e","fi","|","j","."," ","\u23ce\u23ce","\u00bb","*","i","U","l","$","e","  ","\u00ab","unb","f","^","ou",".","   ","XX","I",".","  ","4","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["above"," disclosed"," compounds",","," for"," example"," the"," present"," R","5"," ","in"," the"," formula"," (","I",")"," disclosed"," her","ein","be","low","."," Additionally",","," the"," compounds"," of"," the"," present"," invention"," have"," lower"," CB","1"," ","activity"," than"," the"," compounds"," disclosed"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["<!","DOCTYPE"," html",">","\u23ce","<","html",">","\u23ce","  ","<","head",">","\u23ce","    ","<","style",">","\u23ce","      ","/*","\u2191"," Est","ilo"," para"," dar"," formato"," a"," la"," gal","er\u00eda"," */","\u23ce","      ",".","gallery"," {","\u23ce","        ","display",":"]}]}],"top_logits":["\u21ea","A","N","D","T","E","R","P","O"],"bottom_logits":["\\xf6","\\xfe","\u21b9","\\xfa","\u001d","\u0010","\\xc1","\u0011","\f","\\xf8"]}