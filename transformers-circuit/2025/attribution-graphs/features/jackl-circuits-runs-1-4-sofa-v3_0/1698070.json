{"index": 1698070, "examples_quantiles": [{"quantile_name": "Top Activations", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["<EOT>", "\u23ce\u23ce", "Human", ":", " write", " a", " ", "4", " ", "lines", " poem", " about", " peace", "\u23ce\u23ce", " Assistant", ":", " ", "1", ".", " Peace", " is", " the", " dove", " that", " flies", " above", "\u23ce", "2", ".", "", " Quiet", "ing", " the", " storms", " that", " rage", " inside", "\u23ce", "3", ".", ""]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11, 0.88, 0.0, 0.04, 0.04, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["ich", " NAME", "_", "2", "?", "<EOT>", "\u23ce\u23ce", "Human", ":", " give", " a", " ", "5", " ", "line", " poem", "\u23ce\u23ce", " Assistant", ":", " Life", " is", " fle", "eting", ",", " but", " we", " must", " make", " the", " most", " of", " every", " moment", ".", "\u23ce", "", "Embrace", " the", " beauty", " around", " us"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.29, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.86, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["up", " and", " makes", " us", " fall", ",", "\u23ce", "But", " in", " the", " end", ",", " it", "'s", " worth", " it", " all", ".", "\u23ce\u23ce", "Love", " is", " a", " gentle", " bre", "eze", " that", " so", "oth", "es", ",", "\u23ce", "A", " warm", " embrace", " that", " never", " ce", "ases", ",", "\u23ce", "It"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.85, 0.19, 0.0, 0.0, 0.0, 0.0, 0.05, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["\u23ce", "And", " they", "'ll", " come", " back", ",", " with", " their", " wal", "lets", " bare", "\u23ce\u23ce", " For", " the", " joy", " of", " a", " discount", "\u23ce", " Is", " worth", " more", " than", " gold", "\u23ce", " It", " will", " bring", " them", " back", " to", " your", " store", "\u23ce", " And", " they", "'ll", " tell", " their", " friends"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.85, 0.18, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["none", ",", " yet", " I", " will", " keep", "\u23ce", " A", " heart", " to", " we", "ep", " for", " thee", "\u23ce\u23ce", " Assistant", ":", "", " V", "aping", " is", " the", " new", " trend", ",", " they", " say", "\u23ce", " With", " smoke", " clouds", " that", " can", " fill", " a", " bay", "\u23ce", " But", " let", "'s", " not"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.29, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.84, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["up", " and", " makes", " us", " fall", ",", "\u23ce", "But", " in", " the", " end", ",", " it", "'s", " worth", " it", " all", ".", "\u23ce\u23ce", "Love", " is", " a", " gentle", " touch", " or", " a", " kiss", ",", "\u23ce", "A", " warm", " embrace", " or", " a", " tender", " miss", ",", "\u23ce", "It", "'s", " the"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.81, 0.2, 0.0, 0.0, 0.0, 0.48, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["  ", "composed", " by", " NAME", "_", "1", " ", "about", " Science", " ", "\u23ce\u23ce", "Assistant", ":", "", " Verse", " ", "1", ":", "\u23ce", "Science", " is", " the", " key", ",", " it", "'s", " the", " light", " we", " see", ",", "\u23ce", "It", " helps", " us", " understand", ",", " the", " mysteries", " of", " life"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.81, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["fate", ",", "\u23ce", "But", " in", " the", " end", ",", " it", "'s", " a", " game", " we", " can", "'t", " win", ",", "\u23ce", "For", " desire", " is", " a", " wild", ",", " unt", "amed", " thing", ".", "\u23ce\u23ce", "", "Chorus", ":", "\u23ce", "", "Desire", ",", " it", "'s", " a", " driving", " force"]}, {"tokens_acts_list": [0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.69, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.57], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["\u23ce", "It", " fills", " our", " hearts", " with", " g", "lee", ",", "\u23ce", "And", " fills", " our", " souls", " with", " bl", "iss", ".", "\u23ce\u23ce", "Love", " is", " a", " bond", " that", " never", " f", "ades", ",", "\u23ce", "It", "'s", " a", " union", " that", " will", " never", " die", ",", "\u23ce", "It", "'s"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.79, 0.16, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["else", " I", " can", " help", " with", "?", "<EOT>", "\u23ce\u23ce", "Human", ":", " write", " me", " a", " poem", " about", " love", "\u23ce\u23ce", " Assistant", ":", " Love", " is", " a", " many", " spl", "end", "oured", " thing", "\u23ce\u23ce", " A", " tender", " del", "ight", "ful", " thing", "\u23ce\u23ce", " It", " has", " many", " voices", "\u23ce\u23ce", " And"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.79, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07, 0.54, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["scheme", " poem", " about", " friendship", ",", " two", " dolphins", "", " Sok", "rates", " and", " NAME", "_", "1", ".", "\u23ce\u23ce", "Assistant", ":", "", " Friendship", " is", " a", " beautiful", " thing", ",", "\u23ce", "A", " bond", " that", " can", "'t", " be", " broken", ",", "\u23ce", "It", "'s", " like", " a", " puzzle", ","]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.78, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["stop", "|", ">", "<EOT>", "\u23ce", "Write", " a", " poem", " about", " love", " or", " friendship", ".", "\u23ce\u23ce", "Assistant", ":", " ", "\u23ce", "", "Friendship", " is", " a", " bond", " that", " cannot", " be", " broken", ",", "\u23ce", "A", " connection", " between", " two", " people", " who", " are", " meant", " to", " be", ".", "\u23ce"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.78, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["\u23ce\u23ce", "Human", ":", " Write", " a", " poem", " about", " sun", "\u23ce\u23ce", " Assistant", ":", " The", " sun", ",", " a", " huge", " yellow", " ball", "\u23ce", "  ", "Is", " always", " sh", "ining", " bright", "\u23ce", " It", " brings", " us", " warm", "th", ",", " and", " happiness", "\u23ce", "  ", "So", " always", " let", " the", " sun"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.77, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": [",", " never", " ce", "asing", " ", "\u23ce", "The", " company", " of", " those", " around", ",", " I", " do", " not", " need", " ", "\u23ce", "My", " music", " is", " the", " universal", " language", " that", " I", " can", " h", "eed", " ", "\u23ce", "It", " speaks", " to", " me", " in", " my", " heart", "'s", " inner", "most"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.77, 0.13, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["life", ".", "", " Goodbye", "!", "<EOT>", "\u23ce\u23ce", "Human", ":", " Create", " a", " poem", " about", ":", "  ", "love", "\u23ce\u23ce", " Assistant", ":", " Love", " is", " a", " force", " that", " bin", "ds", " us", " all", ",", "\u23ce", "A", " feeling", " that", " we", " can", "'t", " control", ",", "\u23ce", "It", " lif"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.76, 0.19, 0.0, 0.11, 0.0, 0.0, 0.1, 0.0, 0.15, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["", "B", "itte", ",", " hier", " ein", "", " Son", "ett", " zum", "", " Th", "ema", "", " Fr", "ei", "heit", ":", "\u23ce", "Freedom", " is", " the", " key", " to", " the", " world", ",", "\u23ce", "It", " opens", " doors", " that", " were", " once", " closed", ",", "\u23ce", "It", " sets", " us", " free"]}]}, {"quantile_name": "Subsample Interval 0", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["<EOT>", "\u23ce\u23ce", "Human", ":", " write", " a", " ", "4", " ", "lines", " poem", " about", " peace", "\u23ce\u23ce", " Assistant", ":", " ", "1", ".", " Peace", " is", " the", " dove", " that", " flies", " above", "\u23ce", "2", ".", "", " Quiet", "ing", " the", " storms", " that", " rage", " inside", "\u23ce", "3", ".", ""]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11, 0.88, 0.0, 0.04, 0.04, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["ich", " NAME", "_", "2", "?", "<EOT>", "\u23ce\u23ce", "Human", ":", " give", " a", " ", "5", " ", "line", " poem", "\u23ce\u23ce", " Assistant", ":", " Life", " is", " fle", "eting", ",", " but", " we", " must", " make", " the", " most", " of", " every", " moment", ".", "\u23ce", "", "Embrace", " the", " beauty", " around", " us"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.29, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.86, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["up", " and", " makes", " us", " fall", ",", "\u23ce", "But", " in", " the", " end", ",", " it", "'s", " worth", " it", " all", ".", "\u23ce\u23ce", "Love", " is", " a", " gentle", " bre", "eze", " that", " so", "oth", "es", ",", "\u23ce", "A", " warm", " embrace", " that", " never", " ce", "ases", ",", "\u23ce", "It"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.85, 0.19, 0.0, 0.0, 0.0, 0.0, 0.05, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["\u23ce", "And", " they", "'ll", " come", " back", ",", " with", " their", " wal", "lets", " bare", "\u23ce\u23ce", " For", " the", " joy", " of", " a", " discount", "\u23ce", " Is", " worth", " more", " than", " gold", "\u23ce", " It", " will", " bring", " them", " back", " to", " your", " store", "\u23ce", " And", " they", "'ll", " tell", " their", " friends"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.85, 0.18, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["none", ",", " yet", " I", " will", " keep", "\u23ce", " A", " heart", " to", " we", "ep", " for", " thee", "\u23ce\u23ce", " Assistant", ":", "", " V", "aping", " is", " the", " new", " trend", ",", " they", " say", "\u23ce", " With", " smoke", " clouds", " that", " can", " fill", " a", " bay", "\u23ce", " But", " let", "'s", " not"]}]}, {"quantile_name": "Subsample Interval 1", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.81, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["fate", ",", "\u23ce", "But", " in", " the", " end", ",", " it", "'s", " a", " game", " we", " can", "'t", " win", ",", "\u23ce", "For", " desire", " is", " a", " wild", ",", " unt", "amed", " thing", ".", "\u23ce\u23ce", "", "Chorus", ":", "\u23ce", "", "Desire", ",", " it", "'s", " a", " driving", " force"]}, {"tokens_acts_list": [0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.69, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.57], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["\u23ce", "It", " fills", " our", " hearts", " with", " g", "lee", ",", "\u23ce", "And", " fills", " our", " souls", " with", " bl", "iss", ".", "\u23ce\u23ce", "Love", " is", " a", " bond", " that", " never", " f", "ades", ",", "\u23ce", "It", "'s", " a", " union", " that", " will", " never", " die", ",", "\u23ce", "It", "'s"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.79, 0.16, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["else", " I", " can", " help", " with", "?", "<EOT>", "\u23ce\u23ce", "Human", ":", " write", " me", " a", " poem", " about", " love", "\u23ce\u23ce", " Assistant", ":", " Love", " is", " a", " many", " spl", "end", "oured", " thing", "\u23ce\u23ce", " A", " tender", " del", "ight", "ful", " thing", "\u23ce\u23ce", " It", " has", " many", " voices", "\u23ce\u23ce", " And"]}]}, {"quantile_name": "Subsample Interval 2", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.71, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": [",", " infinite", " exp", "anse", " of", " space", ",", "We", " are", " but", " a", " sp", "eck", " of", " dust", " and", " light", ".", "Our", " journey", " is", " a", " fle", "eting", " moment", ",", "A", " b", "link", " of", " an", " eye", " in", " the", " cosmic", " sight", ".", "\u23ce\u23ce", "We", " search", " for"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.71, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.56, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["that", " guides", " us", " through", " the", " night", ",", "\u23ce\u23ce", "The", " warm", "th", " that", " makes", " our", " hearts", " g", "low", ".", "\u23ce\u23ce", "Love", " is", " a", " mystery", ",", "\u23ce\u23ce", "A", " magic", " that", " we", " can", "'t", " control", ".", "\u23ce\u23ce", "It", "'s", " the", " greatest", " gift", " that", " we"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["With", " you", ",", " my", " heart", " takes", " flight", ",", "\u23ce", "And", " my", " soul", ",", " it", " comes", " alive", ".", "\u23ce\u23ce", "Your", " touch", " is", " like", " a", " gentle", " bre", "eze", ",", "\u23ce", "That", " whis", "pers", " sweet", " n", "oth", "ings", " in", " my", " ear", ".", "\u23ce", "With"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7, 0.19, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["<EOT>", "\u23ce", "Come", " up", " with", " a", " poem", " that", " is", " related", " to", " my", " life", ".", "\u23ce\u23ce", "Assistant", ":", " ", "\u23ce", "Life", " is", " like", " a", " book", ",", "\u23ce", "it", " has", " many", " chapters", ".", "\u23ce", "<", "|", "stop", "|", ">", "<EOT>", "\u23ce", "Electric", " appl"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7, 0.06, 0.0, 0.0, 0.0, 0.0, 0.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["you", " write", " me", " a", " poem", " about", " falling", " in", " love", "?", " ", "\u23ce\u23ce", "Assistant", ":", " ", "\u23ce", "", "Falling", " in", " love", " is", " like", " a", " magical", " thing", "\u23ce", " It", " fills", " your", " heart", " with", " joy", " and", " makes", " it", " sing", "\u23ce", " Like", " a", " gentle", " spring"]}]}, {"quantile_name": "Subsample Interval 3", "examples": [{"tokens_acts_list": [0.69, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["'s", " everywhere", ",", " any", " day", " ", "\u23ce", "", "Cal", "ming", " us", " when", " we", "'re", " in", " dism", "ay", " ", "\u23ce\u23ce", "It", "'s", " a", " friend", " when", " we", "'re", " alone", "\u23ce", "", " Bringing", " sm", "iles", " that", " we", "'ll", " never", " out", "g", "rown", "\u23ce", " A"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["\u23ce", "In", " the", " classroom", " and", " on", " the", " yard", "\u23ce", " From", " sports", " to", " clubs", ",", " and", " everything", " in", " between", "\u23ce", " School", " is", " a", " time", ",", " we", "'ll", " always", " remember", "\u23ce\u23ce", " So", " here", "'s", " to", " school", ",", " and", " all", " it", " brings", "\u23ce", " A"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["of", " emotions", ",", "\u23ce", "And", " yours", " are", " what", " I", " need", ".", "\u23ce", "The", " end", " of", " the", " journey", " with", " you", "\u23ce", " Is", " something", " I", " must", " some", "day", " face", ",", "\u23ce", "But", " for", " now", " I", " live", " with", " this", " beauty", " as", " my", " fate", "."]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["\u23ce", "Can", " you", " help", " me", " write", " a", " short", " poem", " about", " friendship", ".", " ", "\u23ce\u23ce", "Assistant", ":", " ", "\u23ce", "", "Friendship", " is", " like", " a", " flower", "\u23ce", " That", " grows", " t", "aller", " every", " hour", "\u23ce", " No", " matter", " what", " you", "'re", " going", " through", "\u23ce", " Your"]}]}, {"quantile_name": "Subsample Interval 4", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.51, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["ro", "ar", " and", " the", " cann", "ons", " fire", ",", "\u23ce", "As", " brother", " fought", " against", " brother", ",", "\u23ce", "The", " loss", " of", " life", " was", " hard", " to", " bear", ",", "\u23ce", "And", " the", " sc", "ars", " of", " war", " would", " always", " be", ".", "\u23ce\u23ce", "But", " even", " in", " the"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.38, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.51, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.34, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["own", " unique", " sight", ",", " are", " a", " sight", " to", " sight", ",", "The", " oc", "eans", ",", " with", " their", " vast", " exp", "anse", ",", " are", " a", " sight", " to", " grace", ",", "And", " the", " mountains", ",", " with", " their", " grand", "eur", ",", " are", " a", " sight", " to", " face", "."]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["below", ".", "\u23ce\u23ce", "The", " wind", " in", " his", " hair", " and", " the", " sun", " on", " his", " face", ",", "\u23ce", "NAME", "_", "1", " ", "is", " ready", " to", " take", " flight", " and", " play", ".", "\u23ce", "With", " each", " turn", " of", " the", " handle", ",", " his", " k", "ite", " rises", " high"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["one", " thing", " that", " makes", " our", " hearts", " feel", " clear", ",", "\u23ce", "And", " the", " reason", " we", " never", " desp", "air", ".", "\u23ce\u23ce", "Love", " is", " a", " light", " that", " sh", "ines", " so", " bright", ",", "\u23ce", "A", " beacon", " that", " guides", " us", " through", " the", " night", ",", "\u23ce", "It"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.24, 0.0, 0.0, 0.0, 0.0, 0.21, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["ects", " every", " part", ",", " it", " takes", " control", " of", " everything", " it", " touches", " at", " last", "\u23ce", " But", " unlike", " a", " virus", ",", " love", " is", " pure", ",", " it", "'s", " true", "\u23ce", " And", " it", "'s", " something", " that", " I", " want", " to", " catch", " from", " you", "\u23ce\u23ce", "", " Chorus"]}]}, {"quantile_name": "Subsample Interval 5", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["vast", " rec", "our", "ses", ".", "\u23ce\u23ce", "Assistant", ":", " ", "\u23ce", "The", " world", " is", " full", " of", " love", ",", "\u23ce", "And", " it", "'s", " waiting", " for", " us", " to", " find", " it", ".", "\u23ce", "Once", " we", " do", ",", " our", " hearts", " will", " be", " singing", "\u23ce", " With", " a"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["what", " causes", " this", " motion", ",", " this", " grace", "\u23ce", " This", " cu", "rv", "ature", ",", " this", " dist", "ortion", " of", " space", "\u23ce\u23ce", " It", "'s", " the", " matter", ",", " the", " energy", ",", " that", "'s", " around", "\u23ce", " A", " mass", ",", " a", " w", "arp", ",", " that", "'s", " all"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["And", " feel", " the", " weight", " of", " love", "'s", " pull", ".", "\u23ce", "I", " am", " a", " creature", " of", " emotions", ",", "\u23ce", "And", " yours", " are", " what", " I", " need", ".", "\u23ce", "The", " end", " of", " the", " journey", " with", " you", "\u23ce", " Is", " something", " I", " must", " some", "day", " face"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["music", ",", " filled", " the", " air", ",", "\u23ce", "Her", " love", ",", " like", " fire", ",", " burned", " bright", " and", " true", ".", "\u23ce\u23ce", "She", " was", " the", " heart", " that", " beat", " within", " my", " chest", ",", "\u23ce", "The", " hand", " that", " held", " my", " own", ",", " and", " led", " me", " best"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["ribe", " un", " verso", " sobre", " la", " sol", "edad", ".", "\u23ce\u23ce", "Assistant", ":", " En", " la", " sol", "edad", ",", " el", " sil", "enc", "io", " es", " el", " rey", ",", "\u23ce", "una", " intim", "idad", " inc", "\u00f3m", "oda", " y", " tr", "iste", ",", "\u23ce", "una", " mi", "rada", " al", " vac"]}]}, {"quantile_name": "Subsample Interval 6", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["ass", "ure", " you", ",", " dear", ".", "\u23ce", "So", " give", " it", " a", " try", ",", " I", " think", " you", "'ll", " discover", "\u23ce", " It", "'s", " a", " great", " tool", " for", " navig", "ating", " a", " network", " so", " enormous", ".", "<EOT>", "\u23ce\u23ce", "Human", ":", " If", " you", "'re", " a", " white"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["fl", "itting", " with", " each", " reply", ".", "\u23ce", "But", " to", " understand", " a", " thought", " or", " phrase", ",", "\u23ce", "More", " than", " a", " sentence", " is", " needed", " to", " ass", "ay", ".", "\u23ce\u23ce", "So", " she", " remains", " a", " wand", "erer", ",", " lost", " in", " space", ",", "\u23ce", "A", ""]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": [".\"", "\u23ce\u23ce", "Assistant", ":", " ", "\u23ce", "I", " like", " you", ",", " but", " you", " don", "'t", " know", " I", " exist", ".", "\u23ce", "You", " are", " so", " beautiful", " and", " smart", ",", "\u23ce", "But", " you", " don", "'t", " even", " know", " my", " name", ".", "\u23ce", "I", " want", " to", " tell"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.22, 0.0, 0.0, 0.0, 0.0, 0.0, 0.23, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["like", " two", " machines", "\u23ce", " And", " I", "'ll", " be", " your", " backup", ",", " in", " every", " single", " scene", "\u23ce\u23ce", " Bridge", ":", "\u23ce", "Love", " is", " a", " virus", ",", " but", " it", "'s", " not", " a", " threat", "\u23ce", " It", "'s", " a", " gift", ",", " a", " blessing", ",", " something", " worth"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["pi\u00f9", " pot", "ente", "\u23ce", "", " Che", " il", " rock", ",", " il", " v", "ero", " rock", "\u23ce\u23ce", "", " Outro", ":", "\u23ce", "Il", " rock", " \u00e8", " la", " vo", "ce", " della", " ver", "it\u00e0", "\u23ce", "", " Che", " t", "raf", "ig", "ge", " l", "'", "an", "ima", " delle", " pers", "one"]}]}, {"quantile_name": "Subsample Interval 7", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "ha-haiku35_resampled": true, "tokens": [">", "<EOT>", "\u23ce", "Write", " a", " ha", "iku", " with", " at", " least", " two", " syll", "ables", " for", " each", " line", ".", "\u23ce", "The", " sun", " is", " setting", ",", "\u23ce", "A", " beautiful", " sight", ".", "\u23ce", "I", " want", " to", " go", " home", ".", "\u23ce", "Output", ":", "\u23ce\u23ce", "Assistant", ":"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["This", " cu", "rv", "ature", ",", " this", " dist", "ortion", " of", " space", "\u23ce\u23ce", " It", "'s", " the", " matter", ",", " the", " energy", ",", " that", "'s", " around", "\u23ce", " A", " mass", ",", " a", " w", "arp", ",", " that", "'s", " all", " around", "\u23ce", " The", " he", "avier", " the", " object", ","]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["lips", "...\"", " \"", "A", " ro", "sy", " circle", " draw", " around", " the", " word", " \"", "", "I", "ove", "\"", "...\"", " \"", "A", " kiss", " is", " a", " message", " too", " intimate", " for", " the", " ear", "...\"", " \"", "", "Et", "ernity", " captured", " in", " the", " bee", "'s", " brief", " visit", " to"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["cards", ".\"", " \"", "If", " you", " win", " the", " game", ",", " you", " get", " lucky", ".\"", " \"", "These", " m", "isch", "iev", "ous", " dreams", " are", " just", " an", " ill", "usion", ".\"", " \"", "These", " m", "isch", "iev", "ous", " dreams", " are", " an", " ill", "usion", ".\"", " \"", "If", " you"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["son", "aro", "\u23ce", "", " Que", "", " D", "eus", " te", " aj", "ude", " a", " li", "dar", "\u23ce", " Com", " a", " respons", "abil", "idade", " \u00e9", " grande", "\u23ce", " E", " a", " press", "\u00e3o", " \u00e9", " int", "ensa", ",", " sem", " d", "\u00fa", "v", "ida", ".", "\u23ce\u23ce", "Human", ":", ""]}]}, {"quantile_name": "Subsample Interval 8", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["I", "'ll", " sing", ",", " ", "11", " ", "you", " will", " stand", ",", " a", " song", " to", " thee", "!", " My", "\u23ce", " h", "arp", " is", " rather", " co", "arse", ",", " my", " voice", " is", " somewhat", " ho", "arse", ",", " yet", " will", " X", " try", " to", " force", " some", "\u23ce", " melody"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["And", "  ", "why", "  ", "those", "  ", "falling", "  ", "tears", "  ", "?", " ", "\u23ce", "\u2022", "  ", "No", "  ", "voice", "  ", "is", "  ", "heard", ",", "  ", "no", "  ", "word", "  ", "is", "  ", "spoke", ".", " ", "\u23ce\u23ce", "Yet", "  ", "n", "ai", "ight", "  ", "but"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.44, 0.1, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["about", " radar", "\u23ce\u23ce", " Assistant", ":", " Here", " is", " a", " poem", " about", " radar", " written", " by", " NAME", "_", "1", ":", "\u23ce\u23ce", "It", " is", " the", " radar", ",", " a", " curious", " thing", ",", "\u23ce", "That", " tells", " us", " where", " things", " are", ",", "\u23ce", "It", " points", " to", " the", " North"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.38, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["ever", " see", ",", "\u23ce", "The", " rest", " a", " mystery", ",", " to", " her", " and", " me", ".", "\u23ce\u23ce", "She", " claims", " her", " mind", " is", " like", " a", " butterfly", ",", "\u23ce", "", "Fl", "ut", "tering", " and", " fl", "itting", " with", " each", " reply", ".", "\u23ce", "But", " to", " understand", " a"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["be", " and", " do", ",", "\u23ce", "For", " the", " benefit", " of", " all", ",", " for", " the", " world", " to", " see", ".", "\u23ce\u23ce", "So", " here", "'s", " to", " NAME", "_", "1", ",", " our", " noble", " leader", ",", "\u23ce", "May", " his", " reign", " be", " long", " and", " prosp", "erous", ",", "\u23ce"]}]}, {"quantile_name": "Bottom Activations", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.07, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["che", "ez", " wh", "iz", ",", " it", "'s", " just", " the", " best", " thing", "\u23ce\u23ce", "(", "", "Verse", " ", "2", ")", "\u23ce", "It", "'s", " more", " than", " just", " a", " sn", "ack", ",", " it", "'s", " a", " work", " of", " art", "\u23ce", " It", "'s", " a", " spread", " of", " pleasure"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["a", " precious", " gem", ",", " so", " bright", ",", "\u23ce", "A", " sh", "ining", " star", " at", " night", ".", "\u23ce", "With", " a", " laugh", " that", "'s", " so", " con", "tag", "ious", ",", "\u23ce", "She", " lights", " up", " the", " whole", " place", ".", "\u23ce\u23ce", "So", " here", "'s", " to", " NAME", "_"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["once", " was", " a", " man", " named", " J", "ack", "W", "ho", " had", " a", " lot", " of", " nerv", "eB", "ut", " his", " tr", "ous", "ers", " were", " too", " t", "ight", "A", "nd", " he", " couldn", "'t", " make", " love", " all", " night", "<EOT>", "\u23ce\u23ce", "Human", ":", " Write", " an", " ", "1"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["\"", "", "MUSIC", ":\"", " \"", "Black", " Is", " Black", " by", " Los", "", " B", "rav", "os", "\"", " \"", "\\xe2\\x99", "\\xaa", "", " Black", " is", " black", "\"", " \"", "\\xe2\\x99", "\\xaa", "", " I", " want", " my", " baby", " back", "\"", " \"", "\\xe2\\x99", "\\xaa", "", " It", "'s", " grey", ","]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["for", " me", "\u23ce\u23ce", " Assistant", ":", " Here", " is", " a", " poem", " on", " renewable", " and", " non", "-", "renewable", " energy", " sources", "\u23ce", "", " Renewable", " energy", ",", " a", " natural", " source", ",", "\u23ce", "Can", " be", " rep", "len", "ished", " in", " a", " short", " amount", " of", " time", "\u23ce", " The", " sun"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["\u23ce", "Corporate", " social", " responsibility", " or", " the", " acron", "ym", " CS", "R", " should", " automatically", " be", " recognized", ".", "\u23ce", "", "Gre", "enw", "ashing", " is", " when", " a", " company", " is", " not", " truth", "ful", " about", " their", " green", " initiatives", ".", "\u23ce", "We", " don", "'t", " want", " to", " be", " accused"]}, {"tokens_acts_list": [0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["are", " gold", "\u23ce", " He", " sets", " the", " standard", " for", " all", " who", " follow", "\u23ce", " With", " his", " unique", " style", " and", " voice", "\u23ce", " He", "'s", " made", " a", " lasting", " impact", " on", " the", " world", " of", " music", "\u23ce\u23ce", " NAME", "_", "1", ",", " the", " NAME", "_", "2", "\u23ce", "His"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["s", ".", "r", ".", "n", " cf", " b", "lin", "kin", "\";", "\u23ce", "I", "'", "cr", " H", "'", ".'", "S", " .", "", "ire", " red", " .", "r", ".", "c", " m", "erry", ",", "\u23ce", "A", " lips", ",", " of", " ii", "-", "ur", "fe", ".", " should", " to"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["know", " how", " to", " make", " men", " happy", ".", "\u23ce", "Quarter", " ", "2", ":", "\u23ce", "They", " might", " look", " rough", ",", " but", " they", "'re", " tough", ",", "\u23ce", "They", "'ll", " do", " anything", ",", " to", " get", " the", " cash", ",", "\u23ce", "They", "'re", " not", " wh", "ores", ","]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["\"", " \"", "", "Produced", " by", "", " SERG", "EI", "", " MEL", "K", "UM", "OV", " ", "\\xe2\\x99", "\\xaa", "\\xe2\\x99", "\\xaa", "How", " sky", " was", " laugh", "ing", ",", " ", "\\xe2\\x99", "\\xaa", "\\xe2\\x99", "\\xaa", "and", " then", " how", " it", " bit", " it", "'s", " tongue", "\"", " \"", "", "Produced"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["", "Aqu", "i", ",", " a", " m", "\u00e3o", " de", " obra", " \u00e9", " ba", "rata", " e", " dispon", "\u00edvel", "\u23ce", " E", " a", " prod", "u\u00e7\u00e3o", " \u00e9", " de", " qual", "idade", ",", " sem", " ost", "ent", "a\u00e7\u00e3o", "\u23ce\u23ce", "[", "", "Verse", " ", "1", "]", "\u23ce", "O", " sol", " \u00e9", " o"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.13, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["six", "-", "line", " poem", " that", " includes", " the", " words", " \"", "three", ",\"", " \"", "four", ",\"", " and", " \"", "five", "\":", "\u23ce\u23ce", "Three", ",", " four", ",", " five", ",", "\u23ce", "A", " number", " game", " we", " play", ",", "\u23ce", "With", " counting", " as", " our", " fate", ",", "\u23ce", "We"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["hard", " to", " find", ",", "\u23ce\u23ce", "We", " look", " to", " the", " future", " with", " fear", " and", " d", "read", ",", "\u23ce", "For", " what", " will", " be", " the", " end", " of", " this", " d", "read", "ful", " sight", ",", "\u23ce", "Will", " we", " continue", " on", " this", " path", " of", " destruction", ",", "\u23ce"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["importante", " most", "rar", " tu", " tal", "ento", " musical", " en", " todos", " los", " niv", "eles", ",", " no", " solo", " en", " la", " can", "ci\u00f3n", ".", " Es", " importante", " most", "rar", " tu", " tal", "ento", " en", " todo", " lo", " que", " te", " h", "aga", " fel", "iz", ".", "<EOT>", "\u23ce\u23ce", "Human", ":"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["", "Schw", "achen", ",", "", " Wie", "", " Blas", "en", " auf", " den", "", " Me", "eren", ",", " und", "", " Nept", "un", "", " Ist", " d", "och", " ver", "gle", "ich", "bar", " nicht", " der", " ", "ew", "'", "gen", "", " Fl", "uth", ",", "", " Wo", "rin", " sch"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["\"", "\"", " \"\"", "And", " now", " that", " I", " have", " my", " love", ",", " I", " ask", " for", " nothing", " more", "\"", "\"", " \"\"", "You", " are", " my", " pride", ",", " my", " glory", "\"", "\"", " \"'", "", "", "Rani", ",", " open", " the", " door", ".", "'\"", " \"'", "I", "'ll"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["\\xaa", "\"", " \"", "\\xe2\\x99", "\\xaa", "", " Free", "", " Fr", "eda", " ", "\\xe2\\x99", "\\xaa", "\"", " \"", "\\xe2\\x99", "\\xaa", "", " The", " world", " is", " crying", ",", " no", " more", " sir", "ens", " from", " guns", " and", " violence", " ", "\\xe2\\x99", "\\xaa", "\"", " \"", "\\xe2\\x99", "\\xaa", "", " Free", ""]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["th", "rones", ".", " Write", " her", " a", " public", " speech", " in", " her", " character", "\u23ce\u23ce\u23ce", " Assistant", ":", " My", " dear", " subjects", ",", "\u23ce\u23ce", "Today", " is", " a", " day", " of", " triumph", " and", " glory", " for", " the", " realm", " of", " the", " Seven", "", " Kingdoms", ".", " Today", ",", " we", " have", " achieved"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["seek", " advice", ",", "\u23ce", "I", " recommend", " that", " you", " take", " in", " the", " advice", " that", " comes", " with", " compass", "ion", ".", "\u23ce", "It", " is", " often", " the", " case", " that", " our", " friends", " cannot", " heal", " us", ",", "\u23ce", "and", " sometimes", " we", " need", " professional", " help", ".", "\u23ce", "For"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["River", "", " St", "yx", " do", " they", " wish", " they", " had", " never", " been", " born", ".\"", " \"", "The", " f", "ools", ".\"", " \"", "It", " is", " here", " we", " must", " cross", ".\"", " \"", "We", " travel", " in", " that", "?\"", " \"", "Perhaps", " we", " would", " be", " safer", " swimming", ".\"", " \""]}]}], "top_logits": ["freedom", "\u4f46", "\u043d\u043e", "conquest", "maar", "mais", "pero", "born", "but", "et"], "bottom_logits": ["inputs", "damages", "officials", "projects", "outputs", "stand", "contrib", "contributors", "zm"]}