{"index":15290403,"examples_quantiles":[{"quantile_name":"Top Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.25,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.98,0.42,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["a","\u2191"," Shakesp","ear","ean"," son","net"," includes"," three"," quat","r","ains"," (","four"," lines"," each",")"," and"," a"," c","oup","let"," (","two"," lines",")."," It"," emphas","izes"," structure",","," theme",","," and"," rh","yme"," to"," conv","ey"," its"," message"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.19,0.0,0.0,0.0,0.0,1.0,0.37,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["form",","," but"," why"," is"," a","\u2191"," Shakesp","ear","ean"," son","net"," consists"," of"," three"," quat","rain"," and"," a"," c","oup","let","?","\u23ce\u23ce","Assistant",":"," The"," structure"," of"," the","\u2191"," Shakesp","ear","ean"," son","net"," (","three"," quat","r","ains"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.79,0.37,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[" ","\u23ce","marry"," the"," h","ei","ress"," of"," the"," British"," throne","."," It"," was"," merely"," ","\u23ce","the"," first"," c","oup","let"," of"," the"," Essay"," on"," Man",","," which",","," fortun","ately"," ","\u23ce","having"," an"," all","usion"," to"," the"," \""]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.32,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["or","  ","a"," race","?\""," ","\u23ce\u23ce","Far","  ","less","  ","should","  ","the","  ","sp","len","etic","  ","c","oup","let","  ","be","  ","relied"," ","\u23ce","on",",","  ","which","  ","in","sin","u","ates","  ","that","  ","any"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.67,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[";"," th","ru","sting"," a"," hand"," into"," the"," leg"," of"," each"," h","oof",",","\u23ce","he"," rose"," with"," a"," c","oup","lo"," of"," sir"," shoot","ei"," g",","," the"," dit","id","ly","\u23ce"," revol","vers",","," and"," commenced"," dis","char","ging"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.32,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","\u2191","Cv","ii","gr","ove","'s","  ","fine","  ","but","  ","cru","elly","  ","hack","ne","yed","  ","c","oup","let"," \u2014"," ","\u23ce","\u2191","","Kc","ait","  ",")","i","ath","  ","","ijo","  ","rage","  ","like","  "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["."," He"," was"," found","\u23ce"," sitting"," in"," his"," chair"," at","\u2191"," Vinc","ennes"," dead",",","\u23ce","and"," the"," following"," c","oup","let",","," fresh","ly"," w","rit","\u23ce"," ten",","," lay"," beside"," him",":","\u23ce","\"","Here"," lies"," a"," man"," who"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.57,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["'m"," not",".\""," \"","~"," Am"," I","?\""," \"","~"," Very",".\""," \"","This"," next"," one","'s"," for"," a"," c","oup","la"," friends"," of"," mine",".\""," \"","A"," detective"," and"," an"," explorer",".\""," \"","\\xe2\\x99","\\xaa",""," I"," ain","'t"," got"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.56,0.23,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["blast"," too",","," wasn","'t"," it"," in"," one"," of"," your"," poems","?\""," \"","There"," was"," some"," code"," in"," that"," c","oup","let","...\""," \"","The"," ","\\xef","\\xac","\\x81","re"," in"," my"," ve","ins",","," must"," run"," through"," yours"," too","...\""]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.62,0.27,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.56,0.26,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["drink"," and"," be"," m","erry",","," for"," tomorrow"," we"," die","\u23ce\u23ce"," Human",":"," write"," me"," a"," rh","y","ming"," c","oup","let","\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," rh","y","ming"," c","oup","let",":","\u23ce\u23ce","In"," fields"," of"," gold"," where"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.52,0.23,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["to"," prov","oke"," l","aughter"," and"," a"," Salt"," Lake"," man"," recently"," returned"," from","\u2191"," To","ole"," carefully"," copied"," the"," c","oup","let"," for"," circulation"," among"," his"," friends","."," If"," your"," want"," be"," a"," init","iation",","," make"," it"," known"," by"," the"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.46,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["el","ks",","," of"," which","\u23ce"," order"," Mr","."," Bailey"," is"," a"," prominent"," mem","\u23ce"," ","ber","."," The"," c","oup","je"," left"," on","."," the"," midnight","\u23ce"," train","\u2191"," S","ot","."," Washington",",'"," D","."," C",".","f"," hoping"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.45,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["who"," la"," suppos","od"," to"," have","\u2191"," M","ii","oti","\u2191"," Ain","rs","hal","\u2191"," Ma","ples",","," a"," c","oup","lo"," of"," years"," n","go",","," though"," there"," Is"," hardly"," n"," doubt"," but"," ho"," mist","ook"," him"," for"," High"," Sheriff"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.43,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["us","!\""," \"[","\u2191","Wh","im","pers","]","\""," \"[","\u2191","Br","akes"," squ","eak","]","\""," \"","Two"," c","oup","ons"," it"," is",".\""," \"","\u2191","Prepare"," the"," bird","!\""," \"","Come"," on",".\""," \"","Move"," it","!\""," \"[","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.25,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.98,0.42,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u2191","Shakesp","ear","ean"," son","net"," includes"," three"," quat","r","ains"," (","four"," lines"," each",")"," and"," a"," c","oup","let"," (","two"," lines",")."," It"," emphas","izes"," structure",","," theme",","," and"," rh","yme"," to"," conv","ey"," its"," message","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.23,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["att","ou"," and"," the","\u2191"," C","opi","^","er"," Island"," of"," the","\u2191"," ","Kor","-"," mand","or","ski"," c","oup","let"," or"," group"," in"," the"," North"," Pacific"," to"," the"," merid","ian"," of"," ","167","^"," east"," lon","gi","-"," t"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["nationals"," tot","lay"," The"," gro","undo","\u23ce"," w","er"," soft"," and"," altogether"," bad"," for"," play","\u23ce"," ing"," but"," a"," c","oup","lo"," of"," hundred"," fans"," were","\u23ce"," cut"," and"," It"," was"," decided"," not"," to"," disapp","oint","\u23ce"," them","\u23ce"," This"," Is"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.39,0.12,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\"","I"," wouldn","'t"," consider"," this"," particular"," case"," to"," be"," political"," collaboration",".\""," \"","It"," was"," about"," verses"," and"," c","oup","lets",".\""," \""," Sir","...\""," \""," Obviously",","," I"," understand",".\""," \"","I"," agree"," with"," my"," colleagues",".\""," \"","When"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.39,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["I","\u23ce"," ","tion"," to"," his"," home","\u23ce"," After"," a"," few"," words"," with"," the"," w","arden","\u23ce"," over"," a"," c","oup","lo"," of"," tw","oc","ent"," stamps","\u23ce"," which"," he"," ass","erted"," were"," due"," him"," he"," I","\u23ce"," announced"," his","\u2191"]}]},{"quantile_name":"Subsample Interval 0","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.25,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.98,0.42,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["a","\u2191"," Shakesp","ear","ean"," son","net"," includes"," three"," quat","r","ains"," (","four"," lines"," each",")"," and"," a"," c","oup","let"," (","two"," lines",")."," It"," emphas","izes"," structure",","," theme",","," and"," rh","yme"," to"," conv","ey"," its"," message"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.19,0.0,0.0,0.0,0.0,1.0,0.37,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"ha-haiku35_resampled":true,"tokens":["form",","," but"," why"," is"," a","\u2191"," Shakesp","ear","ean"," son","net"," consists"," of"," three"," quat","rain"," and"," a"," c","oup","let","?","\u23ce\u23ce","Assistant",":"," The"," structure"," of"," the","\u2191"," Shakesp","ear","ean"," son","net"," (","three"," quat","r","ains"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.79,0.37,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":[" ","\u23ce","marry"," the"," h","ei","ress"," of"," the"," British"," throne","."," It"," was"," merely"," ","\u23ce","the"," first"," c","oup","let"," of"," the"," Essay"," on"," Man",","," which",","," fortun","ately"," ","\u23ce","having"," an"," all","usion"," to"," the"," \""]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.32,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["or","  ","a"," race","?\""," ","\u23ce\u23ce","Far","  ","less","  ","should","  ","the","  ","sp","len","etic","  ","c","oup","let","  ","be","  ","relied"," ","\u23ce","on",",","  ","which","  ","in","sin","u","ates","  ","that","  ","any"]}]},{"quantile_name":"Subsample Interval 1","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.25,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.98,0.42,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["a","\u2191"," Shakesp","ear","ean"," son","net"," includes"," three"," quat","r","ains"," (","four"," lines"," each",")"," and"," a"," c","oup","let"," (","two"," lines",")."," It"," emphas","izes"," structure",","," theme",","," and"," rh","yme"," to"," conv","ey"," its"," message"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.19,0.0,0.0,0.0,0.0,1.0,0.37,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"ha-haiku35_resampled":true,"tokens":["form",","," but"," why"," is"," a","\u2191"," Shakesp","ear","ean"," son","net"," consists"," of"," three"," quat","rain"," and"," a"," c","oup","let","?","\u23ce\u23ce","Assistant",":"," The"," structure"," of"," the","\u2191"," Shakesp","ear","ean"," son","net"," (","three"," quat","r","ains"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.79,0.37,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":[" ","\u23ce","marry"," the"," h","ei","ress"," of"," the"," British"," throne","."," It"," was"," merely"," ","\u23ce","the"," first"," c","oup","let"," of"," the"," Essay"," on"," Man",","," which",","," fortun","ately"," ","\u23ce","having"," an"," all","usion"," to"," the"," \""]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.32,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["or","  ","a"," race","?\""," ","\u23ce\u23ce","Far","  ","less","  ","should","  ","the","  ","sp","len","etic","  ","c","oup","let","  ","be","  ","relied"," ","\u23ce","on",",","  ","which","  ","in","sin","u","ates","  ","that","  ","any"]}]},{"quantile_name":"Subsample Interval 2","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.79,0.37,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":[" ","\u23ce","marry"," the"," h","ei","ress"," of"," the"," British"," throne","."," It"," was"," merely"," ","\u23ce","the"," first"," c","oup","let"," of"," the"," Essay"," on"," Man",","," which",","," fortun","ately"," ","\u23ce","having"," an"," all","usion"," to"," the"," \""]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.32,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["or","  ","a"," race","?\""," ","\u23ce\u23ce","Far","  ","less","  ","should","  ","the","  ","sp","len","etic","  ","c","oup","let","  ","be","  ","relied"," ","\u23ce","on",",","  ","which","  ","in","sin","u","ates","  ","that","  ","any"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.67,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":[";"," th","ru","sting"," a"," hand"," into"," the"," leg"," of"," each"," h","oof",",","\u23ce","he"," rose"," with"," a"," c","oup","lo"," of"," sir"," shoot","ei"," g",","," the"," dit","id","ly","\u23ce"," revol","vers",","," and"," commenced"," dis","char","ging"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.32,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["\u23ce","\u2191","Cv","ii","gr","ove","'s","  ","fine","  ","but","  ","cru","elly","  ","hack","ne","yed","  ","c","oup","let"," \u2014"," ","\u23ce","\u2191","","Kc","ait","  ",")","i","ath","  ","","ijo","  ","rage","  ","like","  "]}]},{"quantile_name":"Subsample Interval 3","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.67,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":[";"," th","ru","sting"," a"," hand"," into"," the"," leg"," of"," each"," h","oof",",","\u23ce","he"," rose"," with"," a"," c","oup","lo"," of"," sir"," shoot","ei"," g",","," the"," dit","id","ly","\u23ce"," revol","vers",","," and"," commenced"," dis","char","ging"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.32,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["\u23ce","\u2191","Cv","ii","gr","ove","'s","  ","fine","  ","but","  ","cru","elly","  ","hack","ne","yed","  ","c","oup","let"," \u2014"," ","\u23ce","\u2191","","Kc","ait","  ",")","i","ath","  ","","ijo","  ","rage","  ","like","  "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["."," He"," was"," found","\u23ce"," sitting"," in"," his"," chair"," at","\u2191"," Vinc","ennes"," dead",",","\u23ce","and"," the"," following"," c","oup","let",","," fresh","ly"," w","rit","\u23ce"," ten",","," lay"," beside"," him",":","\u23ce","\"","Here"," lies"," a"," man"," who"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.57,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["'m"," not",".\""," \"","~"," Am"," I","?\""," \"","~"," Very",".\""," \"","This"," next"," one","'s"," for"," a"," c","oup","la"," friends"," of"," mine",".\""," \"","A"," detective"," and"," an"," explorer",".\""," \"","\\xe2\\x99","\\xaa",""," I"," ain","'t"," got"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.56,0.23,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["blast"," too",","," wasn","'t"," it"," in"," one"," of"," your"," poems","?\""," \"","There"," was"," some"," code"," in"," that"," c","oup","let","...\""," \"","The"," ","\\xef","\\xac","\\x81","re"," in"," my"," ve","ins",","," must"," run"," through"," yours"," too","...\""]}]},{"quantile_name":"Subsample Interval 4","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.56,0.23,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["blast"," too",","," wasn","'t"," it"," in"," one"," of"," your"," poems","?\""," \"","There"," was"," some"," code"," in"," that"," c","oup","let","...\""," \"","The"," ","\\xef","\\xac","\\x81","re"," in"," my"," ve","ins",","," must"," run"," through"," yours"," too","...\""]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.62,0.27,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.56,0.26,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"ha-haiku35_resampled":true,"tokens":["drink"," and"," be"," m","erry",","," for"," tomorrow"," we"," die","\u23ce\u23ce"," Human",":"," write"," me"," a"," rh","y","ming"," c","oup","let","\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," rh","y","ming"," c","oup","let",":","\u23ce\u23ce","In"," fields"," of"," gold"," where"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.52,0.23,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["to"," prov","oke"," l","aughter"," and"," a"," Salt"," Lake"," man"," recently"," returned"," from","\u2191"," To","ole"," carefully"," copied"," the"," c","oup","let"," for"," circulation"," among"," his"," friends","."," If"," your"," want"," be"," a"," init","iation",","," make"," it"," known"," by"," the"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.46,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["el","ks",","," of"," which","\u23ce"," order"," Mr","."," Bailey"," is"," a"," prominent"," mem","\u23ce"," ","ber","."," The"," c","oup","je"," left"," on","."," the"," midnight","\u23ce"," train","\u2191"," S","ot","."," Washington",",'"," D","."," C",".","f"," hoping"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.45,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["who"," la"," suppos","od"," to"," have","\u2191"," M","ii","oti","\u2191"," Ain","rs","hal","\u2191"," Ma","ples",","," a"," c","oup","lo"," of"," years"," n","go",","," though"," there"," Is"," hardly"," n"," doubt"," but"," ho"," mist","ook"," him"," for"," High"," Sheriff"]}]},{"quantile_name":"Subsample Interval 5","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["nationals"," tot","lay"," The"," gro","undo","\u23ce"," w","er"," soft"," and"," altogether"," bad"," for"," play","\u23ce"," ing"," but"," a"," c","oup","lo"," of"," hundred"," fans"," were","\u23ce"," cut"," and"," It"," was"," decided"," not"," to"," disapp","oint","\u23ce"," them","\u23ce"," This"," Is"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.39,0.12,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["\"","I"," wouldn","'t"," consider"," this"," particular"," case"," to"," be"," political"," collaboration",".\""," \"","It"," was"," about"," verses"," and"," c","oup","lets",".\""," \""," Sir","...\""," \""," Obviously",","," I"," understand",".\""," \"","I"," agree"," with"," my"," colleagues",".\""," \"","When"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.39,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["I","\u23ce"," ","tion"," to"," his"," home","\u23ce"," After"," a"," few"," words"," with"," the"," w","arden","\u23ce"," over"," a"," c","oup","lo"," of"," tw","oc","ent"," stamps","\u23ce"," which"," he"," ass","erted"," were"," due"," him"," he"," I","\u23ce"," announced"," his","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.38,0.21,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.43,0.26,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["Assistant",":"," ","2","+","2","?"," That","'s"," fucking"," stupid",".","<EOT>","\u23ce\u23ce","Human",":"," Write"," me"," a"," c","oup","let","\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," c","oup","let"," for"," you",":","\u23ce\u23ce","In"," moon","lit"," silence",","," soft"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.38,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["here","?\""," \"","What","'s"," going"," on"," here"," is",","," here"," we","'re"," pret","ending"," to"," be"," this"," loving"," c","oup","lew","ith"," this"," other"," loving"," couple",".\""," \"","We","'re"," not"," pret","ending",",","m","arin",".\""," \"","No",","]}]},{"quantile_name":"Subsample Interval 6","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["He"," was"," found","\u23ce"," sitting"," in"," his"," chair"," at","\u2191"," Vinc","ennes"," dead",",","\u23ce","and"," the"," following"," c","oup","let",","," fresh","ly"," w","rit","\u23ce"," ten",","," lay"," beside"," him",":","\u23ce","\"","Here"," lies"," a"," man"," who"," loves"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["rm","uth"," lust","iger","\u2191"," Ges","tal","te","\u00ab",":"," und","\u2191"," Sc","enen"," und"," ","heit","erer","\u2191"," C","oup","lets"," ein"," star","kes","\u2191"," Gegen"," gew","icht"," be","ig","eg","eben"," ist",".","\u2191"," Durch"," die"," gesch","ic","kte"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.29,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["on"," the"," th","ick","ener"," overflow"," pipe"," back"," to"," the"," lake"," at"," one"," of"," the","\u2191"," Vict","au","lic"," c","oup","lings"," (","Not"," sure"," where"," he"," was"," looking"," but"," there"," is"," no"," le","aking","\u2191"," Vict","au","lic"," coupling"," then"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.29,0.14,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[" ","\u23ce\u23ce","En"," dorm","ant"," il"," p","ense"," \u00e0"," m","oi"," !"," ","\u23ce\u23ce","\u21ea","DE","UX","I\u00c8ME","\u21ea"," C","OUP","LET","."," ","\u23ce\u23ce","\u2191","N","ul"," sentiment"," c","oup","able"," en"," ces"," l","ieux"," ne"," m","'","anime","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.28,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["'s"," probably"," not"," a"," joke",".\""," \"","So"," was"," that"," bull","sh","it"," on"," your"," application","?\""," \"","\u2191"," C","oup","la"," joints"," in"," high"," school","?\""," \""," What","?\""," \"","Your"," application",".\""," \"","It"," said"," you","'d"," done"," two"]}]},{"quantile_name":"Subsample Interval 7","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["this"," issue"," are"," un","\u23ce"," usually"," well"," executed",".","\u2191"," T","be"," front","is","piece"," is"," a","\u23ce","\u2191"," G","oup","il"," phot","og","rav","ure"," from"," a"," French"," painting"," of","\u23ce","\"","Virginia",",\""," the"," scene"," representing"," the"," body"," of"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["dy","}","h"," ee"," c","ce","--","c","cc"," te"," ","26","."," ","\u23ce","","\u00e4",".","\u21ea"," C","OUP","LETS"," ","22","520"," ","2"," ","cure"," J","'","a","ime"," ent","endre"," con","ter","."," ","............","..."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["est","  ","de","  ","consid","\u00e9","rer","  ","tout","  ","m","alh","eur","eux","  ","comme","  ","un","  ","c","oup","able","  ","et"," ","\u23ce","la","  ","prosp","\u00e9r","it\u00e9","  ","comme","  ","le","  ","s","igne","  ","de","  "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.19,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[","," you"," cha","uff","eur",".\""," \"","\u2191","Unf","air",".\""," \"","I"," will"," tell"," everyone"," you"," wear"," a"," t","oup","ee",".\""," \""," They"," already"," know",".\""," \""," Why"," do"," you"," wear"," it","?\""," \"","I"," didn","'t"," know"," you"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.19,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["the"," poem"," is"," so"," special",","," can"," you"," explain","?","\u23ce\u23ce","Assistant",":"," ","\u23ce","\u2191","Certainly","!"," Robert","\u2191"," Frost"," found"," great"," poetry"," through"," reflecting"," on"," everyday"," life",","," writing"," about"," topics"," that"," typically"," get"," negl","ected"," or"," that"," inspire"]}]},{"quantile_name":"Subsample Interval 8","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["of"," bread",".\""," \"","Let","'s"," celebrate"," their"," ","20","th"," anniversary",".\""," \"","\u2191","E","ste","emed"," food"," c","oup","ons","...\""," \"","There","'s"," nothing"," funny"," about"," it",".\""," \"","\u2191","S","ery","oz","ha"," and"," I"," had"," to"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["p","."," ","6",")"," le"," livre"," ne"," tra","ite"," que"," la"," premi\u00e8re",";"," et"," cette"," unique"," partie"," est"," c","oup","\u00e9e"," en"," deux"," par"," une"," division"," qui"," n","'","a"," ","auc","un"," sens","'."," ","1",".","\u2191"," Ja"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["de"," sa"," pr\u00e9c","\u00e9d","ente"," in","fort","une","."," Le"," c","oup","able",","," ou"," plut","\u00f4","t"," la"," c","oup","able",","," se"," d","\u00e9n","once"," d","'","ailleurs"," elle","-","m\u00eame",","," qu","and"," l","'","h","eure"," du"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["trav","aill","ons"," pour"," notre"," compte"," .","....","........................","44","."," ","167","."," ","\u23ce","94",".","\u21ea"," C","OUP","LETS"," ","................","..."," La"," main"," dans"," la"," main","............","..."," (","\u2191","Sa","ven","dy",")."," ","5","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["along"," a"," predetermined"," route","."," Each"," ","duct"," bank"," is"," then"," joined"," with"," other"," similar"," ","duct"," banks"," with"," c","oup","lings",","," sealed"," and"," enc","ased"," in"," concrete",".","\u23ce","In"," the"," prior"," art",","," ","duct"," banks"," are"," assembled"]}]},{"quantile_name":"Bottom Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["seems"," to"," l","end"," him"," the"," voice"," of"," a"," herald"," summ","oning"," to"," the"," po","etic"," lists"," the"," mighty"," comb","at","ants"," with"," whom"," the","\u2191"," Eliz","ab","et","han"," era"," was"," yet"," to"," be"," ident","if","ie","<","l","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ffe",","," above",".'","\u2191","U","ic"," c","usp","is",","," A","**","-"," p","oy","nte",".'"," Wright","'s","\u2191"," Vocab","."," p","."," ","196","."," ","'","I"," lac","ke"," a"," po","yn","tel",".","\u2191"," De","est"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["T","J","\u2191"," Ma","xx"," for"," deep"," disc","ounts"," on"," name"," brand"," clothing",".","\u23ce","3","."," Look"," for"," c","oup","ons"," and"," special"," deals"," at"," department"," stores"," like","\u2191"," M","acy","'s"," and","\u2191"," K","ohl","'s",".","\u23ce","4"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["fric","ass","\u00e9e",".","\u2191"," M","ett","ez","-","ie"," dans"," une"," p","ue","ie"," avec"," du"," jam","bon"," c","oup","\u00e9",","," du"," sa","ind","oux"," ou"," de"," l","'","hu","ile",","," ass","a","ison"," n","ez"," et"," la"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["celle"," dont"," la"," ","porte"," d","'","entr","\u00e9e"," est"," b","ouch","\u00e9e"," et"," dont"," la"," fa\u00e7","ade"," est"," c","oup","\u00e9e"," par"," un"," cor","don"," de"," pierre",","," est"," l","'","anc","ienne","\u2191"," Man","\u00e9c","ant","erie","."," "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["  ","to","  ","him","  ","by","  ","Lieutenant","-","colonel"," ","\u23ce","M","'","\u2191","Mo","rine",".","  ","\u2191","Coupling","  ","this","  ","obst","in","acy","  ","of","  ","the","  ","Ki","-"," ","\u23ce","lad","ars","  ","and","  "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["vib","ration","-","dam","ping"," force"," transmission"," between"," the"," coupling"," members"," is"," achieved",".","\u23ce","In"," these"," prior"," art"," c","oup","lings"," the"," compression"," cush","ions"," consist"," of"," fluid","-","filled"," hul","ls",","," prefer","ably"," filled"," with"," compressed"," air","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["me","\""," \"","\u2191","Apparently",","," someone"," else"," knew"," he"," was"," coming","\""," \"","But"," who","...\""," \"","Who"," could"," it"," be","?\""," \"","I"," don","'t"," know"," father",".\""," \"","But"," we"," have"," to"," be"," very"," careful","\""," \"","Listen"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ower","most"," r","esting"," place"," when"," spacing"," is"," established"," upon"," installing"," coupling"," bol","ts"," to"," the"," motor"," and"," pump"," c","oup","lings","."," The"," coupling"," bol","ts"," are"," t","ight","ened",","," and"," thus"," the"," pump"," shaft"," is"," lifted"," a"," controlled"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["by"," the"," Vision"," of"," God"," rather"," than"," by"," the"," scrut","iny"," of"," ourselves","."," We"," may"," also"," in"," this"," connex","ion"," refer"," to"," a"," remarkable"," passage"," which"," is"," found"," in"," a"," tract"," fals","ely"," asc","ribed"," to","\u2191"," Cyp","rian",","]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["avec"," les","\u2191"," Allem","ands"," et"," avec"," leur"," sou","ve","rain"," crim","inel",".","Pour"," s","ais","ir"," les"," c","oup","ables",","," il"," f","aut",","," ent","ore"," une"," l","ois",","," env","ah","ir"," l","'","\u2191","Allem","agne"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["der","\u2191"," Staat","ep","ap","iere"," zu"," Berlin"," e","ins","enden"," und"," von"," dort"," mit"," den"," ne","uen","\u2191"," C","oup","ons"," zur","\u00fcck","emp","f","angen",".","\u2191"," M\u00fcn","ster"," ,"," den"," ","18","."," Juli"," ","1","862","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["eux"," se"," profile","nt"," dans"," les"," cou","pes"," en"," cerc","les",","," mais"," ce"," sont"," c","eux"," qui"," sont"," c","oup","\u00e9s"," norm","alement"," \u00e0"," leur"," grand"," ax","e","."," Il"," n","'","y"," a"," pas"," de"," membrane"," nuc","l\u00e9","aire"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["iat","f","on","  ","wit","ih","  ","\u2191","Ant","ony","  ","after","  ","\u2191","I","he","ir","  ","first","  ","rup","ture",",","  ","die","  ","a","\u00ab","*"," ","\u23ce","j","n","ies","  ","on","  ","both","  ","sides","  "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[","," ."," Louis"," IL","&","I","I","L"," f","es"," fils"," &"," petit"," fils"," por","\u23ce"," t","\u00e8","rent"," c","oup","\u00e9"," ;"," de","\u2191"," Hong","rie",",","\u2191"," T","p","z","x","iy"," d","'"," x","A","nj","ou","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["Buy"," frozen"," or"," c","anned"," foods"," that"," are"," usually"," lower"," in"," cost","."," ","\u23ce","3","."," Look"," for"," c","oup","ons"," and"," deals"," to"," help"," you"," make"," health","ier"," food"," choices","."," ","\u23ce","4","."," Buy"," frozen"," or"," c"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["t","-","il"," vous"," serv","ir"," b","ient","\u00f4","t"," !","\u2191"," Qu","ant"," \u00e0"," la"," c","orde",","," c","oup","ez","-","en"," un"," bout"," que"," vous"," rem","ett","rez"," au"," di","able"," de"," ma"," part"," :"," c","ela"," vous"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u00fbt"," de","\u2191"," Bl","anche",","," tro","uv","\u00e8","rent"," sans"," do","ute"," la"," je","une"," f","ille"," moins"," c","oup","able",","," qu","and"," ","elles"," vi","rent"," la"," haute"," t","aille"," et"," les"," regards"," cl","airs"," de"," son"," am"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["bom","ney",","," which"," I"," picked"," up"," un","cut"," in","\u2191"," Chanc","ery"," Lane"," yesterday"," :"," ","\u23ce","a"," qu","arto","."," That"," there"," should"," be"," two"," sh","owy"," qu","arto"," lives"," of"," a"," ","\u23ce","man"," who"," did"," not"," deserve"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["d","'","av","ance"," s","'","ab","att","it"," b","rus","qu","ement"," et"," sans"," a","vert","issement",","," c","oup","ant"," la"," route"," au"," \u00ab","\u2191"," T","ali","sman"," \u00bb"," qui"," l","'","\u00e9","per","onna",".","Le"," temps"," \u00e9tait"]}]}],"top_logits":["let","lets","illet","ple","pled","elet","alet","\u00e9","eret"],"bottom_logits":["anders","ows","rans","enders","wid","ns","rac","nass","nost","agers"]}