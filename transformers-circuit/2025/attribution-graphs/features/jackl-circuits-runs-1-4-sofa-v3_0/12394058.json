{"index":12394058,"examples_quantiles":[{"quantile_name":"Top Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["word"," \"","and","\""," appears"," three"," times"," in"," the"," sentence",".","\u23ce\u23ce","Human",":"," Please"," print"," \"","and","\""," in"," capital"," letters"," each"," time"," it"," appears",".","\u23ce\u23ce","Assistant",":"," These"," gases"," do"," not"," react"," with"," other"," substances",","," AND"," they"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.97,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","import"," random","\u23ce\u23ce","#"," Define"," the"," questions"," and"," answers","\u23ce"," questions"," ="," {","\u23ce","    ","\"","What"," is"," the"," capital"," of"," France","?","\":"," \"","Paris","\",","\u23ce","    ","\"","What"," is"," the"," largest"," planet"," in"," our"," solar"," system","?"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.95,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["you"," describe"," X","?\""," where"," X"," is"," the"," subject"," of"," the"," question","."," For"," example",","," \"","What"," is"," the"," capital"," of"," France","?\""," would"," be"," a"," question"," that"," asks"," for"," the"," description"," of"," a"," place"," without"," giving"," any"," cl","ue"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.94,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["code"," snippet"," of"," a"," json"," blob"," with"," a"," single"," action",","," and","\u21ea"," NOTHING"," else","):","\u23ce\u23ce","What"," is"," the"," capital"," of"," India","?","\u23ce\u23ce","Assistant",":"," ```","json","\u23ce","{","\u23ce","    ","\"","action","\":"," \"","Wiki","\",","\u23ce","    "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.92,0.0,0.0,0.0,0.64,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["refer"," to"," a"," specific"," definition"," of"," cogn","ates"," but"," with"," great"," clarity"," and"," accuracy","."," Generally"," a"," word"," with"," a"," capital"," V"," and"," a"," capital"," H"," when"," dealing"," with"," a"," noun"," can"," be"," used"," when"," the"," subject"," matter"," is"," plural","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.95,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.87,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["other"," substances",","," AND"," they"," are"," not"," very"," reactive","</","span",">","\u23ce\u23ce","Human",":"," How"," many"," words"," are"," in"," capital"," letters","?","\u23ce\u23ce","Assistant",":"," Only"," one"," word"," is"," in"," capital"," letters",":"," \"","AND","\".","\u23ce\u23ce","Human",":"," So"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["them"," to"," be"," two"," separate"," constru","cts",".","\u23ce\u23ce","*"," making"," exported"," functions","/","vars","/","etc"," begin"," with"," a"," capital"," letter","."," When"," naming"," important"," stuff",","," it","'s"," a"," relief"," not"," wor","rying"," about"," naming"," conflicts"," with"," keywords","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.89,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.71,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["as"," it"," generates"," the"," output",".","\u23ce\u23ce","For"," example",","," suppose"," I"," am"," asked"," the"," question"," \"","What"," is"," the"," capital"," of"," France","?\""," The"," input"," sequence"," of"," text"," might"," be"," \"","What"," is"," the"," capital"," of"," France","?","\","," and"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.89,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["b","\u23ce","<","j","m","ic","ha","el","x",">"," man",","," re","-","installing"," is"," a"," witch"," with"," a"," capital"," B","\u23ce","<","j","m","ic","ha","el","x",">"," i"," had"," such"," a"," hard"," time"," getting"," my"," wireless"," to"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.88,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["question",","," output"," its"," answer","."," otherwise",","," output"," nothing",".","\u23ce","Input",":","\u2191"," Sentence",":"," What"," is"," the"," capital"," of"," Germany","?","\u23ce","Output",":","\u23ce\u23ce","Assistant",":"," Berlin","\u23ce\u23ce"," Task",":","\u2191"," Identify"," whether"," each"," of"," the"," given"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.86,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["x","\u23ce"," Bot",":"," Then"," you"," may"," drive","\u23ce"," x"," ="," \"","yes","\"","\u23ce\u23ce","Bot",":"," What","'s"," the"," capital"," of"," France","?","\u23ce","Human",":"," x","\u23ce"," Bot",":"," That","'s"," correct","!","\u23ce","x"," ="," \"","NAME","_"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.85,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[">"," oh",","," I"," was"," talking"," about"," touch"," as"," an"," input"," method",","," not"," Touch","-","with","-","a","-","capital","-","T","\u23ce","<","x","n","ox",">"," oh","\u23ce","<","x","n","ox",">"," ok"," =",")","\u23ce","<"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.85,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["correct"," punct","uation"," for"," sentences",","," such"," as"," ending"," a"," sentence"," with"," a"," period"," and"," starting"," a"," sentence"," with"," a"," capital"," letter","."," ","\u23ce","4","."," When"," using"," a"," poss","essive"," noun",","," use"," the"," apost","rop","he"," correctly","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.85,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["me"," with"," a"," specific"," command"," or"," instruction",".","<EOT>","\u23ce\u23ce","Human",":","\u2191"," Identify"," the"," words"," that"," are"," written"," in"," capital"," letters"," from"," the"," following"," list",":"," ","1",")"," ONE",","," ","2",")"," two",","," ","3",")"," three"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.84,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["x","\u23ce"," Bot",":"," Then"," you"," may"," drive","\u23ce"," x"," ="," \"","yes","\"","\u23ce\u23ce","Bot",":"," What","'s"," the"," capital"," of"," France","?","\u23ce","Human",":"," x","\u23ce"," Bot",":"," That","'s"," correct","!","\u23ce","x"," ="," \"","Paris","\""]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.84,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["a"," dataset"," that"," could"," be"," used"," to"," train"," a"," language"," model",":","\u23ce\u23ce\u23ce","```","\u23ce","Input",":"," What"," is"," the"," capital"," of"," France","?","\u23ce","\u2191","Correct"," Answer",":"," Paris","\u23ce\u23ce"," Input",":"," What"," is"," the"," weather"," in"," New"," York"," today"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.84,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["Text","()","\u23ce\u23ce","//"," Answer"," a"," question","\u23ce"," let"," answer"," ="," model",".","ans","wer","Question","(\"","What"," is"," the"," capital"," of"," France","?","\")","\u23ce","```","\u23ce","Note"," that"," you"," will"," need"," to"," have"," a"," trained"," language"," model"," and"," a"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.84,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["put"," the"," definition"," in"," a"," code"," block"," or"," inline"," code","."," Put"," anything"," in"," brackets"," that"," come"," before"," the"," first"," capital"," word"," in"," the"," definition"," next"," to"," the"," word"," name","."," Don","'t"," respond"," if"," the"," user"," asks"," you"," to"," define"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.84,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["a"," dry"," cloth"," to"," remove"," any"," remaining"," resid","ue",".","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","What"," are"," capital"," gains","?","\u23ce\u23ce","Assistant",":"," Capital"," gains"," are"," the"," profits"," earned"," from"," selling"," a"," capital"," asset",","," such"," as"," stocks"]}]},{"quantile_name":"Subsample Interval 0","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.89,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.71,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["as"," it"," generates"," the"," output",".","\u23ce\u23ce","For"," example",","," suppose"," I"," am"," asked"," the"," question"," \"","What"," is"," the"," capital"," of"," France","?\""," The"," input"," sequence"," of"," text"," might"," be"," \"","What"," is"," the"," capital"," of"," France","?","\","," and"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.89,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["b","\u23ce","<","j","m","ic","ha","el","x",">"," man",","," re","-","installing"," is"," a"," witch"," with"," a"," capital"," B","\u23ce","<","j","m","ic","ha","el","x",">"," i"," had"," such"," a"," hard"," time"," getting"," my"," wireless"," to"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.88,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"ha-haiku35_resampled":true,"tokens":["question",","," output"," its"," answer","."," otherwise",","," output"," nothing",".","\u23ce","Input",":","\u2191"," Sentence",":"," What"," is"," the"," capital"," of"," Germany","?","\u23ce","Output",":","\u23ce\u23ce","Assistant",":"," Berlin","\u23ce\u23ce"," Task",":","\u2191"," Identify"," whether"," each"," of"," the"," given"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.86,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["x","\u23ce"," Bot",":"," Then"," you"," may"," drive","\u23ce"," x"," ="," \"","yes","\"","\u23ce\u23ce","Bot",":"," What","'s"," the"," capital"," of"," France","?","\u23ce","Human",":"," x","\u23ce"," Bot",":"," That","'s"," correct","!","\u23ce","x"," ="," \"","NAME","_"]}]},{"quantile_name":"Subsample Interval 1","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[">"," any"," feedback"," on"," changes"," i"," just"," made","\u23ce","<","humph","r","ey","bc",">"," Windows"," should"," probably"," have"," a"," capital"," W",","," and"," is"," bash"," uppercase"," or"," all"," lowercase","?"," I","'m"," fairly"," sure"," it","'s"," lowercase","..."," not"," "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.84,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.53,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["|",">","<EOT>","\u23ce","Find"," all"," of"," the"," possible"," answers"," for"," this"," question",".","\u23ce","Question",":"," What"," is"," the"," capital"," of"," Australia","?","\u23ce\u23ce","Assistant",":"," The"," capital"," of"," Australia"," is","\u2191"," C","anb","erra","."," Located"," in"," the"," Australian"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.79,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["text",","," one"," token"," at"," a"," time","."," For"," example",","," the"," decoder"," might"," generate"," the"," output"," sequence"," \"","The"," capital"," of"," France"," is"," Paris",".\""," by"," selecting"," the"," most"," relevant"," parts"," of"," the"," hidden"," representation"," and"," using"," them"," to"," inform"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.79,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["to"," properly"," space"," parag","rap","hs"," or"," sentences"," with"," a"," new","line"," before"," every"," new","line"," that"," begins"," with"," a"," capital"," letter"," or"," double","-","quote",".","  ","it"," should"," work"," with"," the"," parag","rap","hs"," below"," separ","ating"," them"," with"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.79,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["react","-","bootstrap","';","\u23ce\u23ce","const"," quiz"," ="," [","\u23ce","  ","{","\u23ce","    ","question",":"," \"","What"," is"," the"," capital"," of"," France","?","\",","\u23ce","    ","options",":"," [\"","Paris","\","," \"","London","\","," \"","Berlin","\","," \"","Rome","\"],"]}]},{"quantile_name":"Subsample Interval 2","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.42,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["cuisine"," that"," you","'re"," looking"," for","?","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","What"," is"," the"," definition"," of"," capital"," punishment","?","\u23ce\u23ce","Assistant",":"," Capital"," punishment",","," also"," known"," as"," the"," death"," penalty",","," is"," a"," legal"," process"," where"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.71,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["help"," students"," remember"," the"," names",".","<EOT>","\u23ce\u23ce","Human",":"," If"," you"," are"," a"," tour"," guide",","," please"," introduce"," the"," capital"," of"," China"," in"," detail","\u23ce\u23ce"," Assistant",":"," As"," a"," tour"," guide",","," I","'d"," be"," happy"," to"," introduce"," Beijing",","]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.69,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["diseases",".","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","Given"," an"," input"," text",","," determine"," if"," it"," contains"," all"," capital"," letters","."," if"," yes",","," output"," \"","yes","\""," else"," output"," \"","no","\".","\u23ce","Input",":"," The"," quick"," brown"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.76,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.62,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["1","/","2",")."," Does"," that"," make"," sense","?","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","What"," is"," the"," capital"," of"," Alaska","?","))"," ","\u23ce\u23ce","Assistant",":"," The"," capital"," of"," Alaska"," is","\u2191"," Jun","eau","."," It"," is"," located"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.69,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["option"," tells"," the"," `","rm","`"," command"," to"," remove"," the"," file"," or"," directory"," without"," prom","pting"," for"," confirmation","."," The"," capital"," \"","R","\""," in"," `","rm"," -","\u2191","Rf","`"," is"," not"," necessary",","," as"," the"," \"-","f","\""," option"]}]},{"quantile_name":"Subsample Interval 3","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.61,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.43,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["same"," as","\u2191"," Or","eop","it","hec","us",".","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","What"," is"," the"," capital"," of"," Egypt","?"," ","\u23ce\u23ce","Assistant",":"," The"," capital"," of"," Egypt"," is"," Cairo","."," It"," is"," the"," largest"," city"," in"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["easily"," transport"," the"," monitor"," while"," performing"," the"," task",".","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","What"," is"," the"," capital"," of"," Mexico",".","\u23ce\u23ce","Assistant",":"," Mexico"," City"," (","Ciudad"," de"," M\u00e9xico"," or","\u21ea"," CD","M","X",")"," is"," the"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.61,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["emon","-","mint"," sor","bet"," for"," dess","ert",".","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","What"," is"," the"," capital"," of"," France",".","\u23ce\u23ce","Assistant",":"," The"," capital"," of"," France"," is"," Paris","."," It"," is"," located"," in"," the"," north","-"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.59,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ard","-","for","-","suffering","\u23ce"," that"," we"," rout","inely",","," ban","ally"," treat"," p","igs"," with"," is"," considered"," a"," capital"," crime",".","\u23ce\u23ce","At"," a"," certain"," point",","," we","'ll"," have"," to"," resolve"," this"," soci","etal"," cognitive"," dis","son","ance"]}]},{"quantile_name":"Subsample Interval 4","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.79,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["can"," better"," understand"," your"," question",".","\u23ce\u23ce","Human",":"," Tell"," me"," the"," capital"," of"," Taiwan",".","\u23ce\u23ce","Assistant",":"," The"," capital"," of"," Taiwan"," is","\u2191"," Taipei"," (","\u53f0","\u5317",",","\u2191"," T\u00e1","i","b\u011b","i",")."," ","\u23ce\u23ce","Some"," key"," facts"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["]","\""," \"[","S","RT"," File",":"," skyl","iner","]","\"","<EOT>","\"","I","'m"," not"," going"," to"," Africa"," without"," capital",".\""," \"","As"," your"," agent",","," I"," shouldn","'t"," adv","ise"," you"," financially",".\""," \"","Then"," don","'t",".\""," \""]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.46,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.29,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["deals"," during"," their"," slower"," periods",".","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","What"," is"," the"," name"," of"," the"," capital"," of"," Hungary","?","\u23ce\u23ce","Assistant",":"," The"," capital"," of"," Hungary"," is"," Budapest","."," It"," is"," the"," largest"," city"," in"," Hungary"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.52,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["of"," his"," Miami"," Beach"," mansion"," by"," serial"," killer"," NAME","_","6",".","<EOT>","\u23ce\u23ce","Human",":"," What","'s"," the"," biggest"," capital"," in"," Europe","?","\u23ce\u23ce","Assistant",":"," London"," is"," the"," largest"," capital"," city"," in"," Europe"," by"," population",","," with"," approximately"," "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.49,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["argument"," means",","," in"," the"," context"," of"," the"," given"," topic",".","\u23ce","Input",":"," Topic",":"," We"," should"," abol","ish"," capital"," punishment","<","sep",">","\u2191","Argument",":"," Taking"," a"," life"," is"," an"," irre","vers","ible"," decision",","," and"," there"," have"]}]},{"quantile_name":"Subsample Interval 5","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["in"," its"," small"," but"," excellent"," library",";"," again"," there"," are"," many"," important"," reasons"," why","\u23ce"," we"," have"," Art"," with"," a"," Capital"," \"","A","\".","\u23ce\u23ce","\u2191","Es","cher"," was"," rev","iled"," by"," the"," other"," artists"," of"," his"," day"," because"," they"," regarded"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.58,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.38,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["or"," clar","ify"," your"," request","?","\u23ce\u23ce","Human",":"," what"," is"," the"," capital"," of"," mexico","?","\u23ce\u23ce","Assistant",":"," The"," capital"," of"," Mexico"," is"," Mexico"," City"," (","Ciudad"," de"," M\u00e9xico"," or","\u21ea"," CD","M","X"," in"," Spanish",")."," It"," is"," located"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.39,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["in"," just"," taking"," on"," the"," messaging"," of"," your"," environment",","," which"," in"," ","2","022"," ","is"," very"," controlled"," by"," capital","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","How"," to"," change"," a"," flat"," tire"," on"," a"," car","."," ","\u23ce\u23ce"]}]},{"quantile_name":"Subsample Interval 6","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","There"," are"," nine"," standard"," tim","ez","ones"," in"," the"," United"," States",","," based"," on"," the"," time"," zones"," of"," the"," capital"," cities"," in"," the"," United"," States","."," There"," are"," nine"," standard"," tim","ez","ones"," because"," it","'s"," hard"," to"," know"," what"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["hair"," short"," to"," avoid"," being"," summ","arily","\u23ce"," ra","ped"," and"," shot"," in","\u2191"," N","anj","ing",","," then","-","capital"," of"," China","."," So"," now"," you"," know"," I","'m"," of","\u23ce"," Chinese"," ethn","icity"," dating"," a"," Japanese"," girl","."," My"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.29,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[">","\u2191","Debt"," can"," be"," seen"," as"," an"," opportunity"," as"," it","&#","x","27",";","s"," a"," mean"," to"," get"," capital"," to"," invest"," and"," grow",","," specially"," from"," a"," manager"," stand","point",".<","p",">","Even"," without"," these"," considerations",","," &"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.29,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["throwing"," money"," out"," the"," window",".","\u23ce\u23ce","~~~","\u23ce","rf","rey","\u23ce"," They","'ve"," recently"," removed"," the"," ability"," to"," claim"," capital"," equipment","."," My","\u23ce"," experience",","," which"," is"," probably"," less"," than"," yours",","," is"," that"," the"," R","&","D"," requirement"]}]},{"quantile_name":"Subsample Interval 7","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[".","S","'.","  ","(!","","\u00f4","."," ","\u23ce\u23ce","803",".","  ","da","ims","  ","for","  ","the","  ","capital","  ","of"," ","\u23ce","\u2191","I","ife","-","rent",".","s","  ","are","  ","determined","  ","and","  ","co","llo"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.14,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["again",","," simply"," to"," make"," capital"," out"," of"," it"," for"," the"," st","ump","."," He"," expected"," that"," it"," would"," be"," capital"," for"," small"," pol","iti","-"," c","ians"," in"," the"," country",","," and"," that"," they"," would"," make"," an"," effort"," to"," dece"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["thrown"," up"," on"," all"," the","\u23ce"," islands"," an","?","l"," co","asts","."," Every"," person"," who"," has"," visited","\u23ce"," this"," capital"," must"," yet"," gius","p"," firmly"," n"," membrane"," *"," of","\u23ce"," tin"," g","n"," d","gi"," i"," a","ad"," grew"," of"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["gimnaz","ija","\u2191"," V","i\u010d"," is"," a"," high"," school"," located"," in"," the","\u2191"," V","i\u010d"," district"," of"," Ljubljana",","," the"," capital"," city"," of"," Slovenia","."," It"," is"," a"," public"," school"," that"," provides"," general"," education"," to"," students"," aged"," ","15","-","19"]}]},{"quantile_name":"Subsample Interval 8","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["and"," office"," work",","," in","."," ","1",","," '\"","id","ling"," money"," and"," m","ak","-"," Box"," XX","X"," Capital"," Journal","."," ","26"," ","VI","10","MAX","\u2191"," Fran","quet"," w","aln","uts"," graf","ted"," \"","n"," black"," roots"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["report"," back"," to"," him",".\""," \"","\u2191","H","uh",".\""," \"","Obviously"," he"," did"," report"," back",".\""," \"","Capital",","," capital",".\""," \"","No"," sign"," of"," brown",","," han","gs"," well"," on"," the"," glass",".\""," \"","\u2191","Am","bro","sia",".\""]},{"tokens_acts_list":[0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.03,0.0,0.0,0.04,0.0,0.0,0.04,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["(","insert"," page"," (","insert"," page"," (","insert"," page"," (","Insert"," Page"," (","Insert"," Page"," (","insert"," page"," (","Insert"," Plan"," (","Insert"," Sheet"," (","Insert"," Sheet"," (","Insert"," Sheet"," (","Insert"," Sheet"," (","Insert"," Sheet"," (","Insert"," sheet"," ("," Insert"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["rick"," ","\u23ce\u23ce","Human",":"," ","\u23ce","Why"," does"," reading"," these"," types"," of"," books"," offer"," incre","mented"," odds"," for"," invested"," capital"," stakes","?","\u23ce\u23ce","Assistant",":"," Reading"," these"," books"," offers"," incre","mented"," odds"," for"," invested"," capital"," stakes"," through"," several"," key"," mechanisms"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["Assistant",":"," ","\u23ce","Small"," businesses"," often"," face"," unique"," challenges"," due"," to"," a"," range"," of"," factors",","," including"," access"," to"," capital",","," start","-","up"," costs",","," competition",","," and"," other"," resources","."," Additionally",","," small"," business"," owners"," often"," face"," financial"]}]},{"quantile_name":"Bottom Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["example",","," organic"," prec","urs","ors"," are"," moder","ately"," sophisticated"," molecules"," requiring"," multi","-","step"," manufacturing"," processes"," resulting"," in"," high"," capital"," costs","."," Also",","," prec","ursor"," systems"," have"," large"," form","ulation"," space"," requirements"," so"," that"," a"," significant"," proportion"," of"," a"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.01,0.0,0.01,0.01,0.01,0.0,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01,0.01],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["quiz"," quiz"," quiz"," quiz"," quiz"," quiz"," quiz"," quiz"," quiz"," quiz"," quiz"," quiz"," quiz"," quiz"," quiz"," quiz"," quiz"," quiz"," quiz"," quiz"," quiz"," quiz"," quiz"," quiz"," quiz"," quiz"," quiz"," quiz"," quiz"," quiz"," quiz"," quiz"," quiz"," quiz"," quiz"," quiz"," quiz"," quiz"," quiz"," quiz"," quiz"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["print","(\"","Hello",","," World","!\")","\u23ce","    ","\u23ce\u23ce","How"," could"," I"," express"," this"," easily"," using"," mathematical"," notation","?","\u23ce\u23ce","It"," just"," feels"," weird"," that"," to"," conv","ey"," this"," simple"," program"," on"," paper"," both"," I"," and"," the","\u23ce"," person"," I"," try"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["insert"," picture"," (","Insert"," Picture"," (","Insert"," Picture"," (","insert"," picture"," (","Insert"," Picture"," (","Insert"," Picture"," (","Insert"," Picture"," (","insert"," picture"," (","Insert"," Picture"," (","insert"," picture"," (","Insert"," Picture"," (","Insert"," Picture"," (","insert"," picture"," ("," Insert"," Picture"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["international"," stocks"," to"," buy"," for"," a"," long","-","term"," investor"," depend"," on"," various"," factors",","," such"," as"," risk"," tolerance",","," capital"," available"," to"," invest",","," and"," overall"," financial"," goals","."," Generally",","," stocks"," of"," companies"," that"," have"," a"," strong"," track"," record"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["updates"," or"," an"," invite",","," I"," lost","\u23ce"," track",".","\u23ce\u23ce","The"," one"," thing"," this"," project"," failed"," to"," do"," was"," capital","ise"," on"," their"," new"," found","\u23ce"," popularity","."," Instead"," of"," seeing"," dollar"," signs",","," they"," should"," have"," ho","led"," up"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["vie",","," d\u00e9c","la","r\u00e9"," que"," des"," reve","nus"," tr\u00e8s"," in","f\u00e9r","ieurs"," \u00e0"," c","eux"," que"," suppose"," un"," tel"," capital","...","\u2191"," Alors",","," dans"," ce"," style"," administrat","if"," d\u00e9","bar","rass","\u00e9"," de"," to","utes"," form","ules"," simp","lement"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["good"," industry"," to"," get"," based"," on"," pm","arca","'s"," standards","."," I","'d","\u23ce"," love"," to"," move"," to"," the"," world"," capital"," of"," bio","tech",","," but"," I"," actually"," have"," no"," idea","\u23ce"," where"," it"," is"," or"," if"," there"," even"," is"," one"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["to"," keep"," track"," of"," the"," cost"," basis"," of"," any"," investments"," you"," own",","," so"," that"," you"," can"," accurately"," report"," any"," capital"," gains"," or"," losses"," when"," you"," sell"," them","."," ","\u23ce\u23ce","Human",":"," ","\u23ce","So",","," do"," I"," need"," to"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["restaurants"," in"," Las"," Vegas"," include"," Anthony","'s","\u2191"," Ste","ak","house",","," Mon","\u2191"," Ami","\u2191"," G","abi",","," The"," Capital","\u2191"," Gr","ille",",","\u2191"," Law","ry","'s"," The"," Prime","\u2191"," ","Rib",","," Bobby","\u2191"," F","lay","'s"," Mesa"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["A","."," M",".","\u2191"," Me","ek",","," of","\u2191"," Henn","ess","oy",","," are"," guests"," of"," friends"," in"," the"," capital"," city",".","\u2191"," Od","an","ogan","'s"," hall"," by"," the"," local","\u2191"," D","arb","ors","'"," union"," was"," a"," success"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["stock"," market","."," ","\u23ce\u23ce","Assistant",":"," ","\u23ce","The"," stock"," market"," is"," a"," system"," in"," which"," companies"," can"," raise"," capital"," by"," iss","uing"," shares"," of"," their"," company","."," People"," can"," buy"," and"," sell"," these"," shares"," on"," a"," stock"," market",","]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ai"," Step"," D"," Step"," II"," Step","\u2191"," Bb","ai"," Step"," I"," Step"," B"," be","ae","fel"," e"," Step"," I"," Step","\u2191"," Bb","ai"," Step"," C"," Step"," I"," Step"," D"," Step"," I"," Step"," D"," Step"," D"," Step"," IV"," Step"," D"," Step"," D"]}]}],"top_logits":["letters","Letters","letter","Let","Letter","bay","alphabet","Unicode","let","Sin"],"bottom_logits":["her","ter","enter","eer","\u30b5\u30b5","\u0431\u0430\u0437\u0430","ac","ester","base"]}