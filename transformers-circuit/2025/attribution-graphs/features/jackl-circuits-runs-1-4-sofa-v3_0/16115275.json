{"index": 16115275, "examples_quantiles": [{"quantile_name": "Top Activations", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["\u23ce", "coher", "ent", "p", "ony", "\u23ce", " Yeah", ".", " I", "'m", " a", " scientist", " that", " deals", " with", " huge", " datasets", ".", " _", "", "Huge", "_", ".", " I", " must", " admit", " that", "\u23ce", " I", " do", " cr", "inge", " a", " little", " every", " time", " I", " see", " the", " words", " "]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.67, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["works", " (", "+", " the", " overhead", " each", " function", " adds", " -", "\u23ce", "parameters", ",", " return", " values", ",", " etc", ")", "\u23ce\u23ce", "Sure", ",", " small", " functions", " have", " their", " place", ",", " but", " sometimes", " it", " is", " easier", " to", " have", " a", "\u23ce", " big", " block", " of", " code", " that", " does"]}, {"tokens_acts_list": [0.0, 0.0, 0.67, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98, 0.0, 0.0, 0.56, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["a", "\u23ce", " big", " block", " of", " code", " that", " does", " what", " you", " need", ".", " Or", " as", " another", " comment", " said", ",", " write", "\u23ce", " small", " functions", " but", " big", " procedures", ".", "\u23ce\u23ce", "Now", " for", " my", " favourite", " pet", " pe", "eve", ":", " that", " ", "80", " ", "columns", " is"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.88, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "ha-haiku35_resampled": true, "tokens": ["more", " children", " or", " big", " families", " but", " I", " certainly", " dont", " understand", " the", " ridic", "ule", " that", " some", " people", " can", " get", " from", " having", " large", " families", " and", " dont", " see", " that", " as", " right", " at", " all", ".", "\u23ce\u23ce", "Assistant", ":", "Here", "'s", " a", " balanced", " response", " to", " your"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.52, 0.0, 0.0, 0.0, 0.0, 0.0, 0.92, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["\u23ce", "As", " a", " small", " market", " purs", "uer", ",", " I", " had", " to", " say", " that", " a", " tiny", " market", " is", " different", " from", "\u23ce", " tiny", " effort", ".", "\u23ce\u23ce", "I", " developed", " tor", "app", " guill", "oche", " designer", " (<", "http", "://", "www", ".", "tor", "app", ".", "info", ">)"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.91, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["that", "'s", " full", " of", " love", " and", " warm", "th", ",", "A", " heart", " that", "'s", " always", " in", " touch", ".", "\u23ce\u23ce", "With", " a", " big", " heart", ",", " a", " heart", " that", "'s", " true", ",", "A", " heart", " that", "'s", " kind", " and", " loving", " too", ",", "A", " heart", " that"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.85, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.54, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["expertise", "._", "\u23ce\u23ce", "Its", " actually", " easier", " to", " demonstrate", " expertise", " on", " bigger", " markets", ",", " due", " to", " its", "\u23ce", " density", ".", " A", " small", " market", " can", " only", " have", " so", " many", " experts", ".", "\u23ce\u23ce", " ", "_", "In", " a", " smaller", " market", ",", " you", " can", " iterate", " faster"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.37, 0.58, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.83, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": [",", " un", " petit", " monde", ",", " ni", " m\u00eame", " une", " pet", "ite", " soci\u00e9t\u00e9", ",", " il", " est", " encore", " plus", " pr\u00e9c", "is", "\u00e9ment", " un", " petit", " \u00e9tat", " o\u00f9", " se", " retro", "uvent", " le", " pou", "voir", " l\u00e9gisl", "at", "if", ",", " le", " pou", "voir", " ex", "\u00e9c", "ut", "if", ","]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.82, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["and", " understanding", " the", " intended", " meaning", " of", " amb", "iguous", " phrases", " or", " questions", ".", "\u23ce\u23ce", "5", ".", "", " Increased", " creativity", ":", "", " Bigger", " models", " can", " generate", " more", " creative", " and", " diverse", " responses", ",", " as", " they", " have", " learned", " a", " wider", " range", " of", " linguistic", " patterns", " and"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.82, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.82, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.78, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["\u23ce", "But", ",", " small", " schools", " aren", "'t", " only", " at", " the", " top", ",", " they", "'re", " at", " the", " bottom", ",", " too", "!", " Small", "\u23ce", " schools", " have", " higher", " variance", ".", " Small", " countries", " do", ",", " too", ".", "\u23ce\u23ce", "[", "1", "]", "\u23ce", "[", "http", "://"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.82, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["semant", "ics", ",", " leading", " to", " more", " flu", "ent", " and", " accurate", " language", " generation", ".", "\u23ce\u23ce", "3", ".", " Enhanced", " knowledge", ":", "", " Bigger", " models", " can", " store", " more", " fact", "ual", " and", " common", "-", "sense", " knowledge", " learned", " from", " their", " training", " data", ",", " which", " helps", " them"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.72, 0.0, 0.0, 0.0, 0.0, 0.28, 0.0, 0.0, 0.0, 0.0, 0.0, 0.82, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.82, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["reasonable", "\u23ce", " thing", " to", " think", " about", " doing", ",", " because", " small", " schools", " out", "per", "form", " large", " schools", ".", "\u23ce", "But", ",", " small", " schools", " aren", "'t", " only", " at", " the", " top", ",", " they", "'re", " at", " the", " bottom", ",", " too", "!", " Small", "\u23ce", " schools", " have"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.45, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.82, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["Additionally", ",", " the", " term", " \"", "tiny", "\"", " may", " be", " used", " in", " a", " figur", "ative", " sense", " to", " describe", " something", " that", " is", " small", " or", " insign", "ific", "ant", " compared", " to", " something", " else", ",", " but", " in", " the", " context", " of", " an", " elephant", ",", " it", " would", " not"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.81, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["guin", "ness", " once", " said", ",\"", " \"\"", "There", "'s", " no", " such", " thing", " as", " a", " small", " part", ".\"", " \"", "There", " are", " just", " small", " actors", ".\"", "\"", " \"", "You", "'re", " trying", " to", " school", " me", "?\"", " \"", "", "Didn", "'t", " you", " decide", " to", " take", " this"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.18, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["  ", "plut", "\u00f4", "t", "  ", "contr", "aire", ",", " ", "\u23ce", "pu", "is", "que", "  ", "plus", "  ", "il", "  ", "est", "  ", "petit", ",", "  ", "plus", "  ", "il", "  ", "est", "  ", "fac", "ile", "  ", "\u00e0", "  ", "env", "ah", "ir", ".", " ", "\u23ce", "Un"]}]}, {"quantile_name": "Subsample Interval 0", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.52, 0.0, 0.0, 0.0, 0.0, 0.0, 0.92, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["\u23ce", "As", " a", " small", " market", " purs", "uer", ",", " I", " had", " to", " say", " that", " a", " tiny", " market", " is", " different", " from", "\u23ce", " tiny", " effort", ".", "\u23ce\u23ce", "I", " developed", " tor", "app", " guill", "oche", " designer", " (<", "http", "://", "www", ".", "tor", "app", ".", "info", ">)"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.91, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["that", "'s", " full", " of", " love", " and", " warm", "th", ",", "A", " heart", " that", "'s", " always", " in", " touch", ".", "\u23ce\u23ce", "With", " a", " big", " heart", ",", " a", " heart", " that", "'s", " true", ",", "A", " heart", " that", "'s", " kind", " and", " loving", " too", ",", "A", " heart", " that"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.85, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.54, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["expertise", "._", "\u23ce\u23ce", "Its", " actually", " easier", " to", " demonstrate", " expertise", " on", " bigger", " markets", ",", " due", " to", " its", "\u23ce", " density", ".", " A", " small", " market", " can", " only", " have", " so", " many", " experts", ".", "\u23ce\u23ce", " ", "_", "In", " a", " smaller", " market", ",", " you", " can", " iterate", " faster"]}]}, {"quantile_name": "Subsample Interval 1", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.81, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["guin", "ness", " once", " said", ",\"", " \"\"", "There", "'s", " no", " such", " thing", " as", " a", " small", " part", ".\"", " \"", "There", " are", " just", " small", " actors", ".\"", "\"", " \"", "You", "'re", " trying", " to", " school", " me", "?\"", " \"", "", "Didn", "'t", " you", " decide", " to", " take", " this"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.18, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": true, "tokens": ["  ", "plut", "\u00f4", "t", "  ", "contr", "aire", ",", " ", "\u23ce", "pu", "is", "que", "  ", "plus", "  ", "il", "  ", "est", "  ", "petit", ",", "  ", "plus", "  ", "il", "  ", "est", "  ", "fac", "ile", "  ", "\u00e0", "  ", "env", "ah", "ir", ".", " ", "\u23ce", "Un"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.39, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.78, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["ers", ",", " flowers", " or", "\u23ce", " enthusi", "am", ".", "\u23ce", "The", " Small", " College", "**.", "\u23ce", "(", "The", " Standard", ".)", "\u23ce", "The", " small", " academy", " and", " the", " small", " col", "\u23ce", " l", "ege", " need", " no", " ap", "ology", ".", " Some", " of", " them", "\u23ce", " have", " as", " able"]}]}, {"quantile_name": "Subsample Interval 2", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.38, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": [".", "\u23ce\u23ce", "", "Slide", " ", "2", ":", " What", " is", " a", " large", " language", " model", "?", "\u23ce\u23ce", "*", "", " Explain", " what", " a", " large", " language", " model", " is", " and", " how", " it", " works", ".", "\u23ce\u23ce", "", "Slide", " ", "3", ":", " The", " importance", " of", " data", " c", "uration"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7, 0.36, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["ieren", " und", " sie", " in", " die", "", " Real", "it\u00e4t", " um", "z", "u", "set", "zen", ".\"", "\u23ce\u23ce", "Assistant", ":", " \"", "We", " support", " small", " and", " medium", "-", "sized", " enterprises", " in", " valid", "ating", " and", " bringing", " their", " digital", " v", "isions", " to", " life", ".\"", "\u23ce\u23ce", "Human", ":"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "ha-haiku35_resampled": true, "tokens": ["what", " an", " L", "L", "M", " (", "Large", " Language", " Model", ")", " is", " in", " one", " sentence", ",", " WITHOUT", " using", " the", " words", " \"", "large", "\",", " \"", "language", "\"", " or", " \"", "model", "\"", " in", " your", " response", ".", "\u23ce\u23ce", "Assistant", ":", " An", " L", "L", "M", " is"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["\"", "It", "'s", " such", " a", " big", " pu", "tt", "-", " such", " a", " big", ",", " big", " pu", "tt", ".\"", " \"", "I", " like", " big", " put", "ts", ",", " and", " I", " cannot", " lie", ".\"", " \"", "Money", ".\"", " \"", "You", " know", " what", " would", " be", " sad", "?\"", " \""]}]}, {"quantile_name": "Subsample Interval 3", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.56, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "ha-haiku35_resampled": true, "tokens": ["great", " options", "!", " Can", " most", " of", " these", " applies", " to", " large", " living", " rooms", " as", " well", " or", " is", " this", " more", " suited", " to", " small", " ones", "?", "\u23ce\u23ce", "Assistant", ":", " Great", " question", "!", " Most", " of", " these", " tips", " are", " actually", " versat", "ile", " and", " can", " be", " applied"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["devices", " of", " Satan", ",", " \"", " We", " are", " not", " ignor", "ant", " of", " them", ".\"", " Above", " all", ",", " ", "\u23ce", "they", " have", " little", " acqu", "aint", "ance", " with", " the", " dec", "eit", "fulness", " of", " their", " ", "\u23ce", "own", " hearts", " and", " the", " power", " of", " their", " fl"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.16, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["  ", "got", "  ", "all", " ", "\u23ce", "that", "  ", "it", "  ", "needed", ".", "  ", "Don", "'t", "  ", "call", "  ", "it", "  ", "small", " ;", "  ", "it", "  ", "is", "  ", "large", "  ", "enough", "  ", "if", "  ", "God", " ", "\u23ce", "only", "  ", "give", "  ", "it"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["know", " what", " they", " were", " walking", " into", ".\"", " \"", "Well", ",", " this", " place", " to", " me", " feels", " like", " the", " very", " definition", " of", " small", " town", " America", ".\"", " \"", "It", "'s", " on", " the", " border", " between", " Virginia", ",", " Kentucky", " and", " Tennessee", ".\"", " \"", "It", "'s", " the"]}]}, {"quantile_name": "Subsample Interval 4", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.44, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.48, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["the", " technologies", " I", " mention", " below", ".<", "p", ">", "Some", " thoughts", " on", " why", " micro", " services", " are", ":<", "p", ">", "-", "", " Micro", " services", " are", " small", " and", " can", " be", " built", " independently", ".", " So", " building", " new", " services", " should", " be", " easier", " than", " building", " or", " adding"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": [" ", "\u23ce", "5", ".", "", " Limit", " cl", "utter", ".", " Don", "'t", " try", " to", " c", "ram", " too", " much", " stuff", " into", " a", " small", " space", ".", " Consider", " purchasing", " dual", "-", "purpose", " furniture", " and", " limiting", " your", " decor", "ations", " to", " the", " ess", "entials", ".", "\u23ce", "<"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["and", " start", " taking", " action", "?", "\u23ce\u23ce", "Assistant", ":", " ", "\u23ce", "1", ".", " Set", " small", ",", " achiev", "able", " goals", ".", " Setting", " small", ",", " achiev", "able", " goals", " will", " help", " break", " down", " the", " larger", " task", " into", " manag", "eable", " segments", " and", " will", " help", " you", " to"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["with", " bias", " and", " in", "acc", "uracy", " in", " the", " model", "'s", " output", ".", "\u23ce\u23ce", "Overall", ",", " the", " paper", " argues", " that", " while", " large", " language", " models", " can", " be", " very", " useful", ",", " they", " also", " come", " with", " significant", " risks", " and", " limitations", " that", " must", " be", " carefully", " considered"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.19, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": [",", " thick", " and", " dense", " scal", "p", " hair", " in", " men", "?", "\u23ce\u23ce", "Assistant", ":", " There", " are", " no", " genes", " specifically", " for", " a", " long", ",", " thick", ",", " and", " dense", " scal", "p", " hair", " in", " men", ".", " However", ",", " genes", " do", " play", " a", " role", " in", " determining"]}]}, {"quantile_name": "Subsample Interval 5", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["pretty", " splash", " screen", " and", " l", "vm", ".", "\u23ce", "<", "G", "_", "A", "_", "C", ">", " bj", "sn", "ider", ":", " usually", " small", " enough", " to", " contain", " all", " bar", " one", " of", " your", " required", " modules", "/", "drivers", ",", " which", " you", " won", "'t", " real", "ise", " for"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["to", " learn", " the", " actual", " sentiment", " analysis", " task", " from", " such", " unh", "elp", "ful", " examples", ",", " compared", " to", " smaller", " models", ".", " The", " larger", " models", " can", " somehow", " detect", " the", " underlying", " pattern", " (", "positive", " vs", " negative", " sentiment", ")", " even", " with", " the", " conf", "using", " labels", "."]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["ematic", " that", " tools", " that", " have", " already", " been", " used", " once", " cannot", " be", " reli", "ably", " prepared", " or", " ste", "ril", "ized", " because", " their", " small", " dimensions", " and", " lum", "ens", ".", " Therefore", ",", " patients", " are", " subjected", " to", " the", " realistic", " risk", " of", " cross", "-", "contam", "ination", " if"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["?\"", " \"", "Should", " I", " bring", " your", " grand", "ma", " here", "?\"", " \"", " Dad", ".\"", " \"", " Yes", "?\"", " \"", "I", " want", " a", " small", " photograph", " of", " grand", "ma", ".\"", " \"", "We", " have", " a", " school", " project", " on", " grandmother", ".\"", " \"", "I", " have", " to", " write", " about"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["pal", "aces", " might", " be", " pomp", "ous", " and", " i", "rr", "ational", ",", " but", " plain", " houses", " still", " had", " the", " mer", "its", " of", " plain", " furniture", ".", " Indeed", ",", " whatever", " men", " made", ",", " with", "-", " out", " trying", " to", " be", " artistic", ",", " they", " made", " well", " ;"]}]}, {"quantile_name": "Subsample Interval 6", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["advice", " on", " investing", " energy", " efficiently", "?", " ", "\u23ce\u23ce", "Assistant", ":", " ", "\u23ce", "1", ".", " Start", " small", " \u2013", " begin", " by", " making", " small", " changes", " to", " daily", " habits", " to", " conserv", "e", " energy", ".", "", " Aim", " for", " small", ",", " achiev", "able", " goals", " and", " build", " from"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["by", " moving", " its", " lateral", " parts", " toward", " each", " other", ".", "<EOT>", "", "Mini", "ature", " cameras", " are", " well", " known", ".", "", " Mini", "ature", " cameras", " are", " widely", " used", " in", " contemporary", " cellular", " tele", "phones", ".", " They", " are", " also", " used", " in", " other", " personal", " electronic", " devices", ","]}, {"tokens_acts_list": [0.0, 0.31, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["eating", " large", " on", "ions", " may", " vary", " from", " person", " to", " person", ".", " Some", " potential", " negative", " effects", " that", " could", " occur", " from", " eating", " large", " on", "ions", " include", " ga", "str", "oint", "est", "inal", " dis", "com", "fort", ",", " such", " as", " n", "aus", "ea", " and", " v", "om"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.58, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["question", ",", " as", " \"", "large", "\"", " is", " a", " somewhat", " subj", "ective", " term", ".", " However", ",", " here", " are", " some", " of", " the", " largest", " known", " species", ":", "\u23ce", "-", "Atlas", "", " Moth", " -", " wingspan", " up", " to", " ", "4", " ", "feet", " ", "\u23ce", "-", ""]}]}, {"quantile_name": "Subsample Interval 7", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["ray", " that", " size", ".\"", " \"", "But", " you", " also", " have", " to", " have", " the", " action", "...", " that", " you", " would", " get", " in", " a", " big", " fight", ".\"", " \"", "As", " these", " creatures", " fall", " and", " get", " caught", " up", " in", " the", " v", "ines", "...", " naturally", " the", " v", "ines"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["observ", "ing", " the", " resulting", " response", " of", " the", " energ", "etic", " material", ".", "", " Accordingly", ",", " there", " is", " a", " need", " for", " a", " small", "-", "scale", " external", " fuel", " fire", " test", " apparatus", " for", " haz", "ard", " classification", " and", " also", " to", " probe", " the", " underlying", " physical", " response", " of"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["situation", ",", " we", " can", " still", " have", " fun", " and", " enjoy", " each", " other", "'s", " company", ".", " Who", " knows", ",", " maybe", " being", " this", " small", " will", " give", " you", " a", " new", " perspective", " on", " the", " world", " and", " help", " you", " see", " things", " you", " never", " noticed", " before", ".\"", "\u23ce\u23ce"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["the", " furniture", " might", " not", " be", " strong", " enough", " to", " hold", " it", ",", " that", " could", " also", " be", " an", " issue", ".", " For", " a", " tall", " wooden", "", " Ik", "ea", " book", "sh", "elf", ",", " it", " sounds", " like", " the", " load", " isn", "'t", " very", " heavy", ".", " For", " it"]}]}, {"quantile_name": "Subsample Interval 8", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["usually", " taken", " for", " granted", ",\"", " \"", "the", " theme", " being", ",", " \"", "it", "'s", " not", " the", " big", " events", "...", " but", " the", " little", " moments", "\"", " that", " matter", ".\"", " \"", "I", " just", " had", " a", " daughter", ".\"", " \"", "I", " do", " none", " of", " those", " things", "..."]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": [".\"", " \"", "Her", " bu", "tt", " is", " like", " a", " big", " mountain", " of", " pud", "ding", ".\"", " \"", " Listen", ",", " you", " are", " in", " big", " trouble", ".\"", " \"", " I", " am", "?\"", " \"", "You", " remember", " that", " book", " you", " wrote", "?\"", " \"", "Stan", "'s", " mom", " found", " it"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["oth", "br", "ush", " head", " located", " inside", " of", " a", " sealed", " entry", ";", " wherein", " the", " to", "oth", "br", "ush", " sanit", "izer", " is", " small", " and", " highly", " portable", ";", " wherein", " the", " to", "oth", "br", "ush", " sanit", "izer", " features", " a", " coll", "aps", "ible", " electrical", " plug", " that"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["that", " you", " can", " consider", ":", "\u23ce", "1", ".", " Consider", " purchasing", " a", " smaller", " bag", ":", " If", " your", " trophy", " is", " not", " too", " big", " for", " a", " bag", ",", " you", " may", " want", " to", " consider", " purchasing", " a", " smaller", " bag", " that", " will", " fit", " your", " trophy", ".", " This"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["conditions", " found", " in", " stir", "red", ",", " g", "assed", " systems", " with", " closed", " loop", " control", " of", " culture", " parameters", ".", " To", " date", ",", " small", "-", "scale", " experiment", " runs", " are", " generally", " carried", " out", " in", " individual", " bi", "ore", "act", "ors", ",", " of", " ", "1", " ", "to"]}]}, {"quantile_name": "Bottom Activations", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["that", " explains", " how", " things", " work", " at", " the", " tin", "iest", " levels", ",", " like", " atoms", " and", " molecules", ".", " It", " says", " that", " these", " tiny", " things", " can", " exist", " in", " many", " different", " places", " at", " once", ",", " and", " that", " they", " can", " also", " travel", " through", " time", " before", " we"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["sp", "unc", "te", " d", "ienen", " soll", "te", ",", " wie", " wenn", " die", " ", "\u23ce", "", "An", "zi", "eh", "ung", " in", " kle", "inen", "", " Dist", "an", "zen", " nur", " eine", "", " Ab", "\u00e4n", "der", "ung", " der", "sel", "-", " ", "\u23ce", "ben", " w", "\u00e4", "re"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["80", "x", "9", "c", "O", "FF", "x", "e", "2", "x", "80", "x", "9", "d", " state", ".", "\u23ce", "However", ",", " the", " small", "ness", " also", " means", " that", " the", " area", " of", " the", " overall", " light", " mod", "ulation", " (", "array", ")", " area", " occupied", " by", " the", " transist"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["\u23ce\u23ce", "Human", ":", "", " Czy", " to", " du", "\u017ce", "?", "\u23ce\u23ce", "Assistant", ":", "", " Tak", "\u23ce\u23ce", " Human", ":", "", " Czy", " to", " ma\u0142", "e", "?", "\u23ce\u23ce", "Assistant", ":", "", " Tak", "\u23ce\u23ce", " Human", ":", "", " Jak", " mo\u017ce", " by\u0107", " jed", "noc", "ze\u015b", "nie", " du", "\u017ce"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["it", "'s", "...", " it", "'s", " too", " bright", ".\"", " \"", " Too", " bright", "?\"", " \"", "", "Okay", ".\"", " \"", " It", "'s", " a", " little", " too", " bright", ".\"", " \"", " Thanks", ".\"", " \"", "Oh", ".\"", " \"", " Yeah", ".\"", " \"", "It", "'ll", " work", ".\"", " \"", "Better", "?\""]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["I", " was", " wondering", " which", " will", " be", " the", " better", " choice", " in", " terms", " of", " solid", "ifying", " my", " career", " experience", ".", " At", " the", " large", " company", " I", " see", " that", " there", " is", " brand", " recognition", ".", " The", " down", "side", " is", " that", " the", " team", " is", " mostly", " senior", " developers"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": [",", " it", " is", " known", " to", " use", " a", " spac", "er", ",", " or", " interconn", "ecting", " part", " to", " facilitate", " the", " installation", " of", " the", " small", " volume", " ink", " containers", ".", " In", " such", " systems", ",", " however", ",", " the", " user", " must", " usually", " buy", " one", " or", " more", " interconn", "ecting"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["center", "lines", ".", "", " Practical", " manufacturing", " problems", " may", " prohib", "it", " the", " construction", " of", " terminals", " which", " can", " be", " located", " on", " the", " small", " center", "line", " spac", "ings", " occupied", " by", " the", " conduct", "ors", ".", " Examples", " of", " such", " configurations", " in", " which", " the", " conduct", "ors", " must"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["ht", "", " Frid", "ay", "night", "\u23ce", " night", " th", " attendance", " was", " much", " smaller", " small", "ert", "han", " sm", "ll", "erth", "A", "n", " small", "ert", "han", "\u23ce", " than", " that", " of", " the", " seventh", " v", "enth", " regular", " meet", " meeting", " meeting", " meeting", " ", "\\xc2", "\\xac", "\u23ce", "ing"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["-", "generating", " devices", ",", " such", " as", " mobile", " phones", " and", " tablet", " computers", ".", " In", " addition", ",", " with", " the", " rapid", " growth", " of", " big", " data", " has", " come", " the", " recognition", " that", " the", " analysis", " of", " larger", " data", " sets", " can", " lead", " to", " more", " accurate", " analysis", ".", "\u23ce"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["31", " ", "are", " the", " two", " dominant", " galax", "ies", ".\"", " \"", "Most", " of", " the", " galax", "ies", " in", " our", " local", " group", " are", " small", " dw", "arf", " galax", "ies", ".\"", " \"", "These", " galax", "ies", " each", " have", " collections", " of", " small", " satellite", " galax", "ies", " that", " are", " in"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.34, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["keep", " the", " screen", "ies", " small", " or", " at", " least", " the", " file", "size", " small", "\u23ce", "<", "val", "orie", ">", " I", " like", " big", ",", " but", " this", " should", " all", " be", " view", "able", " on", " someone", "'s", " phone", "\u23ce", "<", "val", "orie", ">", " in", " case", " they", " are"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["chamber", " is", " almost", " replaced", " with", " the", " air", ",", " since", " the", " capacity", " of", " the", " first", " small", " chamber", " is", " small", ",", " a", " small", " amount", " of", " air", " exp", "ands", " due", " to", " changes", " in", " temperature", " or", " the", " like", ",", " and", " therefore", " an", " amount", " of", " ink"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["", "Div", "ya", "'s", " room", "\"", " \"", "You", " have", " to", " share", " it", " with", "", " Div", "ya", "\"", " \"", "It", "'s", " small", " but", " you", " girls", " can", " manage", "\"", " \"", "Just", " ", "1", " ", "month", ",", " during", " struggle", " period", "\"", " \"", "Ok", ","]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["a", " standard", " rail", "car", " holds", " about", " ", "13", ",", "000", " ", "pounds", " of", " goods", ".", "", " Somewhat", " surprisingly", ",", " these", " giant", " container", " ships", " are", " still", " about", " ", "7", " ", "times", " smaller", " than", " the", " size", " of", " an", " oil", " su", "pert", "an", "ker"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["uring", " some", "\u23ce", " hours", " (", "fortun", "ately", " not", " the", " entire", " flight", " time", ")", " of", " crying", " and", " st", "ares", ".", "\u23ce\u23ce", "Small", " children", " are", " a", " part", " of", " human", " life", ",", " and", " a", " no", "isy", " one", " to", " say", " the", " least", ".", " If", "\u23ce"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["be", " thr", "illing", " to", " get", " up", " close", " to", " wild", " animals", " in", " their", " natural", " habitat", ".", "\u23ce", "4", ".", " Cost", ":", " Open", " vehicles", " are", " typically", " less", " expensive", " to", " operate", " than", " closed", " vehicles", ",", " which", " can", " make", " them", " a", " more", " affordable", " option", " for"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["way", " it", " handles", " multiple", " files", " being", " opened", " at", " once", " (", "tabs", " at", " the", "\u23ce", " top", ")", " and", " some", " other", " \"", "small", "\"", " features", ".", " I", " just", " never", " have", " gotten", " used", " to", "", " Atom", " or", "\u23ce", " similar", " interfaces", ".", "\u23ce\u23ce", "I", " need"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["-", "loaded", " innings", " or", " anything", " like", " that", ".", "\u23ce", "In", " contrast", ",", " softball", " is", " a", " smaller", " game", " played", " on", " a", " smaller", " field", ",", " with", " teams", " of", " nine", " or", " ", "10", " ", "players", " each", ".", " There", " are", " also", " modifications", " you", " can", " make"]}]}], "top_logits": ["ol", "places", "prestigious", "emenea", "(", "ole", "\u00e9r\u00e9r", "and", ",", "olean"], "bottom_logits": ["peque", "\u5c0f", "pequ", "bigger", "\u0431\u043e\u043b\u044c\u0448", "larger", "small", "smaller"]}