{"index":9906857,"examples_quantiles":[{"quantile_name":"Top Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07,0.0,0.0,0.0,0.0,0.1,0.19,0.04,0.27,0.93,0.0,0.04,0.0,0.0,0.0,0.0,0.66,0.39,0.0,0.0,0.0,0.0,0.0,0.0,0.36,0.25,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[":"," Tell"," me"," a"," dirty"," lim","erick",".","\u23ce\u23ce","Assistant",":"," There"," once"," was"," a"," man"," named"," J","ack","Wh","ose"," stomach"," was"," quite"," fl","acc","ck","H","is"," tr","ous","ers"," were"," t","ight","A","nd"," his"," shirt"," was"," right"]},{"tokens_acts_list":[0.05,0.08,0.19,0.0,0.26,0.17,0.0,0.0,0.4,0.86,1.0,0.2,0.0,0.05,0.09,0.06,0.0,0.0,0.0,0.24,0.92,0.49,0.09,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.18,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["was"," a"," young"," woman"," of","\u2191"," Bant","ry"," ","\u23ce","Who"," really"," was"," quite"," a"," rap","sc","all","ion"," ","\u23ce","She"," st","ole"," a"," young"," man","'s"," hat"," ","\u23ce","And"," then"," ran"," away"," with"," that"," ","\u23ce","Which"," caused"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.49,0.77,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["love"," poem"," to"," NAME","_","1","\u23ce\u23ce","Assistant",":"," There"," once"," was"," a"," woman"," named"," C","ec","il","ia","Wh","ose"," beauty",","," they"," did"," all"," adm","ire","A"," swe","eth","eart",","," a"," kind"," heart",","," and"," a"," loving"," sm"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14,0.07,0.09,0.18,0.0,0.17,0.14,0.0,0.0,0.19,0.72,0.51,0.53,0.0,0.0,0.0,0.0,0.0,0.0,0.25,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.27,0.1,0.06,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["lim","erick"," with"," the"," correct"," rh","yme"," scheme",":","\u23ce\u23ce","There"," was"," a"," young"," girl"," from","\u2191"," N","ant","ucket","\u23ce","\u2191"," Whose"," l","ime","ricks"," made"," everyone"," sick","\u23ce"," But"," she"," said"," with"," a"," g","rin","\u23ce","\"","I","'ll"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14,0.0,0.0,0.02,0.0,0.32,0.0,0.52,0.67,0.09,0.0,0.0,0.03,0.0,0.0,0.34,0.04,0.11,0.0,0.0,0.0,0.0,0.21,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[".","\u23ce\u23ce","Here","'s"," an"," example"," of"," a"," lim","erick",":","\u23ce\u23ce","There"," once"," was"," a"," man"," from"," Peru","\u23ce"," Who","'s"," nose"," simply"," wouldn","'t"," do","\u23ce"," So"," he"," packed"," it"," in"," ice","\u23ce"," And"," placed"," it"," in"," a"," dish"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.07,0.0,0.0,0.0,0.0,0.1,0.19,0.04,0.27,0.93,0.0,0.04,0.0,0.0,0.0,0.0,0.66,0.39,0.0,0.0,0.0,0.0,0.0,0.0,0.36,0.25,0.0,0.0,0.0,0.0,0.15,0.05,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[".","\u23ce\u23ce","Assistant",":"," There"," once"," was"," a"," man"," named"," J","ack","Wh","ose"," stomach"," was"," quite"," fl","acc","ck","H","is"," tr","ous","ers"," were"," t","ight","A","nd"," his"," shirt"," was"," right","B","ut"," he"," couldn","'t"," make"," love"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.26,0.0,0.0,0.0,0.0,0.18,0.0,0.0,0.0,0.0,0.65,0.27,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.31,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["a"," lim","erick"," about"," t","oss","ing"," coins",":","\u23ce\u23ce","There"," once"," was"," a"," man"," who"," liked"," to"," t","oss","\u23ce"," A"," coin"," up"," in"," the"," air",","," without"," a"," loss","\u23ce"," He","'d"," flip"," and"," flip","\u23ce"," With"," skill"," and"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.65,0.29,0.0,0.0,0.0,0.0,0.0,0.33,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.18,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["named"," NAME","_","1"," ","with"," a"," small"," dick","\u23ce\u23ce"," Assistant",":"," There"," once"," was"," a"," man"," named"," No","ah","With"," a"," penis"," that"," was"," quite"," small","To"," see"," it"," was"," to"," laugh"," so"," loud","It"," was"," a"," sight"," that"," was"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.31,0.73,0.49,0.42,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.64,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.56,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["lion"," named"," NAME","_","1",",","\u23ce","\u2191","Whose"," m","ane"," was"," as"," fierce"," as"," his"," NAME","_","1",".","\u23ce","He"," ro","amed"," the"," sav","annah",",","\u23ce","With"," power"," and"," authority",",","\u23ce","And"," made"," all"," the"," other"," animals"]}]},{"quantile_name":"Subsample Interval 0","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07,0.0,0.0,0.0,0.0,0.1,0.19,0.04,0.27,0.93,0.0,0.04,0.0,0.0,0.0,0.0,0.66,0.39,0.0,0.0,0.0,0.0,0.0,0.0,0.36,0.25,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":[":"," Tell"," me"," a"," dirty"," lim","erick",".","\u23ce\u23ce","Assistant",":"," There"," once"," was"," a"," man"," named"," J","ack","Wh","ose"," stomach"," was"," quite"," fl","acc","ck","H","is"," tr","ous","ers"," were"," t","ight","A","nd"," his"," shirt"," was"," right"]},{"tokens_acts_list":[0.05,0.08,0.19,0.0,0.26,0.17,0.0,0.0,0.4,0.86,1.0,0.2,0.0,0.05,0.09,0.06,0.0,0.0,0.0,0.24,0.92,0.49,0.09,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.18,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["was"," a"," young"," woman"," of","\u2191"," Bant","ry"," ","\u23ce","Who"," really"," was"," quite"," a"," rap","sc","all","ion"," ","\u23ce","She"," st","ole"," a"," young"," man","'s"," hat"," ","\u23ce","And"," then"," ran"," away"," with"," that"," ","\u23ce","Which"," caused"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.49,0.77,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["love"," poem"," to"," NAME","_","1","\u23ce\u23ce","Assistant",":"," There"," once"," was"," a"," woman"," named"," C","ec","il","ia","Wh","ose"," beauty",","," they"," did"," all"," adm","ire","A"," swe","eth","eart",","," a"," kind"," heart",","," and"," a"," loving"," sm"]}]},{"quantile_name":"Subsample Interval 1","examples":[{"tokens_acts_list":[0.05,0.08,0.19,0.0,0.26,0.17,0.0,0.0,0.4,0.86,1.0,0.2,0.0,0.05,0.09,0.06,0.0,0.0,0.0,0.24,0.92,0.49,0.09,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.18,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["was"," a"," young"," woman"," of","\u2191"," Bant","ry"," ","\u23ce","Who"," really"," was"," quite"," a"," rap","sc","all","ion"," ","\u23ce","She"," st","ole"," a"," young"," man","'s"," hat"," ","\u23ce","And"," then"," ran"," away"," with"," that"," ","\u23ce","Which"," caused"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.49,0.77,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["love"," poem"," to"," NAME","_","1","\u23ce\u23ce","Assistant",":"," There"," once"," was"," a"," woman"," named"," C","ec","il","ia","Wh","ose"," beauty",","," they"," did"," all"," adm","ire","A"," swe","eth","eart",","," a"," kind"," heart",","," and"," a"," loving"," sm"]}]},{"quantile_name":"Subsample Interval 2","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08,0.0,0.0,0.02,0.0,0.12,0.0,0.09,0.54,0.72,0.0,0.0,0.0,0.0,0.0,0.0,0.48,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.14,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":[":"," Tell"," me"," a"," dirty"," lim","erick",".","\u23ce\u23ce","Assistant",":"," There"," once"," was"," a"," man"," named"," J","ack","Wh","ose"," stomach"," was"," quite"," fl","acc","ck","H","is"," tr","ous","ers"," were"," t","ight","A","nd"," his"," shirt"," was"," right"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14,0.07,0.09,0.18,0.0,0.17,0.14,0.0,0.0,0.19,0.72,0.51,0.53,0.0,0.0,0.0,0.0,0.0,0.0,0.25,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.27,0.1,0.06,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["lim","erick"," with"," the"," correct"," rh","yme"," scheme",":","\u23ce\u23ce","There"," was"," a"," young"," girl"," from","\u2191"," N","ant","ucket","\u23ce","\u2191"," Whose"," l","ime","ricks"," made"," everyone"," sick","\u23ce"," But"," she"," said"," with"," a"," g","rin","\u23ce","\"","I","'ll"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14,0.0,0.0,0.02,0.0,0.32,0.0,0.52,0.67,0.09,0.0,0.0,0.03,0.0,0.0,0.34,0.04,0.11,0.0,0.0,0.0,0.0,0.21,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":[".","\u23ce\u23ce","Here","'s"," an"," example"," of"," a"," lim","erick",":","\u23ce\u23ce","There"," once"," was"," a"," man"," from"," Peru","\u23ce"," Who","'s"," nose"," simply"," wouldn","'t"," do","\u23ce"," So"," he"," packed"," it"," in"," ice","\u23ce"," And"," placed"," it"," in"," a"," dish"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.07,0.0,0.0,0.0,0.0,0.1,0.19,0.04,0.27,0.93,0.0,0.04,0.0,0.0,0.0,0.0,0.66,0.39,0.0,0.0,0.0,0.0,0.0,0.0,0.36,0.25,0.0,0.0,0.0,0.0,0.15,0.05,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":[".","\u23ce\u23ce","Assistant",":"," There"," once"," was"," a"," man"," named"," J","ack","Wh","ose"," stomach"," was"," quite"," fl","acc","ck","H","is"," tr","ous","ers"," were"," t","ight","A","nd"," his"," shirt"," was"," right","B","ut"," he"," couldn","'t"," make"," love"]}]},{"quantile_name":"Subsample Interval 3","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.31,0.73,0.49,0.42,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.64,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.56,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["lion"," named"," NAME","_","1",",","\u23ce","\u2191","Whose"," m","ane"," was"," as"," fierce"," as"," his"," NAME","_","1",".","\u23ce","He"," ro","amed"," the"," sav","annah",",","\u23ce","With"," power"," and"," authority",",","\u23ce","And"," made"," all"," the"," other"," animals"]},{"tokens_acts_list":[0.62,0.71,0.74,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.55,0.03,0.0,0.0,0.0,0.0,0.04,0.0,0.1,0.56,0.39,0.08,0.0,0.0,0.0,0.0,0.0,0.18,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[",","\u23ce","Who"," kept"," all"," his"," cash"," in"," a"," bucket",".","\u23ce","But"," his"," daughter",","," named","\u2191"," Nan",",","\u23ce","\u2191","Ran"," away"," with"," a"," man",",","\u23ce","And"," as"," for"," the"," bucket",",","\u2191"," Nan"," took"," it","."," "]},{"tokens_acts_list":[0.42,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.64,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.56,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.11,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["whose"," m","ane"," was"," as"," fierce"," as"," his"," NAME","_","1",".","\u23ce","He"," ro","amed"," the"," sav","annah",",","\u23ce","With"," power"," and"," authority",",","\u23ce","And"," made"," all"," the"," other"," animals"," NAME","_","1",".","\u23ce\u23ce","Human",":","\u2191"]}]},{"quantile_name":"Subsample Interval 4","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.53,0.25,0.31,0.0,0.0,0.0,0.0,0.0,0.07,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":[",","2",",","5"," ","must"," rh","yme","\u23ce\u23ce"," Assistant",":"," There"," once"," was"," a"," rabbit"," named","\u2191"," H","are","\u23ce","\u2191"," Whose"," ho","pping"," was"," beyond"," compare","\u23ce"," He"," loved"," ve","gg","ies"," so"," much"," so","\u23ce"," He","'d"," stuff"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14,0.07,0.09,0.18,0.0,0.17,0.14,0.0,0.0,0.19,0.72,0.51,0.53,0.0,0.0,0.0,0.0,0.0,0.0,0.25,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.27,0.1,0.06,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["erick"," with"," the"," correct"," rh","yme"," scheme",":","\u23ce\u23ce","There"," was"," a"," young"," girl"," from","\u2191"," N","ant","ucket","\u23ce","\u2191"," Whose"," l","ime","ricks"," made"," everyone"," sick","\u23ce"," But"," she"," said"," with"," a"," g","rin","\u23ce","\"","I","'ll"," stop"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.66,0.35,0.37,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.29,0.11,0.0,0.0,0.0,0.0,0.0,0.0,0.12,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["once"," was"," a"," spider"," named"," NAME","_","2","\u23ce","\u2191","Whose"," el","bow"," was"," injured"," in"," a"," rough"," h","uff","\u23ce"," He"," went"," to"," the"," doc","\u23ce"," For"," some"," medication"," and"," a"," quick"," fix","\u23ce"," But"," a","las",","," his"," el"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.48,0.49,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["in"," a"," butterfly"," in"," the"," poem","\u23ce\u23ce"," Assistant",":"," There"," once"," was"," a"," woman"," named"," C","ec","il","ia","Wh","ose"," beauty",","," they"," did"," all"," adm","ire","A"," swe","eth","eart",","," a"," kind"," heart",","," and"," a"," loving"," sm"]}]},{"quantile_name":"Subsample Interval 5","examples":[{"tokens_acts_list":[0.14,0.0,0.0,0.12,0.0,0.32,0.16,0.25,0.0,0.0,0.0,0.0,0.32,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.36,0.08,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["a"," bat","sman"," so"," fine","\u23ce","\u2191"," Whose"," batting"," was"," simply"," sublime","\u23ce"," He"," hit"," the"," ball"," to"," the"," boundary","\u23ce"," With"," such"," great"," precision"," and"," d","ex","te","rity","\u23ce"," That"," the"," crowd"," stood"," and"," che","ered"," in"," un","ison"]},{"tokens_acts_list":[1.0,0.2,0.0,0.05,0.09,0.06,0.0,0.0,0.0,0.24,0.92,0.49,0.09,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.18,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.49,0.1],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["Who"," really"," was"," quite"," a"," rap","sc","all","ion"," ","\u23ce","She"," st","ole"," a"," young"," man","'s"," hat"," ","\u23ce","And"," then"," ran"," away"," with"," that"," ","\u23ce","Which"," caused"," the"," young"," man"," a"," lot"," of"," b","other","\u23ce","<"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["all"," caps","?"," ","\u23ce","Little"," Miss","\u2191"," M","uff","et","\u23ce","\u2191"," Sat"," on"," a"," t","uff","et",",","\u23ce","\u2191","Eating"," her"," cur","ds"," and"," wh","ey",";","\u23ce","Along"," came"," a"," spider",",","\u23ce","Who"," sat"," down"," beside"]}]},{"quantile_name":"Subsample Interval 6","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.31,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["and"," vocabulary"," is"," obsol","ete","\u23ce"," And"," the"," only"," word"," I"," need"," to"," know"," is"," \"","sch","la","fen","\"","\u23ce","For"," the"," chance"," to"," rest"," and"," let"," my"," mind"," ro","am","\u23ce\u23ce"," But"," a","las",","," I"," must"," stay"," aw"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["a"," poem"," about"," ve","gem","ite","\u23ce\u23ce"," Assistant",":","\u2191"," Ve","gem","ite",","," oh","\u2191"," Ve","gem","ite",",","\u23ce","You","'re"," a"," spread"," like"," no"," other",",","\u23ce","You","'re"," s","alty"," and"," sav","ory",",","\u23ce","And"," you"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.09,0.0,0.0,0.0,0.0,0.0,0.24,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[":","\u23ce","NAME","_","1"," ","was"," feeling"," quite"," gl","um","\u23ce"," His"," dinner"," options"," were"," all"," sh","un","ned","\u23ce"," But"," then"," he"," thought"," of"," a"," plan","\u23ce"," To"," make"," a"," meal"," that"," would"," not"," d","isd","ain","\u23ce\u23ce"," He"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[".","\u23ce\u23ce","Assistant",":","\u2191"," J","air","\u2191"," Bol","son","aro",","," oh","\u2191"," J","air","\u2191"," Bol","son","aro","\u23ce"," A"," ca","uda"," do"," pend","ulo"," que"," b","ate","\u23ce"," Uma"," vez"," ele"," foi"," um"," ali","ado","\u23ce","\u2191"," Ag"]}]},{"quantile_name":"Subsample Interval 7","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["poem"," about"," life"," and"," death"," during"," the"," American"," Civil"," War",":","\u23ce\u23ce","The"," war"," between"," the"," North"," and"," South",",","\u23ce","A"," test"," of"," might"," and"," core",",","\u23ce","A"," struggle"," for"," freedom","'s"," sake",",","\u23ce","And"," the"," end"," of"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.18,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["'re","\u2191"," Anim","ani","acs"," *","\""," \"","*"," Meet"," Ralph"," And"," Dr",".","\u2191"," Scr","atch","an","sn","iff"," *","\""," \"","*"," Say"," hi"," to"," Hello","\u2191"," Nurse"," *","\""," \"","*","\u2191"," Good","fe","athers"," f","lock"," together"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["so"," red",";"," And"," his"," nose"," is"," like"," a"," berry",","," And"," he","'s"," as"," b","ald"," as"," Uncle"," Jerry"," On"," the"," head","."," Why",","," he"," isn","'t"," worth"," a"," dollar","!"," All"," he"," does"," is"," cry"," and"," ho","ller"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["tear"," it"," on"," rejection",".","\u23ce","Next"," Sunday"," m","orn"," within"," a"," p","ew","\u23ce"," A"," little"," saint"," saw"," At","\u23ce"," To"," add"," unto"," the"," lit","any","\u23ce"," One"," prayer"," not"," found"," in"," it",".","\u23ce","No"," lily"," in"," the"," chan"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["And"," al","-","o"," shoe"," my"," family",".","\u23ce","So"," Bill"," you"," go"," and"," har","ness","\u2191"," D","oll",",","\u23ce","And"," straight"," to"," town"," w","'ll"," go",","," by"," g","ol",".","\u23ce","And"," stop"," in"," front"," of","\u2191"," K"]}]},{"quantile_name":"Subsample Interval 8","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["a"," long"," winter","'s"," n","ap",",","\u23ce","When"," out"," on"," the"," roof"," there"," arose"," such"," a"," cl","atter",",","\u23ce","I"," sp","rang"," from"," the"," bed"," to"," see"," what"," was"," the"," matter",".","\u23ce","Away"," to"," the"," window"," I"," flew"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["women"," prize"," today"," are"," just"," silly"," gi","go","los","\""," \"","So"," though"," I","'m"," not"," a"," great"," rom","ancer","\""," \"","I"," know"," that"," you","'re"," bound"," to"," answer"," when"," I"," propose"," anything"," goes","\""," \"","The"," world"," has"," gone"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.17,0.0,0.0,0.06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15,0.0,0.0,0.09,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["Is"," loop","-","free"," connectivity",".","\u23ce","    ","\u23ce","      ","A"," tree"," which"," must"," be"," sure"," to"," span"," ","\u23ce","      ","So"," packets"," can"," reach"," every"," L","AN",".","\u23ce","    ","\u23ce","      ","First"," the"," Root"," must"," be"," selected"," ","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["end",","," it","'s"," worth"," it"," all",".","\u23ce\u23ce","Love"," is"," a"," gentle"," bre","eze"," that"," so","oth","es",",","\u23ce","A"," warm"," embrace"," that"," never"," ce","ases",",","\u23ce","It","'s"," the"," light"," in"," the"," darkness"," that"," guides",",","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ado"," e"," rim","ado",",","\u23ce","\u00c9"," dif","\u00edc","il"," es","cr","ever"," um"," po","ema"," of","ens","ivo",",","\u23ce","\u2191","Mas"," eu"," v","ou"," ten","tar",","," n\u00e3o"," se"," pre","oc","upe",",","\u23ce","\u2191","Voc\u00ea"," vai"," ador","ar"]}]},{"quantile_name":"Bottom Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.09,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["sto","\u2191"," C","ajo",","," ","\u23ce\u23ce","Se"," ved","este","!"," dal","\u2191"," C","anto"," al","\u2191"," Bram","ante"," ","\u23ce","Non"," qi"," si"," passa",","," gli"," \u00e8"," jun"," ;"," for","mi","col","alo",","," \u00bb"," ","\u23ce","\u2191","T","ante"]},{"tokens_acts_list":[0.02,0.0,0.27,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[","," ","\u23ce","Could","  ","go","  ","into","  ","Hyde","-","Park","  ","out","  ","a","  ","walk","-"," ","\u23ce","ing",","," ","\u23ce","And","  ","there"," \u2014"," ","\u23ce\u23ce\u23ce","\u21ea","COMIC","    ","\u21ea","SONGS","."," ","\u23ce\u23ce\u23ce","199"," "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["sp","ies"," from"," China","\""," \"","Try"," to"," steal"," your"," mind","'s"," el","ation","\""," \"","Little"," girls"," from"," Sweden","\""," \"","Dream"," of"," silver"," screen"," quot","ations","\""," \"","And"," if"," you"," want"," these"," kind"," of"," dreams","\""," \"","It"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["os"," Angeles"," Times","\u23ce"," Love"," Makes"," Spring",".","\u23ce","Do"," I"," pray"," the"," w","int","ry"," sk","iv","s",".","\u23ce","l",".","et"," the","."," spring","time"," sweet"," arise",".","\u23ce","\u2191","Lot"," these"," we","ary"," was","tes"," of"," sn"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u0444\u043e\u0440","\u043c\u0435",".","\u23ce\u23ce","\u2191","\u041e\u043d\u0430"," \u0441","\u044f\u0434","\u044c"," \u043d\u0430"," \u043a\u043e\u043b","\u0435\u043d\u0438"," \u0441\u0432\u043e","\u0435\u0439"," \u0445","\u043e\u0437","\u044f","\u0439","\u043a\u0438",",","\u23ce","\u0418"," \u0441\u0442\u0440\u0430\u043d","\u043d\u043e",","," \u043d\u043e"," \u044d\u0442\u043e"," \u0434\u0435\u0439\u0441\u0442\u0432","\u0438\u0442\u0435\u043b\u044c","\u043d\u043e"," \u043a\u0440\u0430\u0441","\u0438","\u0432\u043e",".","\u23ce","\u2191","\u041e\u043d\u0430"," \u0441","\u044f\u0434","\u044c"," \u0438"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["kitchen"," with","\u2191"," Din","ah","..."," *","\""," \"","*","\u2191"," Str","um","min","'"," on"," the"," old"," ban","jo"," *","\""," \"","*"," Fee",","," fi",","," fid","dle","-","ee","-","i","-","o"," *","\""," \"","*"," Fee"]},{"tokens_acts_list":[0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[" ","\u23ce\u23ce","\"","Her","  ","regards",",\"","  ","said","  ","Augustus",",","  ","\"","I","'ll","  ","win",",\""," ","\u23ce\u23ce","As","  ","he","  ","pon","dered","  ","in","  ","d","sj","ep","  ","rev","erie"," \u2014"," ","\u23ce","\"","  "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["too"," will"," pass",",\""," says"," a"," butterfly",".","\u23ce","\"","I"," have"," a"," year","ning"," for"," life"," and"," a"," calling","\u23ce"," To"," be"," about"," my"," friend",".\"","\u23ce","\"","Like"," a"," lover"," I"," so"," cher","ish",",\"","\u23ce","\u2191","Gives"," your"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u2191","Fit","2","-.","4","i",":","gust","us",",","  ","you","'ve","  ","dn","m","k","  ","!"," ","\u23ce","y","oo","  ","are","  ","in","so","L","iat",",","  ","sir",",","  ","in","  ","that","  ","p","li"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["dum","pty"," had"," a"," great"," fall",".","\u23ce","All"," the"," king","'s"," horses"," and"," all"," the"," king","'s"," men","\u23ce","\u2191"," Couldn","'t"," put","\u2191"," Hum","pty","\u2191"," Dum","pty"," together"," again",".","\u23ce","<","|","stop","|",">","<EOT>","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["carried","  ","the","  ","Jack","-","ass",",","  ","who"," ","\u23ce\u23ce","never","  ","said","  ","n","ay",","," ","\u23ce\u23ce","\u2191","T","rot","ting","  ","along","  ","the","  ","road",","," ","\u23ce","But","  ","all","  ","changes","  ","end"]},{"tokens_acts_list":[0.11,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\""," \"","\u2191","Aren","'t"," we"," all","?\""," \"","For"," he","'s"," a"," jol","ly"," good"," Once","-","","ler","\""," \"","\u2191","Aren","'t"," we"," all","?\""," \"","Now"," all"," that"," was"," left"," beneath"," the"," bad","-","sm","elling"," sky"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["sp","it","tin","',"," toilet","-","tra","inin","'"," changed"," our"," song","\""," \"","We","'re"," situated"," bac","hel","ors",","," fathers","-","in","-","wait","in","'\""," \"","Rather"," hang"," with"," you"," than"," the"," one"," he","'s"," been"," dating","\""]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["and"," the","\u2191"," F","anta"," ","\\xe2\\x99","\\xaa","\""," \"","\\xe2\\x99","\\xaa",""," Credit"," cards"," and"," the"," sc","amm","ers"," ","\\xe2\\x99","\\xaa","\""," \"","\\xe2\\x99","\\xaa","","\u2191"," Hit","tin","'"," off"," l","icks"," in"," the"," b","ando"," ","\\xe2\\x99"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[","," and"," ar","rog","ance",".\""," \"","To"," the"," right"," a"," m","ink",","," to"," the"," left"," a"," m","ink",","," and"," the"," freedom"," bell"," in"," the"," heart",".\""," \"","And"," (","?\""," ","\")"," social"," is"," what"," we"," want",","]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.08,0.0,0.0,0.0,0.04,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["mystery"," masked"," man"," was"," smart"," ","\\xe2\\x99","\\xaa","\""," \"","\\xe2\\x99","\\xaa",""," He"," got"," himself"," a","\u2191"," T","onto"," ","\\xe2\\x99","\\xaa","\""," \"","\\xe2\\x99","\\xaa",""," ","'","\u2191","Cause","\u2191"," T","onto"," did"," the"," dirty"," work"," For"," free"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ours"," was"," started"," because"," of"," a"," mis","under","standing"," with"," a"," super","-","pretty"," ghost"," named","\u2191"," Ma","vis",","," who"," turned"," out"," to"," be"," real"," nice"," in"," the"," end",".\""," \"","All"," due"," respect",","," private",","," but"," that","'s"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["the"," belly"," dance","\u23ce\u23ce","[","\u2191","Chorus","]","\u23ce","Rock"," the"," ca","sb","ah",","," rock"," the"," ca","sb","ah","\u23ce"," Rock"," the"," ca","sb","ah",","," let"," the"," belly"," dance","\u23ce"," Rock"," the"," ca","sb","ah",","," rock"," the"," ca"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["  ","etc","."," ","\u23ce\u23ce","En","  ","au","mu","sse","  ","un","  ","je","une","  ","j","\u00e9s","uite"," ","\u23ce\u23ce","\u2191","All","ait","  ","dev","ant","  ",";"," ","\u23ce","\u2191","Gra","vement","  ","march","ait","  ","\u00e0","  ","sa"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["arriving"," ","\u23ce\u23ce","Assistant",":"," NAME","_","1",":","\u23ce","I"," was"," all"," set"," to"," plant"," my"," seed"," pot","atoes","\u23ce"," But"," the"," mail","man"," never"," brought"," them"," to"," me","\u23ce"," I"," checked"," the"," calendar",","," it"," said"," they","'d"," be"]}]}],"top_logits":["Who","His","Named","Found","Known","Including","Got","Used","commercially"],"bottom_logits":["munt","quals","limit","ywhere","sep","seva","secrets","fit","presence","pivot"]}