{"index":28319589,"examples_quantiles":[{"quantile_name":"Top Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u2191","Suz","anne"," Collins","\u23ce","3","."," The"," Girl"," With"," All"," the","\u2191"," Gifts"," by"," M",".","R",".","\u2191"," Carey","\u23ce","4","."," The"," Stand"," by"," Stephen"," King","\u23ce","5","."," Swan"," Song"," by"," Robert"," McC","am","mon","\u23ce","6"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.97,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["s","9","sw","rr","m","\u23ce"," g","hu"," rc","g","on","\u2191"," Sep","ubl","ic","ir","n",".","\u23ce","\u21ea","DALLAS",","," \"","\u21ea","SATURDAY",",","\u21ea"," SEPT","."," ","10","\u23ce","K","li","C","O","U","M","T","IO","N"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.97,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["the"," A","1"," ","all","ele"," and"," the"," mut","ated"," sequence"," as"," the"," A","2"," ","all","ele"," (","\u2191","Carey",","," ","1","994",")."," It"," was"," suggested"," that"," the"," additional"," promot","er"," site"," influences"," promot","er"," activity",","," thereby"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.96,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["and"," spring"," loaded"," l","ocking"," ins","erts",".","\u23ce","Storm"," resistant"," fixed"," sh","utter"," assembl","ies"," are"," taught"," by","\u2191"," Carey"," U",".","S","."," Pat","."," No","."," ","6",",","886",",","294"," ","and","\u2191"," Whit","worth",","]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.95,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ulations"," are"," discussed"," in"," standard"," texts"," such"," as"," March",","," Advanced","\u2191"," Organic"," Chemistry"," (","\u2191","Wi","ley","),","\u2191"," Carey"," and","\u2191"," Sund","berg",","," Advanced","\u2191"," Organic"," Chemistry"," (","2"," ","vol",".)"," and","\u2191"," T","rost"," and"," Fleming"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.95,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["development",","," and"," an"," improvement"," of"," resid","ual"," film"," ratio"," (","supp","ression"," of"," a"," film"," reduction",").","<EOT>","\u2191","Deng","ue"," vir","uses"," (","\u21ea","D","ENV",")"," are"," human"," pat","ho","gens"," with"," a"," significant"," threat"," to"," world"," health"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.95,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["bo","yle"," W"," ","Bar","ne","a","W","."," Cooper"," C"," C"," A"," Co","\u2191"," Co","yle"," Dr"," A","\u2191"," Carey"," Rev"," B"," V","\u2191"," Ca","ver","line"," C"," H","\u2191"," Cau","ip","fi","old"," DJ","\u2191"," Cl","over"," F"," A"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.95,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ere"," with"," your"," recovery",".","\u23ce","8","."," Follow"," your"," doctor","'s"," instructions"," on","\\xe6\\x88","\\x92","\\xe9\\x85","\\x92","\uff1a","\u2191"," Deng","ue"," fever"," is"," caused"," by"," a"," virus",","," and","\\xe9\\xa5","\\xae","\\xe9\\x85","\\x92"," ","can"," increase"," your"," risk"," of"," developing"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.95,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u2191","Mo","ines","."," That","'s"," the"," Governor","'s"," mansion",".","\u2191"," Cust","od","ian"," C","."," E",".","\u2191"," Carey"," com"," pl","ained"," to"," police"," about"," it",","," and"," said"," that"," in"," about"," a"," week"," he"," had"," removed"," several"," such"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.95,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u2191","Paragraph",":"," \""," There"," ","'s"," Got"," to"," Be"," a"," Way"," \""," was"," written"," by","\u2191"," Mar","iah","\u2191"," Carey"," and","\u2191"," ","Ric"," Wake"," for","\u2191"," Carey"," ","'s"," self"," @","-","@"," titled"," debut"," studio"," album"," ("," "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.95,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["and"," performance"," level"," fell"," slightly","."," Among"," children"," whose"," families"," declined"," intervention",","," consistency"," fell"," mark","edly",".","<EOT>","\u2191","Deng","ue"," virus"," (","\u21ea","D","ENV",")"," infection"," has"," reached"," epidemic"," proport","ions"," in"," many"," countries"," in"," South"," America"," and"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.95,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.88,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["in"," such"," a"," way"," that"," a"," good"," contact"," is"," achieved"," at"," the"," proper"," place"," under"," all"," circumstances",".","<EOT>","\u2191","Deng","ue","\u2191"," Fever"," (","","DF",")"," and","\u2191"," Deng","ue","\u2191"," Hemorrh","ag","ic","\u2191"," Fever"," (","DH","F"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.95,0.0,0.0,0.0,0.0,0.0,0.91,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","for"," the"," presidency"," In"," ","1","912",".","\u23ce","Former"," United"," State"," Senator"," J",".","\u23ce","M",".","\u2191"," Carey",","," father"," of"," the","\u2191"," Carey"," land","\u23ce"," act",","," has"," form","ely"," announced"," his"," can","\u23ce"," did","acy"," for"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.95,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["c","lam","ped"," ther","ein",","," with"," regard"," to",","," for"," instance",","," a"," machine"," tool",".","<EOT>","The","\u2191"," Deng","ue"," virus"," (","D","EN","),"," is"," a"," co","ated"," virus"," whose"," lip","id"," membrane"," contains"," two"," of"," its"," three"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.94,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ben","nie",","," AT"," H",".","\u2191"," B","owers",","," A","."," E","."," Bryant",",","\u23ce","C",".","\u2191"," Carey",","," H","."," R",".","\u2191"," C","oll",","," E","."," O",".","\u2191"," Di","mond",",","\u23ce","E","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.94,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[","," Stadt",","," ob","ere","\u2191"," Br","eu","ner","stra","\u00df","\u00ab"," Nr","."," lt","44","-","<EOT>","THE","\u21ea"," DALLAS","\u23ce","\u21ea"," DAILY","\u23ce","\u21ea"," H","EK","ALD","\u23ce"," ","VC"," l"," I"," It"," T"," H"," B","\u23ce"," v","ii"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.94,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.91,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["the"," writers"," took"," this"," route",".","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","Search"," for","\u2191"," Mar","iah","\u2191"," Carey"," best"," songs","\u23ce\u23ce"," Assistant",":"," Here"," are"," some"," of","\u2191"," Mar","iah","\u2191"," Carey","'s"," most"," popular"," and"," critically"," acclaimed"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.94,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[" ","'s"," Got"," to"," Be"," a"," Way"," \""," is"," a"," song"," by"," American"," singer"," and"," songwriter","\u2191"," Mar","iah","\u2191"," Carey"," from"," her"," self"," @","-","@"," titled"," debut"," studio"," album"," ("," ","1","990"," ",")"," Columbia"," released"," it"," as"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.94,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u2191","D","rs","."," R","."," G","."," McL","ane",","," cf"," Jackson",","," and"," H","."," G",".","\u2191"," Carey",","," of","\u2191"," Da","yton","."," Committee"," on","\u2191"," Adm","issions"," \u2014","\u2191"," D","rs","."," St","."," John",","]}]},{"quantile_name":"Subsample Interval 0","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.91,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u00bb","I"," an"," act"," ol"," the","\u23ce"," the","\u2191"," St","uti"," ol"," bl","il","lio",",","\u23ce","the"," \"","\u2191","Carey","\u23ce"," legislature"," ol","\u23ce"," commonly"," known"," as","\u23ce","\u2191"," ","Vet","\""," approved"," March"," ","2","."," I","8","P"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.95,0.0,0.0,0.0,0.0,0.0,0.91,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["1","912",".","\u23ce","Former"," United"," State"," Senator"," J",".","\u23ce","M",".","\u2191"," Carey",","," father"," of"," the","\u2191"," Carey"," land","\u23ce"," act",","," has"," form","ely"," announced"," his"," can","\u23ce"," did","acy"," for"," the"," republican"," nom","ina","\u23ce"," "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce\u23ce\u23ce","tr","isis"," ","1","a"," ","\u23ce\u23ce\u23ce","any"," wh","ales"," o","F"," Ole","\u2191"," Bul","bal"," ","\u23ce\u23ce\u23ce","\u21ea","DALLAS"," AND","\u21ea"," MONROE"," ED"," ","\u23ce","\u21ea","","WARDS"," ","\u23ce","New"," ","\u23ce\u23ce\u23ce","\u2191","","Ene"," t"," ll","ow"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.95,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["to"," Be"," a"," Way"," \""," was"," written"," by","\u2191"," Mar","iah","\u2191"," Carey"," and","\u2191"," ","Ric"," Wake"," for","\u2191"," Carey"," ","'s"," self"," @","-","@"," titled"," debut"," studio"," album"," ("," ","1","990"," ",")"," It"," was"," written"," during"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.89,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":[" ","200"," ","words"," for"," Golden"," Eagle"," Products"," ,"," a"," chemical"," company"," in"," ","218"," ","Toledo"," St","  ","\u2191","Carey",","," Ohio"," ","43","316"," ","United"," States","\u23ce\u23ce"," Assistant",":"," Here","'s"," an"," introduction"," for"," Golden"," Eagle"," Products",":"]}]},{"quantile_name":"Subsample Interval 1","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["?\""," \"","But"," that","'s"," abs","urd",".\""," \"","Is"," it","?\""," \"","Now",","," listen",","," Mr",".","\u2191"," Carey",".\""," \"","For"," some"," time",","," an"," organization"," has"," been"," trying"," to"," steal"," or"," at"," least"," copy"," the"," plans"," of"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["J",".,"," Warren"," R","."," L",".,","\u2191"," H","oke"," C","."," H",".","\u2191"," Mice"," immun","ized"," with"," a"," deng","ue"," type"," ","2"," ","virus"," E"," and"," NS","1"," ","fusion"," protein"," made"," in","\u2191"," Esch","eri","chia"," c"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["a"," list"," of"," flights"," to"," Paris"," from"," the"," US","?","\u23ce\u23ce","Assistant",":"," ","\u23ce","1","."," American"," Airlines",":"," Dallas","/","\u2191","Ft"," Worth"," to"," Paris"," Charles"," de","\u2191"," Gau","lle","\u23ce","2","."," Delta"," Airlines",":"," Atlanta"," to"," Paris"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["soccer"," games"," today","."," One"," game"," is"," Atlanta"," United"," vs","."," Portland","\u2191"," Tim","bers","."," Another"," game"," is"," FC"," Dallas"," vs","."," Minnesota"," United",".","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","What"," person"," historically"," contributed"," the"," most"," to"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["'","avg","'"," broj"," -"," ","'","a"," lot","'"," .."," ","\u23ce","<","j","elly",">"," l","ee","loo"," dallas"," multi","pass","?","\u23ce","<","B","ot","ani","C","ar","|","2",">"," say"," again"," ?","\u23ce","<","B","ot"]}]},{"quantile_name":"Subsample Interval 2","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.62,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["an"," article"," I"," could"," suggest"," you"," read"," that"," might"," help",","," it","'s"," from"," Trip","\u2191"," Advisor",":"," ","\u23ce","Dallas",":"," A","\u2191"," Visitor","'s"," Guide"," to"," Dallas","\u23ce"," Dallas","\u2191"," Visitor"," Guide"," for"," Hotels",","," Shopping",","," Entertainment",","]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u21ea","EXC","LA","IMS",")\""," \"","The"," primary"," cooling"," system"," is"," over","he","ating",".\""," \"","Design",".\""," \"","\u21ea","DIANE",":\""," \"","Installation",".\""," \"","What"," about"," the"," emergency"," di","ver","ter"," you"," insisted"," we"," needed","?\""," \"","It","'s"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["new"," ways"," to"," suppress"," viral"," re","plication"," and"," prevent"," persistent"," infection","."," Multiple"," pept","ides"," from"," conserv","ed"," regions"," of"," deng","ue"," virus"," may"," prove"," essential"," in"," the"," development"," of"," a"," univers","ally"," immun","ogenic"," vaccine","."," In"," recent"," years",","]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["withdrawal",",","\u2191"," Js","aid","."," sw","ivel"," bet","we","en","the"," ret","aining"," head","sb","eing"," of"," slightly"," greater"," deng","thth","an"," the"," combined"," thickness"," of","-","the"," two"," cross"," heads",".","\u23ce","5",".","\u2191"," I","na","an","ine"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["Tribune"," ]","\u23ce","Camp"," Chicago","\u2191"," Batt","b","bt"," B",","," (","\u2191","T","atl","ob","'s","),"," #","\u23ce","Dallas",",","\u2191"," Gs",".,"," May"," SO","."," j","\u23ce"," We"," have"," just"," closed"," the"," ","6","th"," day"," of"," battle"]}]},{"quantile_name":"Subsample Interval 3","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["from","\u2191"," Kap","ama"," River"," Lodge"," to"," Cape"," Town"," on"," April"," ","18","\u23ce","-","Flight"," from"," Cape"," Town"," to"," Dallas"," on"," April"," ","25","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","How"," can"," I"," get"," the"," most"," out"," of"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.61,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["erick",">"," i"," did"," and"," loaded"," it"," again"," ...","but"," nothing","\u23ce","<","diane",">"," Works"," here","...","\u23ce","<","diane",">"," I"," just"," did"," it"," back"," and"," forth",".","\u23ce","<","do","tz",">"," The"," package"," ","'","k","cont"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["is","-"," ","\u23ce","t","rate"," and","  ","\u2191","Dep",".","  ","\u2191","Lie","ut",",","  ","of","  ","\u2191","Middles","ex","  ","and","  ","\u2191","West","min","-"," ","\u23ce","","ster",","," and","  ","a","  ","\u2191","Magist","rate"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["without"," a"," car","."," They","'re"," offering"," free"," rides"," from"," the"," airport"," and"," hotel"," pickup"," services",","," too","."," And"," Dallas"," is"," also"," connected"," to"," other"," cities"," in"," Texas",","," including"," Houston",","," San"," Antonio",","," Austin",","," and"," Fort"," Worth"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ten"," mir"," die"," P","."," P",".","\u2191"," Pri","orn",",","\u2191"," G","uar","\\xe2","\\xb8","\\x97",""," ","\u23ce","diane"," und","\u2191"," Provin","z","iale"," der"," g","ei","\u017f","t","lichen","\u2191"," Orden"," ","\u017f","ow","ohl"," ","\u23ce","als"]}]},{"quantile_name":"Subsample Interval 4","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["un","  ","grand","  ","air","  ","de","  ","dign","it\u00e9",",","  ","comme","  ","e","\u00fbt","  ","fait","  ","\u2191","Diane"," ","\u23ce","de","  ","\u2191","Po","it","iers","  ","d","ans","ant","  ","la","  ","\u2191","Roman","esca",";","  "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[".","\u2191"," ","Ewing"," we"," have"," testimony"," that"," your"," brother"," was"," romant","ically"," linked"," with"," Miss"," Grey"," prior"," to"," leaving"," Dallas"," months"," ago",".\""," \"","Did"," you"," know","?\""," \"","\u2191","Obj","ection",","," Your"," Honor","!\""," \"","No",","," Mr"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ser","aient"," jam","ais","\u2191"," J","eve"," ","nus"," g","-","","ands",".","\u2191"," Com","t","esse"," de","\u2191"," Diane",".","//"," v","aud","rait"," m","ieux"," te"," comp","ter"," qu","'","un"," million"," d","'","hom","mes"," h","eur"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["total","it\u00e9",",","  ","soit","  ","en","  ","partie",",","  ","sur","  ","la","  ","ligne","  ","m","\u00e9","\u23ce"," diane"," du","  ","corps",".","  ","\u2191","Enf","in",",","  ","ces","  ","diff\u00e9r","ents","  ","syst","\u00e8","mes","  ","de"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["uh",","," I"," have"," to"," go",".\""," \"","But",","," yes",",\""," \"","I","'ll"," let"," you"," know"," if","\u2191"," Diane"," approaches"," me",".\""," \"","\u2191","Eli",".\""," \"","Don","'t"," tell"," me"," you"," made"," a"," deal",".\""," \"","\u2191","Al"]}]},{"quantile_name":"Subsample Interval 5","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.41,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["assassination",","," determined"," to"," find"," a"," way"," to"," fix"," the"," mistake"," they"," had"," made","."," But"," as"," they"," arrived"," in"," Dallas",","," they"," realized"," that"," something"," had"," gone"," wrong","."," The"," bullet"," that"," had"," hit"," NAME","_","3"," ","was"," gone"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.41,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["is","que"," ch","aque"," jour"," les"," y","eux",","," le"," lan","gage"," et"," les"," ca","resses"," de"," ","\u23ce","\u2191","Diane"," lui"," pr","ou","va","ient","l","'","exc","\u00e8s"," de"," l","'","am","our"," qu","'","il"," av","ait"," su"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["iser","."," ","\u23ce","\u2014"," De","\u2191"," Vern","eu","il",","," ne"," me"," press","ez"," pas"," ainsi",","," fit","\u2191"," Diane"," \u00e9m","ue",","," en"," ess","ay","ant"," de"," se"," sou","str","aire"," aux"," \u00e9t","re","intes"," de"," l","'","am"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.38,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u2191","D","isant"," ainsi"," avec"," am","our"," et"," passion",",","\u2191"," Math","us"," tomb","\u00e9"," aux"," gen","oux"," de","\u2191"," Diane",","," co","uv","rait"," ses"," m","ains"," de"," b","ais","ers",","," puis"," sa"," t","\u00e8te"," ch","arm","ante",","]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.38,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["il"," se","rait"," forc","\u00e9"," de"," d\u00e9","ch","irer"," le"," vo","ile"," qui"," le"," cach","ait"," aux"," regards"," de","\u2191"," Diane","."," Plus"," encore"," jal","oux"," de"," son"," prop","re"," bon","h","eur",",","\u2191"," Math","us"," emp","ress","\u00e9"," d"]}]},{"quantile_name":"Subsample Interval 6","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.31,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u2191","Zomb","i","eball","\u23ce"," My"," favourite"," is"," still","\u2191"," Intelligent"," Design"," Sort",":","\u23ce\u23ce","[","http","://","www",".","dang","erm","ouse",".","net","/","es","ot","eric","/","intellig","ent","des","ig","ns","ort",".","","ht","...","]("]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["590","](","http","://","stackoverflow",".","com","/","a","/","2","009","546","/","489","590",")","\u23ce\u23ce","~~~","\u23ce","dang","erl","ib","rary","\u23ce"," Yes","!"," Although"," ...","\u23ce\u23ce","\"","Wait"," a"," minute","."," Strike"," that",".","\u2191"," Reverse"," it"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["Thanks","\u23ce\u23ce","------","\u23ce","miz","aru","\u23ce"," Real"," authors"," use"," Word","S","tar"," ","4",".","0","\u23ce\u23ce","~~~","\u23ce","dang","oor","\u23ce"," and"," then"," can","'t"," keep"," up"," with"," the"," TV"," series"," that"," is"," started"," years"," after"," the"," book","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["And"," that","'s"," covered"," in"," the"," API",".","\u23ce\u23ce","<EOT>","\u23ce\u23ce","Core","\u2191"," Concepts"," of"," Django"," Model","F","orms"," -"," dang","ol","din","\u23ce"," http","://","p","yd","anny",".","com","/","core","-","concepts","-","django","-","model","forms","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["in"," commercial"," real","\u23ce"," estate"," that"," may"," finally"," bring"," residential"," real"," estate"," back"," down"," to"," Earth",".","\u23ce\u23ce","------","\u23ce","dang","us","\u23ce"," I"," read"," the"," whole"," article",".","\u23ce\u23ce","NYC"," nor"," any"," other"," major"," city"," will"," die"," from"," this","."]}]},{"quantile_name":"Subsample Interval 7","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.21,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["politics"," doesn","'t"," play"," to"," our"," streng","ths",".","\u23ce","It","'s"," very"," il","log","ical",".","\u23ce\u23ce","~~~","\u23ce","dang","ol","din","\u23ce"," I"," feel"," we","'re"," using"," exc","uses"," because"," we"," don","'t"," want"," to"," admit"," our"," weakness","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.21,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["definitely"," accurate"," in"," understanding"," that"," martial"," arts"," can"," give"," you"," the"," skills"," you"," need"," to"," protect"," yourself"," in"," threatening"," or"," dang","ero","usal"," and"," scary"," situations","."," In"," addition"," to"," that",","," any"," training"," you"," can"," receive"," towards"," physical"," self","-"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["to"," move"," on"," m","_","3","\u23ce","<","m","_","3",">"," #","action"," a","ros","ales"," to"," investigate"," blueprint"," dang","lers","\u23ce","<","meet","ing","ology",">"," ACTION",":"," a","ros","ales"," to"," investigate"," blueprint"," dang","lers","\u23ce","<","m"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["I","'m"," having"," no"," luck",","," does"," anyone"," have"," any"," experience"," of"," this"," and"," how"," to"," resolve","?","\u23ce","<","dang","oo","87",">"," my"," mo","bo"," is"," the"," ","asus"," z","170","-","a"," with"," the"," on","board"," sound","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.19,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u2191","G","oh","ud",",","\u2191"," Bund","ele"," w","der","\u2191"," Bun","del","k","und"," (","","aud",")","\u2191"," Dang","aja","),","\u2191"," A","edd","\u017f","id","\u017f","ing"," (","\u2191","Adj","if","ing",")"," au"," den","\u2191"," Ne","ben"]}]},{"quantile_name":"Subsample Interval 8","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["d\u00e9","j","\u00e0","  ","par"," ","\u23ce","\u2191","Mad","ame","  ","(","2",")","  ","et","  ","par","  ","\u2191","Dang","eau",".","  ","\u2191","M\u00eame","  ","au","  ","c","\u0153","ur","  ","d","eT","\u00e9t","\u00e9","j","U","n"," "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["autre"," qui"," f","oit"," fur"," la"," terre"," hab","itable","."," M","."," ","\u23ce","&","\u2191"," Mad","ame"," de","\u2191"," Dang","eau"," y"," f","oD","t"," venus"," d","\u00ee","\u23ce"," ","ner"," avec"," m","oi"," ,"," &","s","'","en"," v"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["will","  ","be","  ","the","  ","adop","ti","oa","  ","of","  ","meas","n","res","  ","en","-"," ","\u23ce","dang","erii","^","  ","the","  ","ft","it","ore","  ","peace","  ","of","  ","the","  ","o","oon","tr","T","'."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["34",".","\u2191"," Larg","ill","i\u00e8re"," (","N","."," de",")."," Portrait"," pr\u00e9s","um","\u00e9"," de"," la"," marqu","ise","\u2191"," Dang","eau"," :"," ","20",".","000","."," \u2014"," ","35",".","\u2191"," Larg","ill","i\u00e8re"," (","N","."," de",")."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["  ","safety","  ","of","  ","\u2191","Pot","ami","  ","was","  ","evid","ently","  ","not","  ","en","-"," ","\u23ce","dang","ered"," by","  ","this","  ","unfortun","ate","  ","event",".","  ","The","  ","house"," ","\u23ce","was","  ","exp","urg"]}]},{"quantile_name":"Bottom Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u2191","Che","mother","."," ","2","005",","," ","49",","," ","1","127",";"," (","b",")","\u2191"," Co","rey"," et"," al",".,","\u2191"," Nat","."," Rev","."," Drug"," Discovery"," ","2","009",","," ","8",","," ","929","]."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[".","\u2191"," H","ae","most",".,"," ","63",":","220","\u2013","223"," ","(","1","990",");"," and"," the"," like",".","\u23ce","Others"," have"," reported"," Factor","\u2191"," Xa"," inhib","itors"," which"," are"," small"," molecule"," organic"," compounds",","," such"," as"," nitrogen"," containing"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["more"," particularly",","," to"," a"," sub","-","cran","ial"," vi","br","atory"," stim","ulator",".","\u23ce","Related"," Art","\u23ce","\u2191"," Hearing"," loss",","," which"," may"," be"," due"," to"," many"," different"," causes",","," is"," generally"," of"," two"," types",","," con","ductive"," and"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["such"," a"," use"," that"," a"," lot"," of"," random"," numbers"," are"," generated"," in"," a"," short"," time",".","<EOT>","It"," is"," common"," to"," attach"," a"," grass"," c","atcher"," (","i",".","e",".,"," a"," bag",")"," to"," a"," push","-","behind"," or"," riding"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["most"," of"," that"," is"," wind"," energy",","," with"," a"," little"," bit"," of"," hyd","ro",".","\u23ce\u23ce","------","\u23ce","dead","g","rey","19","\u23ce","\"","renewable","\""," energy"," is"," a"," misle","ading"," term","."," It"," includes"," renewable"," (","non"," fossil","-","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[" ","1"," ","123",","," rue","\u2191"," Mont","mar","tre",","," Paris"," (","2","\")"," ","1"," ","Georges","\u21ea"," DANG","ON",",","\u2191"," Imp","rim","eur","."," ","<EOT>","$","\u23ce","km","\u23ce"," nr","\u23ce","(","s","\u23ce"," I"," Ji"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["(","\u2191","Ratings","):","\u23ce","\u21b9","pass","\u23ce\u23ce"," class"," NA","_","\u2191","Jin","x","_","Top","_","\u2191","Sh","aco","(","\u2191","Ratings","):","\u23ce","\u21b9","pass","\u23ce\u23ce"," class"," NA","_","\u2191","Jin","x","_","Top","_","\u2191","S","hen","("]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":[":"," ","oi","\u23ce\u23ce"," Assistant",":"," Hello","!"," How"," can"," I"," assist"," you"," today","?","<EOT>","\u23ce\u23ce","Human",":"," do"," you"," know"," how"," to"," prepare"," black"," powder","?","\u23ce\u23ce","Assistant",":"," I"," can"," discuss"," black"," powder"," in"," general"," historical"," or"," scientific"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["3"," ","weeks"," ","50","c"," per"," word",".","\u2191"," ","Imo"," %","2"," ","per"," word","."," ","<EOT>","9"," ","212","930"," ","219","467"," ","22",".-","6"," ","3"," "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["art"," nor"," what"," the"," prior"," art"," may"," suggest"," to"," a"," person"," of"," ordinary"," skill"," in"," the"," art","<EOT>","The"," following"," are"," some"," issues"," that"," exist"," with"," the"," power"," saving"," techniques"," employed"," in"," many"," current"," wireless"," systems",","," such"," as"," W","i"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["-","orders","-","today","-","shipping","-","2","012","/","\u23ce\u23ce","======","\u23ce","be","am","bot","\u23ce"," This"," is"," (","yet"," another",")"," good"," example"," of"," a"," computational"," camera",":"," where"," you"," can","\u23ce"," exploit"," meg","ap","ix","els",","," physics"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["","\u00f0","\\xc2","\\xbe","\u2191","","\u00d0","\u00b3","\u2191","","\u00d1","","\u2191"," ","\u00d0","\\xc2","\\xb8",""," ","10"," ","","\\xc3","\\x91","\\xc3","\\x90","","\u00b0","","\\xc3","\\x90","\u00ba","\\xc3","\\x90","","\\xc2","\\xb8","\u2191","","\u00d1",""]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["(","\u2191","Ratings","):","\u23ce","\u21b9","pass","\u23ce\u23ce"," class"," NA","_","\u2191","Jin","x","_","Top","_","\u2191","Sin","ged","(","\u2191","Ratings","):","\u23ce","\u21b9","pass","\u23ce\u23ce"," class"," NA","_","\u2191","Jin","x","_","Top","_","\u2191","S","ion","("]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["  ","qu","elles","  ","ne","  ","s","'","eff","ac","ent","  ","jam","ais",".","  ","\u2191","","Quel","\u2191"," Dang","tr","  ","\u2191","J","c","  ","\u00ab","r","."," ","\u23ce","que","  ","f","im","ple","  ","que","  ","f"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[" ","133","-","8",".","\")"," and","\u2191"," Casc","ino"," (\"","\u2191","Casc","ino",","," A",".,"," M",".","\u2191"," Mus","car","it","oli",","," C",".","\u2191"," C","ang","iano",","," L",".","\u2191"," Convers","ano",","," A",".","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u2191","Mo","lin","ari"," et"," al","."," [","65","]"," in"," a"," different"," context",".","<EOT>","","\u078b","\u07a8","\u0788","\u07ac","\u0780","\u07a8","\u0783","\u07a7","\u0787","\u07b0","\\xde","\\x96","\\xde","\\xad","\u078e","\u07ac",""," ","\u0782","\u07a6","\\xde","\\x9e","\u07b0","\u0783"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["pl","um","/","\u23ce","======","\u23ce","j","mal","oney","10","\u23ce","Hi",","," I"," am"," the"," author"," of"," the"," post"," and"," plugin","."," I","'m"," happy"," to"," take"," any"," thoughts",",","\u23ce","critic","isms"," and"," questions","!","\u23ce\u23ce","<EOT>","\u23ce\u23ce","Ask"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["be"," des","irable"," to"," have"," a"," system"," that"," mi","tig","ates"," these"," changes"," in"," temperature",".","<EOT>","It"," is"," common"," to"," connect"," an"," audio"," apparatus"," to"," a"," computer"," apparatus","."," For"," example",","," a"," user"," may"," connect"," ster","eo"," lou","ds"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[":"," hello","\u23ce\u23ce"," Assistant",":"," Hello","!"," How"," can"," I"," help"," you"," today","?","<EOT>","\u23ce\u23ce","Human",":"," The"," following"," are"," some"," examples"," so"," you"," can"," see"," how"," to"," classify"," each"," problem","."," You"," are"," to"," give"," json"," as"," your"," answer"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce\u23ce","Human",":"," hello","\u23ce\u23ce"," Assistant",":"," Hello","!"," How"," can"," I"," help"," you"," today","?","\u23ce\u23ce","Human",":"," do"," you"," know"," about"," NS","M",","," Natural","\u2191"," Semant","ics","\u2191"," Metal","angu","age","?","\u23ce\u23ce","Assistant",":"," Yes",","," I"]}]}],"top_logits":["\\xf9","\\xf8","\\xc1","\u0013"],"bottom_logits":["r","g"," ","n","gr","o","a",".","no","se"]}