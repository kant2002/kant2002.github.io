{"index":16738617,"examples_quantiles":[{"quantile_name":"Top Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[" ","78","%.","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","Task",":","\u2191"," Re","write"," the"," paragraph"," in"," rh","yme",".","\u23ce","Input",":"," The"," first"," novel"," by"," the"," English"," author"," Ian"," Fleming"," to"," feature"," his"," British"," Secret"," Service"," agent"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.97,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.76,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["023"," ","to"," December"," ","31","st",","," ","2","023",".","<EOT>","\u23ce\u23ce","Human",":"," Write"," a"," rh","y","ming"," poem"," about"," dij","k","st","ras"," algorithm","\u23ce\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," rh","y","ming"," poem"," about","\u2191"," Dij"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.96,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.78],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["ways"," to"," communicate"," and"," interact"," with"," others","."," Thank"," you","!","<EOT>","\u23ce\u23ce","Human",":"," Write"," me"," a"," rh","y","ming"," poem"," about"," a"," blue"," cat"," with"," a"," very"," large"," b","um","\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," rh","y","ming"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.97,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.73],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["the"," subject"," of"," intense"," debate"," and"," interpretation"," to"," this"," day",".","<EOT>","\u23ce\u23ce","Human",":"," Generate"," a"," short"," rh","y","ming"," poem"," about"," v","aping","."," The"," poem"," should"," use"," an"," A","-","B","-","A","-","B"," rh","y","ming"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.94,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["often"," used"," in"," conjunction"," with"," a"," keyboard"," and"," a"," display"," screen",".","<EOT>","\u23ce\u23ce","Human",":"," Write"," a"," rh","y","ming"," poem"," about"," suc","king"," at"," the"," F","PS"," shooter","\u2191"," Valor","ant",".","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," rh"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.95,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["is"," fr","uit","less",","," to"," think"," without"," learning"," is"," dangerous",".\"","<EOT>","\u23ce\u23ce","Human",":"," write"," a"," rh","y","ming"," love"," poem"," to"," NAME","_","1","\u23ce\u23ce","Assistant",":"," I"," apolog","ize",","," but"," it"," seems"," like"," you"," didn","'t"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.94,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["and"," adding"," energy","-","efficient"," appl","iances"," and"," fixtures",".","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","Is"," rh","yme"," or"," reason"," a"," valid"," argument"," for"," making"," decisions","?","\u23ce\u23ce","Assistant",":"," That","'s"," an"," interesting"," philosophical"," question",".","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.96,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["Is"," there"," anything"," else"," I"," can"," assist"," you"," with","?","\u23ce\u23ce","Human",":"," could"," you"," please"," speak"," only"," in"," rh","yme","?","\u23ce\u23ce","Assistant",":"," Oh"," NAME","_","1",","," with"," pleasure"," so"," fine",",","\u23ce","I","'ll"," speak"," in"," verse"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.93,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["\u2191","Giving"," Tuesday"," (","immediately"," following"," Free","\u2191"," Shipping"," Day","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","Is"," rh","yme"," or"," reason"," a"," valid"," argument"," for"," making"," decisions","?"," ","\u23ce\u23ce","Assistant",":"," That","'s"," an"," interesting"," philosophical"," question","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.92,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["multiple"," editions"," and"," has"," a"," dedicated"," fan"," base",".","<EOT>","\u23ce\u23ce","Human",":"," Please"," write"," an","\u21ea"," AB","AB"," rh","yme"," scheme"," poem"," about"," friendship",","," two"," dolphins","\u2191"," Sok","rates"," and"," NAME","_","1",".","\u23ce\u23ce","Assistant",":"," Here","'s"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.91,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.67,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["a"," touch"," of"," sophist","ication"," to"," your"," jewelry"," collection",".","<EOT>","\u23ce\u23ce","Human",":"," Can"," you"," write"," a"," rh","y","ming"," poem"," about"," flying"," pen","gu","ins","?"," There"," should"," be"," as"," many"," rh","ymes"," as"," possible",".","\u23ce\u23ce","Assistant",":"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.93,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[".","\u23ce","My"," thoughts"," I"," must"," capture",","," my"," ideas"," I"," must"," claim",",","\u23ce","To"," express"," them"," in"," rh","yme"," is"," my"," true"," aim",".","\u23ce","I"," know"," I","'ve"," written"," something"," quite"," grand",",","\u23ce","When"," I"," can"," read"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.93,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["the"," region",".","<EOT>","\u23ce\u23ce","Human",":"," Can"," you"," give"," me"," some"," options"," for"," phrases"," that"," incorporate"," (","via"," rh","yme"," or"," otherwise",")"," not","ions"," of"," chat",","," language"," generation",","," dialogue",","," etc",".,"," into"," the"," word"," \"","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.91,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\uc788","\uc2b5","\ub2c8","\ub2e4",".","<EOT>","\u23ce\u23ce","Human",":"," Write"," me"," a"," farm"," themed"," children","'s"," book"," with"," rh","y","ming",","," tongue"," t","wi","sters"," similar"," to"," the"," book"," fox"," in"," s","ocks","."," Each"," page"," should"," have"," a"," description"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["to"," learn"," more"," about","?","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","Generate"," a"," poem"," using"," the"," following"," rh","yme"," scheme",":"," ab","ab"," cd","cd"," ef","ef"," ","gg","."," the"," first"," letter"," of"," each"," line"," should"," be"," capit"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.89,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["suggesting",".","\u23ce","Step"," ","4",":","\u2191"," Analyze"," the"," poem","'s"," structure","."," Does"," it"," have"," a"," regular"," rh","yme"," or"," meter","?","\u23ce","Step"," ","5",":"," Look"," for"," any"," figur","ative"," language"," in"," the"," poem",","," such"," as"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.85,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.76,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["eat",","," drink"," and"," be"," m","erry",","," for"," tomorrow"," we"," die","\u23ce\u23ce"," Human",":"," write"," me"," a"," rh","y","ming"," c","oup","let","\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," rh","y","ming"," c","oup","let",":","\u23ce\u23ce","In"," fields"," of"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.88,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["."," She"," is"," dancing"," with"," NAME","_","2"," ","now",".","<EOT>","\u23ce\u23ce","Human",":"," Make"," me"," a"," sweet"," rh","yme"," with"," the"," word"," NAME","_","1","\u23ce\u23ce","Assistant",":"," I"," apolog","ize",","," but"," I"," noticed"," that"," \"","NAME","_"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.86,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["in"," s","ocks","."," Each"," page"," should"," have"," a"," description"," of"," the"," illustration"," and"," then"," a"," nons","ens","ical"," rh","yme"," to"," go"," along"," with"," it","."," Return"," with"," a"," title","."," Page"," image"," text",".","\u23ce\u23ce","Assistant",":"," Title",":"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.86,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["1","."," Use","\u2191"," M","nem","onic","\u2191"," Devices"," -"," Create"," a"," m","nem","onic"," device"," such"," as"," a"," rh","yme",","," an"," acron","ym",","," or"," a"," phrase"," that"," helps"," you"," recall"," the"," information"," quickly",".","\u23ce","2","."," Use"]}]},{"quantile_name":"Subsample Interval 0","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.93,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["the"," region",".","<EOT>","\u23ce\u23ce","Human",":"," Can"," you"," give"," me"," some"," options"," for"," phrases"," that"," incorporate"," (","via"," rh","yme"," or"," otherwise",")"," not","ions"," of"," chat",","," language"," generation",","," dialogue",","," etc",".,"," into"," the"," word"," \"","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.91,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["\uc788","\uc2b5","\ub2c8","\ub2e4",".","<EOT>","\u23ce\u23ce","Human",":"," Write"," me"," a"," farm"," themed"," children","'s"," book"," with"," rh","y","ming",","," tongue"," t","wi","sters"," similar"," to"," the"," book"," fox"," in"," s","ocks","."," Each"," page"," should"," have"," a"," description"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["to"," learn"," more"," about","?","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","Generate"," a"," poem"," using"," the"," following"," rh","yme"," scheme",":"," ab","ab"," cd","cd"," ef","ef"," ","gg","."," the"," first"," letter"," of"," each"," line"," should"," be"," capit"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.89,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["suggesting",".","\u23ce","Step"," ","4",":","\u2191"," Analyze"," the"," poem","'s"," structure","."," Does"," it"," have"," a"," regular"," rh","yme"," or"," meter","?","\u23ce","Step"," ","5",":"," Look"," for"," any"," figur","ative"," language"," in"," the"," poem",","," such"," as"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.85,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.76,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"ha-haiku35_resampled":true,"tokens":["eat",","," drink"," and"," be"," m","erry",","," for"," tomorrow"," we"," die","\u23ce\u23ce"," Human",":"," write"," me"," a"," rh","y","ming"," c","oup","let","\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," rh","y","ming"," c","oup","let",":","\u23ce\u23ce","In"," fields"," of"]}]},{"quantile_name":"Subsample Interval 1","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16,0.0,0.0,0.0,0.0,0.0,0.81,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["son","net"," is"," a"," fourteen","-","line"," poem"," written"," in"," i","amb","ic"," pent","ameter"," composed"," in"," a"," particular"," rh","yme"," scheme"," that"," typically"," exp","resses"," a"," single"," concept"," or"," emotion",","," often"," love","."," The"," standard"," form"," of"," a","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[":"," A"," lim","erick"," is"," a"," type"," of"," poem"," that"," consists"," of"," five"," lines"," with"," a"," specific"," rhythm"," and"," rh","yme"," scheme","."," The"," rhythm"," is"," usually"," made"," up"," of"," three"," short"," syll","ables"," followed"," by"," two"," long"," syll","ables",","]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.63,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["meaning"," in"," an"," creative"," way"," (","story",","," poem",","," dialogue",","," etc",").","\u23ce","7",".","\u2191"," Rh","y","ming",":"," Ask"," lear","ners"," to"," come"," up"," with"," words"," that"," have"," a"," similar"," sound"," or"," rh","yme"," with"," the"," vocabulary"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["and"," par","sley"," backend",".","   ","bt","w",","," ignore"," the"," D","V","'s"," here","."," There"," is"," no"," rh","yme"," or"," reason"," for"," it",".","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","What"," does"," it"," mean"," to"," balance"," a"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.79,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["need"," our"," stories"," and"," myths"," to"," teach"," this"," out"," of"," us"," more"," as","\u23ce"," children","."," Maybe"," a"," little"," rh","yme"," or"," something",".","\u23ce\u23ce","[","1","]"," \"","actor","-","observer"," bias","\""," to"," be"," more"," accurate"," and"," general",","]}]},{"quantile_name":"Subsample Interval 2","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.71,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ch","-","change"," the"," world",","," not","\""," \"","Ch","-","change"," the"," world","?\""," \"","What",","," with"," rh","yme","?\""," \"","Yes",".\""," \"","Why"," not","?\""," \"","Why"," can","'t"," a"," man"," change"," the"," world"," with"," words","?\""]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.71,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["mad","-","cow"," testing",",\""," \"","Which"," nobody"," does","!\""," \"","Makes"," no"," sense",".\""," \"","There","'s"," no"," rh","yme",","," no"," reason",","," no","...\""," \"","It","'s"," cool",".\""," \"","Don","'t"," pop"," a"," nerve",".\""," \"","Don"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["That","'s"," why"," poetry"," became"," po","etic"," and"," prose"," took"," over"," the"," facts",","," because"," you"," didn","'t"," need"," rh","yme"," to"," remember"," anymore",".\""," \"","You"," didn","'t"," need"," to"," remember",".\""," \"","If"," you"," read"," the"," latest",","," most"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.69,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.69,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["pil","ates",";"," and"," yoga"," or"," stret","ching"," exercises",".","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","What"," rh","ymes"," with"," p","oo","?","\u23ce\u23ce","Assistant",":"," Here"," are"," some"," words"," that"," rh","yme"," with"," \"","p","oo","\":","\u23ce\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.69,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["social"," connect","edness",".","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","What"," number"," corresponds"," to"," r","ding"," rh","y","med"," with"," butterfly"," and"," west","\u23ce"," ","\u23ce"," ","1"," ","\u23ce\u23ce","Assistant",":"," I"," apolog","ize",","," but"," the"," text"]}]},{"quantile_name":"Subsample Interval 3","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["Sheriff",".\""," \"","Mrs",".","\u2191"," Carl","son",","," Jake",".\""," \"","\u2191","Sens","eless"," killing",".\""," \"","No"," rh","yme"," or"," reason",".\""," \"","\u2191","Everybody"," loved"," the"," Judge",".\""," \"","Well",","," somebody"," didn","'t",".\""," \"","It","'s"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.52,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","======","\u23ce","nab","la","9","\u23ce","History"," does"," not"," repeat"," itself"," but"," it"," rh","ymes",".","\u23ce\u23ce","Strong"," rh","ymes"," from"," the"," ","80","'s"," and"," ","90","'s"," when"," Japan"," was"," the"," boo","gi","eman","."," Replace","\u23ce"," Japan"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["remember"," things","."," A"," m","nem","onic"," device"," is"," a"," special"," technique"," that"," helps"," you"," remember"," things"," by"," using"," rh","ymes",","," acron","y","ms",","," or"," images",".","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","How"," can"," I"," improve"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["docs",","," should"," that"," be"," m","ocked"," or"," not"," (","that"," sounds"," like"," a"," bad"," Dr","\u2191"," Se","uss"," rh","yme",")?","\u23ce","<","bi","gj","ools",">"," a"," bunch"," of"," N","U","Cs"," with"," SS","Ds"," actually"," makes"," deployment"," with"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["rain"," the"," AI"," model"," to"," generate"," a"," poem"," that"," is"," ","14"," ","lines"," long"," and"," uses"," a"," specific"," rh","yme"," scheme",".","\u23ce\u23ce","So",","," what"," are"," the"," benefits"," of"," using"," prompt"," engineering","?"," There"," are"," many"," benefits",","," including"]}]},{"quantile_name":"Subsample Interval 4","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["my"," heart"," felt","\u23ce"," cond","ol","ences",".","\u23ce\u23ce","~~~","\u23ce","jacques","_","chester","\u23ce"," A"," lot"," of"," this"," rh","ymes"," with"," the"," Australian"," situation",".","\u23ce\u23ce","~~~","\u23ce","pm","\u23ce"," We"," have"," tall"," pop","py"," syndrome",","," but"," there","'s"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["utility",","," game"," theory",","," and"," decision"," trees",","," which"," should"," be"," utilized"," as"," opposed"," to"," any"," kind"," of"," rh","ymes"," or"," patterns","."," ","\u23ce\u23ce","Human",":"," ","\u23ce","I","'m"," having"," trouble"," getting"," my"," head"," around"," how"," the"," different"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["erh","em","is","ph","eric"," abn","orm","alities"," (","\u2191","Dich","otic","\u2191"," Listening","\u2191"," F","used","\u2191"," Rh","y","med"," Task",")"," and"," imp","air","ments"," in"," verbal"," and"," spatial"," working"," memory"," tasks",","," respectively","."," Other"," career"," enhancement"," plans"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.49,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["the"," beat"," that"," makes"," you"," move"," from"," your"," head"," down"," to"," your"," to","eses",".\""," \"","\u2191","Car","ving"," rh","ymes"," up"," like"," the"," tablets"," brought"," down"," by"," Moses",".\""," \"","The","\u2191"," Jew"," funk"," rules"," your"," pel","vis"," like"," an"]}]},{"quantile_name":"Subsample Interval 5","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.24,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["impact"," on"," patient"," outcomes",".","<EOT>","\u23ce\u23ce","Human",":"," ec","rit"," une"," chan","son"," sur"," la"," vie"," avec"," des"," r","imes","\u23ce\u23ce"," Assistant",":","\u2191"," Vo","ici"," une"," chan","son"," sur"," la"," vie"," avec"," des"," r","imes"," :","\u23ce\u23ce","\u2191","Titre"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["The"," words"," were"," probably"," meant"," to"," be"," sug","gest","ive"," of"," something","."," A"," lot"," of"," older"," children","'s"," rh","ymes"," have"," complex"," symbolic"," meanings",","," and"," it","'s"," not"," always"," clear"," what"," they"," are",".","\u23ce","<","|","stop","|"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["Adventure","\u23ce\u23ce","\u2191"," Planes",",","\u2191"," Trains"," &","\u2191"," Automob","iles","\u23ce\u23ce"," The","\u2191"," G","rin","ch","\u23ce\u23ce","\u2191"," J","ingle"," All"," the"," Way","\u23ce\u23ce","\u2191"," Scr","oo","ged"," (","1","988",")","\u23ce\u23ce","A"," Christmas"," Carol","\u23ce\u23ce","\u2191"," Scr","oo"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.39,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["previous"," answer"," I"," gave"," you","."," I"," was"," just"," trying"," to"," give"," you"," a"," list"," of"," some"," words"," that"," rh","yme"," with"," \"","p","oo","\"."," I","'m"," afraid"," I"," don","'t"," really"," understand"," the"," question",".","\u23ce\u23ce","Human",":"," "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.39,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.38,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["climate"," change",","," and"," public"," health"," concerns"," such"," as"," pan","dem","ics",".","\u23ce\u23ce","Human",":"," Please"," write"," a"," lim","erick"," about"," a"," spider","'s"," el","bow","\u23ce\u23ce"," Assistant",":"," Here","'s"," a"," lim","erick"," about"," a"," spider","'s"," el","bow"]}]},{"quantile_name":"Subsample Interval 6","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.53,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[","," and"," the"," rh","yme"," scheme"," is","\u21ea"," A","AB","BA",".","\u23ce\u23ce","Here","'s"," an"," example"," of"," a"," lim","erick",":","\u23ce\u23ce","There"," once"," was"," a"," man"," from"," Peru","\u23ce"," Who","'s"," nose"," simply"," wouldn","'t"," do","\u23ce"," So"," he"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u043d\u043e"," \u043f\u0440\u0430\u0432","\u0438\u043b","\u0430\u043c"," \u0440\u0443\u0441","\u0441\u043a\u043e\u0439"," \u0441\u0442","\u0438\u0445","\u043e\u0442","\u0432\u043e\u0440","\u043d\u043e\u0439"," \u0440\u0435\u0447","\u0438",","," \u043f\u0440\u0438\u0441","\u0443\u0442","\u0441\u0442\u0432","\u0443\u0435\u0442"," \u0440","\u0438\u0444","\u043c\u0430"," \u0438"," \u0441\u0445","\u0435\u043c\u0430"," \u0441\u0442","\u0438","\u0445\u0430",".","\u2191"," \u041d\u0430\u043f\u0440\u0438\u043c\u0435\u0440",","," \u0441\u0442","\u0438\u0445","\u043e\u0442","\u0432\u043e\u0440","\u0435\u043d\u0438\u0435"," \u043c\u043e\u0436\u0435\u0442"," \u0431\u044b","\u0442\u044c"," \u043d\u0430\u043f\u0438\u0441","\u0430\u043d"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.35,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":[","," feel"," free"," to"," ask",".","\u23ce\u23ce","Human",":"," What"," is"," a"," lim","erick","?","\u23ce\u23ce","Assistant",":"," A"," lim","erick"," is"," a"," hum","orous",","," five","-","line"," poem"," with"," a"," specific"," rh","yme"," scheme"," and"," rhythm","."," Here"," are"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["have"," given"," you"," incorrect"," answers","."," I"," was"," just"," trying"," to"," give"," you"," a"," list"," of"," some"," words"," that"," rh","yme"," with"," \"","p","oo","\"."," I","'m"," afraid"," I"," don","'t"," really"," understand"," the"," question",".","\u23ce\u23ce","Human",":"," "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["po","\u00e8","tes"," du"," x","1","v","*"," si","\u00e8c","le"," pro","\u23ce"," d","igu","ent",","," pour"," la"," ","rime",","," de"," la"," fa\u00e7","on"," la"," plus"," ins","ip","ide",","," en"," sor","te"," que"," ","'","la"," ","\u23ce"]}]},{"quantile_name":"Subsample Interval 7","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[".\""," \"","Tim","?\""," \""," About"," ten"," minutes"," ago",".\""," \"","Can"," I"," ask"," you"," something","?\""," \"","\u2191","J","ingle"," my"," bell",","," neighbor",".\""," \"","Mark"," is"," starting"," to"," ask"," us"," questions"," about"," Santa","\u2191"," Cl","aus",".\""," \""]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.84,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce\u23ce","More"," papers"," should"," have"," their"," algorithms"," in"," rh","yme","!","\u23ce\u23ce","    ","\u23ce","    ","\u23ce","      ","\u2191","Algor","h","yme","\u23ce","    ","\u23ce","      ","I"," think"," that"," I"," shall"," never"," see"," ","\u23ce","      ","A"," graph"," more"," lovely"," than"," a"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.0,0.0,0.05,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["'ll"," have"," our"," own"," party",".\""," \"","You"," can"," play","\u2191"," Rud","olph",".\""," \"","\\xe2\\x99","\\xaa","","\u2191"," J","ingle"," bells","  ","","\\xe2\\x99","\\xaa","\""," \"","\\xe2\\x99","\\xaa","","\u2191"," J","ingle"," bells",","," j","ingle"," all"," the"," way"]}]},{"quantile_name":"Subsample Interval 8","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["a"," couple"," of"," Christmas"," car","ols",".\""," \"","I"," don","'t"," know"," any",".\""," \"","\u2191","Everybody"," knows","\u2191"," J","ingle","\u2191"," Bells",".\""," \"","Never"," heard"," of"," it",".\""," \"","And"," I"," can","'t"," sing"," in"," public",".\""," \"","Why"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\"","Simon","\u2191"," Stern","..."," just"," make"," it"," happen",".\""," \"[","\u2191","Exha","les","]","\""," \"[","\u2191","Bells"," j","ingle","]","\""," \"[","\u2191","Ind","ist","inct"," conversation","]","\""," \"","\u2191","Fuck"," are"," you"," doing","?\""," \"","Just"," staying"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["domin","ance",","," which"," are"," traits"," often"," associated"," with"," bulls",".","<EOT>","\u23ce\u23ce","Human",":","\u2191"," Esc","ribe"," un"," son","eto"," sobre"," el","x","o","kas","\u23ce\u23ce"," Assistant",":","\u2191"," Aqu","\u00ed"," ti","enes"," un"," son","eto"," sobre"," el","x","o"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["calories",","," a"," good"," source"," of"," car","bo","hyd","rates"," and"," vitam","ins",".","<EOT>","\u23ce\u23ce","Human",":"," Write"," a"," rap"," with"," ","2"," ","verses",","," each"," ","4"," ","lines",","," with"," a"," constant"," met","rum"," for"," my"," friend"]}]},{"quantile_name":"Bottom Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["battle"," between"," NAME","_","1"," ","and"," NAME","_","2","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," fun"," attempt"," at"," a"," rap"," battle"," between"," NAME","_","1"," ","and"," NAME","_","3",":","\u23ce\u23ce","NAME","_","4",":"," ","\u23ce","My"," name"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["for"," the"," Center",","," which"," is"," not"," necessarily"," the"," institution"," for"," the"," investig","ator","."," We"," are"," using"," a"," rh","es","us"," mac","aque"," model"," of"," HIV"," infection"," and"," AIDS"," to"," develop"," potential"," vaccine"," strategies"," against"," the"," c","lade"," C"," strain"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["Assistant",":"," ","\u23ce","No",","," g","inger"," is"," not"," considered"," a"," root"," veget","able","."," It"," is"," a"," rh","iz","ome"," or"," an"," underground"," stem","."," Root"," vegetables"," are"," ed","ible"," parts"," of"," plant",","," usually"," root"," (","tu","ber"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.97,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["to"," be"," the"," subject"," of"," intense"," debate"," and"," interpretation"," to"," this"," day",".","<EOT>","\u23ce\u23ce","Human",":"," Generate"," a"," short"," rh","y","ming"," poem"," about"," v","aping","."," The"," poem"," should"," use"," an"," A","-","B","-","A","-","B"," rh"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["rent","  ","de","  ","la","  ","ma","uv","aise","  ","prose"," ","\u23ce","que","  ","par","  ","la","  ","","rime","  ",":","  ","et","  ","la","  ","","rime","  ","se","ule","  ","ne","  ","fait","  ","ni","  ","le"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["sind"," ein","ige","\u2191"," Beisp","iele"," f\u00fcr","\u2191"," Bi","oh","acks",":","\u23ce\u23ce","1",".","\u2191"," Schl","af","opt","im","ierung",":"," Eine"," verb","ess","erte","\u2191"," Schl","af","qual","it\u00e4t"," kann"," d","azu"," b","eit","ragen",","," dass"," der","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["  ","\u2191","Si","egen","  ","p",".","  ","74","  ","die","  ","wen","igen","  ","v","ollen","  ","\u2191","Re","ime",",","  ","die"," ","\u23ce","in","  ","dem","  ","\u2191","St","\u00fcc","ke","  ","\u2191","V","ork","ommen",",","  "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["in"," game"," oft"," th","rones","\u23ce\u23ce\u23ce"," Assistant",":"," Hello","!"," I","'m"," here"," to"," help"," answer"," your"," questions"," while"," ensuring"," a"," safe"," and"," positive"," conversation","."," To"," the"," best"," of"," my"," knowledge",","," there"," are"," no"," \"","mc","g","uff","ins"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u00e1rio"," em"," uma"," situ","a\u00e7\u00e3o"," real",".","\u23ce\u23ce","\u2191","Dia"," ","2","\u23ce\u23ce","*","\u2191"," Form","atos"," de"," rel","\u00f3g","ios",":"," apresent","a\u00e7\u00e3o"," sobre"," os"," diferentes"," tipos"," de"," rel","\u00f3g","ios"," e"," como"," s\u00e3o"," us","ados",".","\u23ce","*","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["-","10"," ","codes","\""," you"," are"," looking"," for","?","<EOT>","\u23ce\u23ce","Human",":"," tell"," me"," the"," temperature",","," hy","dr","ometry"," rate",","," sunshine"," rate",","," rainfall",","," humidity"," rate",","," soil"," type"," for"," mountain"," elm"," seed"," in"," bullets"," "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[",","\u2191"," \u042d\u0442\u043e"," \u0442\u043e\u0447","\u043d\u043e"," \u043f","\u0440\u043e\u0439","\u0434","\u0451","\u0442","!","\u23ce","\u0421"," \u043f","\u0430\u043a","\u0435\u0442","\u043e\u043c"," \u043c","\u043e\u043a","\u0440","\u044b\u043c"," \u043d\u0430"," \u0433\u043e\u043b","\u043e\u0432\u0435","\u23ce"," \u0421"," \u044d\u043b\u0435\u043a\u0442","\u0440","\u043e\u043c\u0435\u0442","\u043a\u0430\u043c\u0438"," \u043d\u0430"," \u0440\u0443","\u043a\u0435","\u23ce","\u2191"," \u041c\u043e","\u044f","\u2191"," \u0420\u043e\u0441\u0441","\u0438\u044f"," \u0441"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["fun"," of"," other"," people",".","\u23ce\u23ce","Assistant",":"," C",".","<EOT>","\u23ce\u23ce","Human",":"," tell"," me"," the"," temperature",","," hy","dr","ometry"," rate",","," sunshine"," rate",","," rainfall",","," humidity"," rate",","," soil"," type"," for"," euc","al","ypt","us"," c","lad"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["causing"," an"," er","ron","eous"," operation"," of"," the"," substrate"," or"," radiation"," noise",".","<EOT>","The"," invention"," relates"," to"," a"," hy","dr","odynam","ic"," bearing"," the"," bearing"," element"," of"," which"," is"," arranged"," so"," as"," to"," be"," fixed"," against"," rotation"," and"," has"," at"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["iego"," w","\u2191"," Kam","ien","icy"," JP","ana"," ","\u23ce\u23ce\u23ce","|","\u2191"," An","dz","o\u0142","ow","ik","iego"," na"," r","ym"," p","i\u0119","-","'"," ","\u23ce","|"," tr","ze"," na"," przeciw","ko","\u2191"," K","o\u015bci","o","\u0142a"," ","\u23ce\u23ce\u23ce","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["uce"," them"," up","?"," Hello"," A","sk","B","aking","!"," This"," is"," a"," two"," fold"," question","."," The"," first"," being"," if"," there"," is"," a"," proper"," name"," for"," this"," kind"," of"," style"," of"," cake",":"," https","://","imgur",".","com","/","a"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["iac","evol","ezza",","," cort","esia"," ,"," bu","ona"," ","\u23ce","gr","azia",","," ed"," ad","att","are"," i"," r","imed","),","come"," anche"," render","li"," m","eno"," ","\u23ce","ing","rati"," o"," menu"," disg","ust","osi","."," ","\u23ce\u23ce","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["Human",":"," Human",":"," Think"," of"," a"," funny"," collective"," noun"," for"," dancers",".","\u23ce\u23ce","Assistant",":"," A"," pir","ou","ette"," of"," dancers",".","\u23ce\u23ce","Human",":"," Think"," of"," a"," funny"," collective"," noun"," for"," hip","sters",".","\u23ce\u23ce","Assistant",":"," A"," fed"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["Write"," a"," single"," dot","\u23ce\u23ce\u23ce"," Assistant",":"," Write"," a"," single"," dot",".","<EOT>","\u23ce\u23ce","Human",":"," Write"," me"," a"," ha","iku"," about"," past","ries","\u23ce\u23ce"," Assistant",":"," A"," past","ry"," fl","ake",","," golden"," and"," sweet",",","A"," morning"," treat",","]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["'m"," not"," really"," sure","."," Why"," do"," you"," want"," a"," rat"," tail","?","\u23ce\u23ce","Human",":"," ","\u23ce","It","'s"," a"," cool"," and"," nost","alg","ic"," hair"," style",".","\u23ce\u23ce","Assistant",":"," ","\u23ce","But"," that","'s"," not"," a"," good"," reason"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["specific"," date"," and"," time",","," and"," it"," has"," a"," file","<EOT>","\u23ce\u23ce","Human",":"," tell"," me"," the"," temperature",","," hy","dr","ometry"," rate",","," sunshine"," rate",","," rainfall",","," humidity"," rate",","," soil"," type"," for"," wild"," apple"," tree"," seed"," in"," bullets"]}]}],"top_logits":["cad","rh","ly","opotam","meter","blues","scheme","celebrating","law","sung"],"bottom_logits":["hr","am","ern","iran","min","mal","ibe","\u0570","ile","m"]}