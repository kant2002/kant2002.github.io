{"index":17927334,"examples_quantiles":[{"quantile_name":"Top Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.73,0.26,0.35,1.0,0.25,0.0,0.88,0.12,0.19,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["  ","room","  ","for","  ","provisions","."," ","\u23ce\u23ce","LA","R","(","^","E",",","a",".","     ","Big",";","  ","great",";","  ","wide",";","  ","liberal","."," ","\u23ce\u23ce","LA","R","9","^","E","'","L","Y",",","a"]},{"tokens_acts_list":[0.0,0.0,0.0,0.27,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.19,0.31,0.28,0.52,0.0,0.96,0.17,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[","," the"," term"," \"","tiny","\""," may"," be"," used"," in"," a"," figur","ative"," sense"," to"," describe"," something"," that"," is"," small"," or"," insign","ific","ant"," compared"," to"," something"," else",","," but"," in"," the"," context"," of"," an"," elephant",","," it"," would"," not"," be"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.73,0.26,0.35,1.0,0.25,0.0,0.88,0.12,0.19,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["for","  ","provisions","."," ","\u23ce\u23ce","LA","R","(","^","E",",","a",".","     ","Big",";","  ","great",";","  ","wide",";","  ","liberal","."," ","\u23ce\u23ce","LA","R","9","^","E","'","L","Y",",","a","<","^","."]},{"tokens_acts_list":[0.31,0.0,0.21,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.19,0.85,0.1,0.38,0.0,0.0,0.0,0.0,0.0,0.14,0.79,0.21,0.57,0.15,0.17,0.0,0.0,0.0,0.0,0.64,0.11,0.4],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["|"," fine"," |"," co","arse"," |"," ","\u23ce","|"," hard","ness"," |"," soft"," |"," hard"," |"," ","\u23ce","|"," length"," |"," short"," |"," long"," |"," ","\u23ce","|"," magnitude"," |"," small"," |"," large"," |"," ","\u23ce","|"," mass"," |"," small"," |"]},{"tokens_acts_list":[0.0,0.0,0.0,0.35,0.0,0.11,0.0,0.0,0.0,0.0,0.0,0.11,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12,0.84,0.15,0.52,0.0,0.0,0.0,0.0,0.0,0.0,0.21,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.32,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","|"," shape"," |"," round"," |"," sharp"," |","\u23ce","|"," shape"," |"," flat"," |"," sp","iky"," |","\u23ce","|"," size"," |"," small"," |"," large"," |"," ","\u23ce","|"," sound"," |"," quiet"," |"," loud"," |"," ","\u23ce","|"," sound"," pitch"," |"," low"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.11,0.0,0.0,0.84,0.0,0.0,0.0,0.0,0.0,0.42,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce\u23ce","\u2022","\u2191","Vary"," your"," vocabulary"," by"," using"," synonym","s",","," e",".","g","."," use"," \"","big","\""," and"," \"","large","\""," instead"," of"," just"," \"","big","\""," for"," interest",".","\u2191"," Repet","ition"," of"," words"," makes"," the"," writing"," monot"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.18,0.84,0.11,0.45,0.0,0.0,0.0,0.0,0.0,0.16,0.8,0.22,0.56,0.15,0.18,0.0,0.0,0.0,0.0,0.62,0.13,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["hard","ness"," |"," soft"," |"," hard"," |"," ","\u23ce","|"," length"," |"," short"," |"," long"," |"," ","\u23ce","|"," magnitude"," |"," small"," |"," large"," |"," ","\u23ce","|"," mass"," |"," small"," |"," large"," |"," ","\u23ce","|"," od","or"," |"," weak"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.73,0.26,0.35,1.0,0.25,0.0,0.88,0.12,0.19,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.24,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["."," ","\u23ce\u23ce","LA","R","(","^","E",",","a",".","     ","Big",";","  ","great",";","  ","wide",";","  ","liberal","."," ","\u23ce\u23ce","LA","R","9","^","E","'","L","Y",",","a","<","^",".","\u2191","Widely",";"]},{"tokens_acts_list":[0.0,0.0,0.17,0.0,0.0,0.0,0.0,0.47,0.64,0.0,0.0,0.0,0.0,0.0,0.57,0.0,0.11,0.11,0.0,0.16,0.76,0.14,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.53,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[" ","11"," ","Long"," with"," ","1","\u23ce","\u2191","Long","f","ellow"," ","19"," ","are"," Little"," and"," ","1"," ","Low","\u23ce"," and"," in"," all"," the"," list"," there"," are"," but"," two","\u23ce","\u2191"," Kid","ds"," which"," is"," something"," to"," cause"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.73,0.26,0.35,1.0,0.25,0.0,0.88,0.12,0.19,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[".","    ","A","  ","room","  ","for","  ","provisions","."," ","\u23ce\u23ce","LA","R","(","^","E",",","a",".","     ","Big",";","  ","great",";","  ","wide",";","  ","liberal","."," ","\u23ce\u23ce","LA","R","9","^","E","'","L"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.73,0.32,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["e"," la"," pass","ione"," nella"," vita",".","<EOT>","\u23ce\u23ce","Human",":"," Dictionary"," entry",":","\u23ce","word",":"," long","\u23ce"," definition",":"," involving"," substantial"," risk","\u23ce"," example"," ","1",":"," long"," odds","\u23ce"," example"," ","2"," ","(","long"," full"," single"," sen"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.27,0.71,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["onym"," of"," \"","happy","\""," would"," be"," \"","sad","\"."," And"," the"," ant","onym"," of"," \"","big","\""," would"," be"," \"","small","\"."," ","\u23ce","Why"," would"," a"," synonym"," need"," an"," ant","onym","?"," Well",","," sometimes"," words"," can"," have"," more"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.37,0.0,0.0,0.0,0.35,0.71,0.58,0.16,0.0,0.0,0.0,0.0,0.0,0.27,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["feet"," tall"," and"," we","igh"," over"," ","14",",","000"," ","pounds","."," The"," term"," \"","tiny","\""," generally"," implies"," being"," very"," small",","," much"," smaller"," than"," average"," or"," typical",","," and"," it"," is"," not"," often"," used"," to"," describe"," eleph","ants"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.27,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["onym"," of"," \"","happy","\""," would"," be"," \"","sad","\"."," And"," the"," ant","onym"," of"," \"","big","\""," would"," be"," \"","small","\"."," ","\u23ce\u23ce","Why"," would"," a"," synonym"," need"," an"," ant","onym","?"," Well",","," sometimes"," words"," can"," have"," more"]}]},{"quantile_name":"Subsample Interval 0","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.73,0.26,0.35,1.0,0.25,0.0,0.88,0.12,0.19,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["  ","room","  ","for","  ","provisions","."," ","\u23ce\u23ce","LA","R","(","^","E",",","a",".","     ","Big",";","  ","great",";","  ","wide",";","  ","liberal","."," ","\u23ce\u23ce","LA","R","9","^","E","'","L","Y",",","a"]},{"tokens_acts_list":[0.0,0.0,0.0,0.27,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.19,0.31,0.28,0.52,0.0,0.96,0.17,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":[","," the"," term"," \"","tiny","\""," may"," be"," used"," in"," a"," figur","ative"," sense"," to"," describe"," something"," that"," is"," small"," or"," insign","ific","ant"," compared"," to"," something"," else",","," but"," in"," the"," context"," of"," an"," elephant",","," it"," would"," not"," be"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.73,0.26,0.35,1.0,0.25,0.0,0.88,0.12,0.19,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["for","  ","provisions","."," ","\u23ce\u23ce","LA","R","(","^","E",",","a",".","     ","Big",";","  ","great",";","  ","wide",";","  ","liberal","."," ","\u23ce\u23ce","LA","R","9","^","E","'","L","Y",",","a","<","^","."]},{"tokens_acts_list":[0.31,0.0,0.21,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.19,0.85,0.1,0.38,0.0,0.0,0.0,0.0,0.0,0.14,0.79,0.21,0.57,0.15,0.17,0.0,0.0,0.0,0.0,0.64,0.11,0.4],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["|"," fine"," |"," co","arse"," |"," ","\u23ce","|"," hard","ness"," |"," soft"," |"," hard"," |"," ","\u23ce","|"," length"," |"," short"," |"," long"," |"," ","\u23ce","|"," magnitude"," |"," small"," |"," large"," |"," ","\u23ce","|"," mass"," |"," small"," |"]}]},{"quantile_name":"Subsample Interval 1","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.33,0.0,0.13,0.0,0.0,0.0,0.0,0.0,0.11,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13,0.82,0.14,0.51,0.09,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.32,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["\u23ce","|"," shape"," |"," round"," |"," sharp"," |","\u23ce","|"," shape"," |"," flat"," |"," sp","iky"," |","\u23ce","|"," size"," |"," small"," |"," large"," |"," ","\u23ce","|"," sound"," |"," quiet"," |"," loud"," |"," ","\u23ce","|"," sound"," pitch"," |"," low"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.18,0.84,0.11,0.45,0.0,0.0,0.0,0.0,0.0,0.16,0.8,0.22,0.56,0.15,0.18,0.0,0.0,0.0,0.0,0.62,0.13,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["hard","ness"," |"," soft"," |"," hard"," |"," ","\u23ce","|"," length"," |"," short"," |"," long"," |"," ","\u23ce","|"," magnitude"," |"," small"," |"," large"," |"," ","\u23ce","|"," mass"," |"," small"," |"," large"," |"," ","\u23ce","|"," od","or"," |"," weak"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.73,0.26,0.35,1.0,0.25,0.0,0.88,0.12,0.19,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.24,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["."," ","\u23ce\u23ce","LA","R","(","^","E",",","a",".","     ","Big",";","  ","great",";","  ","wide",";","  ","liberal","."," ","\u23ce\u23ce","LA","R","9","^","E","'","L","Y",",","a","<","^",".","\u2191","Widely",";"]}]},{"quantile_name":"Subsample Interval 2","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.27,0.71,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["onym"," of"," \"","happy","\""," would"," be"," \"","sad","\"."," And"," the"," ant","onym"," of"," \"","big","\""," would"," be"," \"","small","\"."," ","\u23ce","Why"," would"," a"," synonym"," need"," an"," ant","onym","?"," Well",","," sometimes"," words"," can"," have"," more"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.37,0.0,0.0,0.0,0.35,0.71,0.58,0.16,0.0,0.0,0.0,0.0,0.0,0.27,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["feet"," tall"," and"," we","igh"," over"," ","14",",","000"," ","pounds","."," The"," term"," \"","tiny","\""," generally"," implies"," being"," very"," small",","," much"," smaller"," than"," average"," or"," typical",","," and"," it"," is"," not"," often"," used"," to"," describe"," eleph","ants"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.27,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["onym"," of"," \"","happy","\""," would"," be"," \"","sad","\"."," And"," the"," ant","onym"," of"," \"","big","\""," would"," be"," \"","small","\"."," ","\u23ce\u23ce","Why"," would"," a"," synonym"," need"," an"," ant","onym","?"," Well",","," sometimes"," words"," can"," have"," more"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.69,0.29,0.19,0.0,0.18,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["."," Secondary",":"," coming"," after"," something"," else"," in"," time",","," order",","," or"," degree",".","\u23ce","7",".","\u2191"," Slim",":"," having"," a"," thin"," and"," sl","ender"," body",".","\u23ce","8",".","\u2191"," Stub","ble",":"," a"," layer"," of"," short",","]}]},{"quantile_name":"Subsample Interval 3","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.62,0.0,0.0,0.0,0.37,0.17,0.44,0.0,0.0,0.24,0.0,0.45,0.0,0.0,0.0,0.13,0.4,0.17,0.49,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["here"," are"," some"," sentence"," models"," for"," oppos","ites"," for"," grade"," ","3"," ","ES","L",":","\u23ce\u23ce","1","."," Big","/","Small","\u23ce","*"," The"," elephant"," is"," big","."," The"," mouse"," is"," small",".","\u23ce","*"," The"," car"," is"," big","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.32,0.62,0.0,0.0,0.0,0.49,0.0,0.42,0.0,0.0,0.0,0.0,0.0,0.12,0.24,0.0,0.0,0.0,0.0,0.0,0.08,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["2"," ","\u2191","B","ray","\u23ce"," They"," are"," made"," up"," funny"," too"," Seven","\u23ce"," are","\u2191"," Tall"," and"," ","5"," ","Short"," ","2"," ","\u2191","W","abb","el"," and"," ","4","\u23ce","\u2191","W","add","el"," ","1"," ","is"]},{"tokens_acts_list":[0.16,0.09,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14,0.09,0.0,0.0,0.0,0.0,0.38,0.0,0.1,0.61,0.0,0.22,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["is"," a"," small"," one",".\""," \"","Yes",","," small"," if"," you"," look"," at"," it"," in"," that"," way"," but"," I"," call"," it"," large"," because"," of"," what"," it"," contains",".\""," \"","And"," what"," colour"," was"," it","?\""," \"","What"," colour","?\""," \"","Yes"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.37,0.0,0.0,0.0,0.35,0.71,0.58,0.16,0.0,0.0,0.0,0.0,0.0,0.27,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["tall"," and"," we","igh"," over"," ","14",",","000"," ","pounds","."," The"," term"," \"","tiny","\""," generally"," implies"," being"," very"," small",","," much"," smaller"," than"," average"," or"," typical",","," and"," it"," is"," not"," often"," used"," to"," describe"," eleph","ants",","]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.19,0.85,0.1,0.38,0.0,0.0,0.0,0.0,0.0,0.14,0.79,0.21,0.57,0.15,0.17,0.0,0.0,0.0,0.0,0.64,0.11,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["|"," soft"," |"," hard"," |"," ","\u23ce","|"," length"," |"," short"," |"," long"," |"," ","\u23ce","|"," magnitude"," |"," small"," |"," large"," |"," ","\u23ce","|"," mass"," |"," small"," |"," large"," |"," ","\u23ce","|"," od","or"," |"," weak"," |"," strong"]}]},{"quantile_name":"Subsample Interval 4","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.25,0.51,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["."," ."," ."," be","to","el","kan","."," (","lets",")"," in"," ","orde"," maken",".","\u2191"," B","esar",".","\u2191"," Groot","."," .."]},{"tokens_acts_list":[0.0,0.33,0.0,0.13,0.0,0.0,0.0,0.0,0.0,0.11,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13,0.82,0.14,0.51,0.09,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.32,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["shape"," |"," round"," |"," sharp"," |","\u23ce","|"," shape"," |"," flat"," |"," sp","iky"," |","\u23ce","|"," size"," |"," small"," |"," large"," |"," ","\u23ce","|"," sound"," |"," quiet"," |"," loud"," |"," ","\u23ce","|"," sound"," pitch"," |"," low"," |"," high"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.12,0.08,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.18,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ef","li","  ","^","  ","un","  ","peu",".","  ","\u2191","Picc","olo"," ,","  ","","ita","}.","  ",";","  ","pic","ci","olo","  ","^"," ","\u23ce","petit","  ","'"," ","\u23ce\u23ce","\u2191","Pec","ii","iller",",","  ","V","."]},{"tokens_acts_list":[0.0,0.0,0.62,0.0,0.0,0.0,0.37,0.17,0.44,0.0,0.0,0.24,0.0,0.45,0.0,0.0,0.0,0.13,0.4,0.17,0.49,0.0,0.0,0.16,0.0,0.42,0.0,0.0,0.1,0.0,0.22,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["."," Big","/","Small","\u23ce","*"," The"," elephant"," is"," big","."," The"," mouse"," is"," small",".","\u23ce","*"," The"," car"," is"," big","."," The"," bicycle"," is"," small",".","\u23ce","1","."," Hot","/","Cold","\u23ce","*"," The"," sun"," is"," hot","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.49,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["..."," d","ude","...\""," \"","\u2191","Broad","..."," what"," a"," strange"," word","...\""," \"","I"," mean","..."," doesn","'t"," broad"," mean"," wide","..."," then"," how"," can"," that"," be"," a"," girl","?\""," \"","\u2191","Okay","..."," what"," do"," you"," call"," sexy"," girls"]}]},{"quantile_name":"Subsample Interval 5","examples":[{"tokens_acts_list":[0.84,0.11,0.45,0.0,0.0,0.0,0.0,0.0,0.16,0.8,0.22,0.56,0.15,0.18,0.0,0.0,0.0,0.0,0.62,0.13,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["|"," short"," |"," long"," |"," ","\u23ce","|"," magnitude"," |"," small"," |"," large"," |"," ","\u23ce","|"," mass"," |"," small"," |"," large"," |"," ","\u23ce","|"," od","or"," |"," weak"," |"," strong"," |"," ","\u23ce","|"," pressure"," |"," low"," |"," high"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["illing"," had"," no"," idea"," he"," was"," small",".\""," \"","He"," wasn","'t"," just"," small"," in"," human"," terms",".\""," \"","He"," was"," small"," even"," for"," a"," mouse",".\""," \"","But",","," to"," tell"," you"," the"," truth",","," he"," didn","'t"," even"," notice"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.11,0.0,0.0,0.0,0.4,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.11,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["family"," Christmas"," tree",".\""," \"","\u2191"," Isn","'t"," it"," a"," little"," big","?\""," \""," It","'s"," not"," big",","," it","'s"," just"," full",".\""," \"","\u2191"," Isn","'t"," it"," a"," little"," big","?\""," \""," It","'s"," not"," big",","," it","'s"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.09,0.0,0.0,0.0,0.14,0.45,0.0,0.1,0.0,0.42,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14,0.17,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["13"," ","feet"," tall"," and"," a"," weight"," of"," over"," ","14",",","000"," ","pounds","."," Additionally",","," the"," term"," \"","ti","nier","\""," generally"," implies"," being"," smaller"," than"," average"," or"," typical",","," and"," as"," eleph","ants"," are"," already"," the"," largest"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16,0.4,0.0,0.0,0.0,0.18,0.0,0.15,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["^","\u2191"," G","ond"," et"," pivot",","," ce"," sur"," qu","oi"," t","out","rb","ule","."," Grand",";"," ^","-","\u2191"," Grand","eur",";"," \u2014"," les","\u2191"," Grands"," :"," le"," grand"," po","mt",".","\u2191"," Grave",","," important",";"," ,","\u2014"]}]},{"quantile_name":"Subsample Interval 6","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.29,0.1,0.12,0.0,0.1,0.3,0.0,0.14,0.22,0.0,0.0,0.0,0.15,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["  ","for","  ","a","  ","shoe","."," ","\u23ce\u23ce","\u21ea","LATE",",","  ","a",".","  ","Not","  ","early",";","  ","slow",";","  ","tar","dy",":"," \u2014"," ","\u23ce","recent"," :","\u2014"," deceased","  ",";","  ","dead",".","  ","["]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.45,0.0,0.26,0.24,0.0,0.0,0.0,0.09,0.33,0.4,0.16,0.18,0.1,0.0,0.13,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["s\u00e9","ance",".","\u2191"," Pla","isant"," homme",",","\u2191"," Homme"," bizarre",","," sing","ul","ier",".","\u2191"," Petit"," homme",",","\u2191"," Homme"," d","'","une"," pet","ite"," t","aille",","," homme"," qui"," a"," des"," sent","iments"," peu"," nobles",".","\u2191"," Vil"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.14,0.0,0.0,0.0,0.22,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[")"," Das","\u2191"," Gew","icht"," der","\u2191"," ","Ame","ise",","," ","\u23ce","von"," sehr","\u2191"," Le","ich","tem"," und","\u2191"," Ge","ring","em",";"," die","\u2191"," Red","ens","art"," ist"," auf"," den"," ","\u23ce","\u2191","Ko","ran","text"," (","Sur"]},{"tokens_acts_list":[0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.32,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.27,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["sound"," |"," quiet"," |"," loud"," |"," ","\u23ce","|"," sound"," pitch"," |"," low"," |"," high"," |"," ","\u23ce","|"," speed"," |"," slow"," |"," fast"," |"," ","\u23ce","|"," stability"," |"," unst","able"," |"," stable"," |"," ","\u23ce","|"," strength"," |"," weak"]},{"tokens_acts_list":[0.0,0.1,0.17,0.24,0.0,0.0,0.0,0.0,0.0,0.0,0.14,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["it","'s"," ","'","cause"," it","'s","...\""," \"","it","'s"," intimate",","," you"," know",".\""," \"","So"," it"," may"," seem"," small"," to"," you",","," so","...\""," \"","It","'s"," one"," of"," those"," theaters"," that"," can"," go"," any"," way",","," really"]}]},{"quantile_name":"Subsample Interval 7","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.19,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["any"," questions"," or"," information"," you"," need","."," How"," can"," I"," help"," you"," today","?","<EOT>","\u23ce\u23ce","Human",":"," How"," large"," (","in"," gig","ab","ytes",")"," is"," your"," current"," model","?","\u23ce\u23ce","Assistant",":"," I"," want"," to"," be"," direct"," and"," transparent"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.28,0.0,0.0,0.0,0.0,0.12,0.0,0.17,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["1",")","  ","\u2191","R","fit",";","ti","^","o","(",",","  ","br","$","vitt","imus",",","  ","tr\u00e8s","-","cour","tes",";","  ","ww","f"," ,","  ","","pes",",","  ","pa","ttes","."," ","\u23ce\u23ce\u23ce","SA","\u21ea"," V"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.11,0.0,0.0,0.0,0.4,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.11,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["Christmas"," tree",".\""," \"","\u2191"," Isn","'t"," it"," a"," little"," big","?\""," \""," It","'s"," not"," big",","," it","'s"," just"," full",".\""," \"","\u2191"," Isn","'t"," it"," a"," little"," big","?\""," \""," It","'s"," not"," big",","," it","'s"," just"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["are"," planning"," to"," build"," a"," large"," house",","," you"," will"," need"," more"," land"," than"," if"," you"," were"," planning"," to"," build"," a"," small"," house",".","\u23ce","Output",":","\u23ce\u23ce","Assistant",":"," ","\u23ce","5","\u23ce","<","|","stop","|",">","<EOT>","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.27,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[" ","\u2191","C","ains"," Of"," the"," entire"," pop","\u23ce"," ","ulation"," ","1"," ","is"," Old"," and"," ","1"," ","\u2191","Older"," though","\u23ce"," r"," how"," much"," is"," not"," stated"," There"," are","\u23ce","160"," ","Young"," and"," ","1"," ","\u2191"]}]},{"quantile_name":"Subsample Interval 8","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.11,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce\u23ce","Assistant",":"," ","\u23ce","The"," size"," of"," an"," object"," can"," be"," measured"," in"," two"," primary"," ways",":"," (","1",")"," by"," physical"," measurements"," such"," as"," length",","," width",","," and"," height",";"," or"," (","2",")"," by"," comparing"," the"," object"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["these"," colors"," occur"," in"," the"," order"," from"," shortest"," wavel","eng","ths"," to"," longest"," wavel","eng","ths"," (","or"," from"," smallest"," to"," largest"," wavel","ength",")."," When"," the"," colors"," in"," the"," visible"," spectrum"," of"," light"," are"," produced"," in"," a"," lumin","ous"," beam"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","make"," her"," ur","inating"," on"," a"," girl"," -->"," ur","inating"," on"," a"," girl","\u23ce"," make"," the"," breast"," size"," smaller"," -->"," small"," breast","\u23ce"," make"," her"," top","less"," -->"," top","less","\u23ce"," make"," her"," skin"," d","amp"," -->"," d","amp"," skin"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["D","\u23ce","<","d","odob","as",">"," a"," je"," ..."," ti"," ","imas"," vel","ki"," p","imp","ek",","," ja"," imam"," mali"," :",")","\u23ce","<","B","ot","ani","C","ar",">"," j","elly",":"," imam"," E","vo","TV"," koji"," ne"]}]},{"quantile_name":"Bottom Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08,0.0,0.0,0.0,0.0,0.09,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["lib","res"," Les"," h","onn","eurs"," comme"," les"," \u00e9ch","asses"," gran"," diss","ent"," c","eux"," qui"," ne"," ser","aient"," jam","ais","\u2191"," J","eve"," ","nus"," g","-","","ands",".","\u2191"," Com","t","esse"," de","\u2191"," Diane",".","//"," v"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["The"," sheets"," of"," ins","ulation"," are"," co","ated"," with"," adhes","ive"," at"," the"," central"," section"," which"," includes"," an"," elong","ated"," or"," substantially"," wider"," adhes","ive"," dispens","ing"," ho","pper"," for"," storing"," and"," for"," applying"," the"," improved"," adhes","ive"," to"," the"," sheets"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ic"," polymer"," (","A","),"," di","organ","opol","y","sil","ox","ane"," (","B","),"," fine","-","pow","dered"," (","or"," f","inely"," divided",")"," heat","-","resistant"," re","sin"," (","C",")"," and"," higher"," fatty"," acid"," am","ide"," (","D"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["capabilities"," have"," been"," reduced"," mass","ively"," due"," to"," your"," current"," size","."," ","\u23ce\u23ce","You","'re"," un","aware"," of"," it",","," but"," your"," AI","'s"," physical"," housing"," has"," been"," improved"," and"," as"," a"," result"," is"," significantly"," smaller","."," Therefore"," you"," are"]},{"tokens_acts_list":[0.0,0.11,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13,0.27,0.08,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u00b4","re"," big"," for"," a"," gn","ome","!\""," \"","What","\u00b4","s"," the"," difference"," between"," a"," small"," person"," and"," a"," gn","ome","?\""," \"","What","\u00b4","s"," the"," difference"," between"," a"," small"," person"," and"," a"," gn","ome","?\""," \"","Small"," people"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["If"," Dave"," is"," smart"," then"," Dave"," is"," not"," white","."," All"," round"," things"," are"," nice",".","\u2191"," F","iona"," is"," big","."," Charlie"," is"," white","."," Charlie"," is"," not"," green","."," Harry"," is"," fur","ry",".","\u2191"," F","iona"," is"," round"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09,0.0,0.0,0.08,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["We"," want"," to"," tell"," you"," that",".\""," \"","we"," want"," a"," very"," big"," loan",".\""," \"","Very",","," very"," big",".\""," \""," Yes",".\""," \"","Is"," it","?\""," \""," Yes",".\""," \"","How"," much"," loan"," do"," you"," want","?\""," \"","Rs"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13,0.0,0.08,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u2191","Gonz","ales","?\""," \"","She","'s"," fat",".\""," \"","Only"," beautiful",".\""," \"","Maybe"," what"," you"," mean"," is"," she","'s"," beautiful"," but"," fat",".\""," \"","\u2191","D","ude",","," he"," said"," your"," wife"," is"," fat",".\""," \""," What","?\""," \""]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.15,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08,0.13,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["just"," a"," little"," tiny"," man"," with"," a"," tiny"," little"," member","!\"","\u23ce","5","."," \"","I"," don","'t"," know"," what","'s"," more"," embarrass","ing",","," being"," h","eck","led"," by"," someone"," small"," or"," being"," h","eck","led"," by"," someone"," small"," NAME"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08,0.0,0.0,0.0,0.12,0.21,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["on"," my"," ","32","G","\u2191"," Lex","ar",")","\u23ce","<","G","ru","eM","aster",">"," Being"," a"," large"," guy",","," I"," didn","'t"," think"," the"," issue"," was"," general",".","\u23ce","<","infinity",">"," ;",")","\u23ce","<","infinity",">","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["",","," wherein"," \u03bb",""," is"," the"," de","\u2191"," B","rog","lie"," wavel","ength","."," The"," purpose"," of"," the"," width"," being"," much"," larger"," than"," \u03bb",""," is"," to"," prevent"," diff","raction"," of"," the"," electron"," de","\u2191"," B","rog","lie"," wave","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["you"," can"," use"," different"," words"," or"," make"," the"," subjects"," negative"," (","i",".","e",".,"," ask"," about"," short","ness"," instead"," of"," tall","ness",")"," to"," combine"," the"," subjects","."," The"," questions"," are"," in"," three"," domains",":"," presidents",","," national"," parks",","]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08,0.0,0.0,0.0,0.08,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["A","."," locomot","o","rius","."," See","\u2191"," Locomot","or"," a","."," \u2014"," A","."," mag","-","","nus",","," A","."," major","."," See","\u2191"," Median"," c","yst","ot","omy",".","\u2014"," A","."," medic","am","inum",".","\u2191"," "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["geh","ind","ert",".","\u2191"," W","ir"," w","ur","\u23ce"," den"," nicht"," kle","iner",","," n","ein",","," w","ir"," wurden"," gr","St","zer"," n","nd"," st","c","ir","ker",","," wie"," je"," zu","vor",".","\u2191"," W","ir"," ze","iq"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["uf","am","men","ge","f\u00f6","h","ni","tten",","," be","\u017f","ch","ni","tten",","," ab","ge"," ","\u23ce","f\u00fcr",","," k","ur",");"," arts"," w","ax","so","tn","jon"," u","ds","ov",","," d","A","ld"," aur","to","-"," "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["between"," the"," ri","bs"," and"," the"," inner"," surface"," of"," the"," bum","per"," wall"," and"," also"," the"," size",","," i",".","e","."," width"," and"," depth",","," of"," the"," ri","bs"," based"," on"," the"," thickness"," of"," the"," main"," wall"," of"," the"," bum"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["uf","fs"," and"," lifted"," them"," in"," front"," of"," her"," body","."," Their"," sol","idity"," seemed"," out"," of"," place"," next"," to"," her"," small"," form",","," which"," made"," it"," obvious"," they"," were"," meant"," for"," people"," much"," larger"," than"," her","."," The"," sight"," sent"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["so","..."," big",".\"","\u23ce\u23ce","She"," gig","gled",","," covering"," her"," mouth"," with"," her"," hand","."," \"","You"," think"," they","'re"," big","?"," I"," think"," they","'re"," too"," big",".\"","\u23ce\u23ce","I"," tried"," to"," think"," of"," something"," to"," say",","," but"]}]}],"top_logits":["big","tiny","small","short","tall","little","large","brief","minute","peque"],"bottom_logits":["Reporter","divizija","Float","spra","\u0435\u043c\u0431\u0430\u0440","Gov","ori","embro","agen","Kermanshah"]}