{"index": 11214495, "examples_quantiles": [{"quantile_name": "Top Activations", "examples": [{"tokens_acts_list": [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "Charles", "", " C", "lu", "-", "n", "ais", ",", " v", "ulg", "a", "irement", " con", "nu", " sous", " le", " nom", " de", " g", "ars", ""]}, {"tokens_acts_list": [0.0, 0.99, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "Session", "\u23ce", " layer", " is", " inher", "ently", " _", "useful", "_", "\u23ce\u23ce", "UDP", "?", "", " Schm", "oo", "", " Dee", "", " P", "ee", "."]}, {"tokens_acts_list": [0.0, 0.98, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "Daniel", ".\"", " \"", "I", " can", "'t", " talk", " now", ".\"", " \"", "The", " customer", " wants", " him", " but", " Daniel", " does", " not", " meet", " the", " qual"]}, {"tokens_acts_list": [0.0, 0.98, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "Robert", " came", " bo", "mt", " with", "\u23ce", " him", " Sunday", " to", " go", " fishing", " Monday", ".", "\u23ce", "J", ".", " J", ".", "", " Quin", "lan"]}, {"tokens_acts_list": [0.0, 0.97, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "April", " .", " Die", "", " Ein", "nah", "me", " von", " (", " k", "ions", " )", " im", "", " Sch", "rim", "mer", "", " Kre", "ise", " ,"]}, {"tokens_acts_list": [0.0, 0.97, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "James", ",", " of", " West", " Brighton", ",", " S", ".", "I", ".", " No", " cards", ".", "", " DIED", ".", " Anderson", ".", "?", "In", " Brooklyn"]}, {"tokens_acts_list": [0.0, 0.97, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "James", "147", ":", " er", "go", " I", " want", " to", " make", " sure", " I", " have", " secure", " back", "ups", " in", " place", " before", " I", " start", " al"]}, {"tokens_acts_list": [0.0, 0.96, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "July", " wore", " so", " ", "sht", "ill", " om", "", " Ha", "usa", "\u23ce", " I", "l", "Ar", "ri", "ik", " dm", " der", " American", "", " Aw"]}, {"tokens_acts_list": [0.0, 0.95, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "International", " League", " the", " Syracuse", " team", " leads", ",", " with", " Detroit", ",", " Toledo", ",", " Toronto", ",", " Rochester", ",", " London", ",", " Hamilton", " and", " Buffalo"]}, {"tokens_acts_list": [0.0, 0.95, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "International", "", " Lon", "gs", "ho", "rem", "ens", "\u23ce", " Association", ".", "\u23ce", "\"", "The", " membership", " of", " the", " mari", "\u23ce", " time", " unions", ",\""]}, {"tokens_acts_list": [0.0, 0.95, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "J", "et", "Trans", "l", "ator", "Base", "(", "const", " std", "::", "string", "&", " ud", "sc", "File", ",", "\u23ce", "                ", "                ", "               ", "const"]}, {"tokens_acts_list": [0.0, 0.94, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "Union", ".", "S", "'", "ad", "ress", "ant", " aux", " diss", "idents", " de", " la", " gau", "che", ",", " M", ".", " de", "", " Fre", "yc"]}, {"tokens_acts_list": [0.0, 0.94, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "William", " S", ".", " Thompson", ".", "\u23ce", "Edwin", " B", ".", "", " Z", "ol", "gler", ",", "", " Ella", " M", ".", "", " Ze", "isf"]}, {"tokens_acts_list": [0.0, 0.94, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "William", "  ", "Smith", ",", " ", "\u23ce\u23ce", "es", "q", ".", "  ", "of", "  ", "", "A", "ston", "  ", "", "Flam", "ville", ",", "  "]}, {"tokens_acts_list": [0.0, 0.94, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "D", "ll", "Struct", "Create", "('", "byte", "[", "32", "]", "'),", " $", "a", "R", "et", "\u23ce", "\u21b9", " Local", " $", "", "DAC", "L"]}, {"tokens_acts_list": [0.0, 0.94, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "D", "U", "", " PR\u00c9", "F", "ET", " ", "\u23ce", "Dans", " sa", " s\u00e9", "ance", " du", " ", "28", " ", "octobre", " ", "1", "934", ","]}, {"tokens_acts_list": [0.0, 0.94, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "O", "B", " FO", "B", "\u23ce", " Business", "", " Purposes", ".", "\u23ce", "W", "M", ".", " C", "*", " DO", "W", ".", "No", ",", " U"]}, {"tokens_acts_list": [0.0, 0.93, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "Mission", " zur", "", " Regener", "ation", " der", "", " M", "ensch", "heit", " ,", " w", "of", "\u00fcr", " ich", " j", "etzt", " wir", "ke", " ,", " war"]}, {"tokens_acts_list": [0.0, 0.93, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "Pierre", ").", "", " COUSIN", " (", "Philippe", ").", " ", "\u23ce", "\u2014", "", " H", "\u00f4", "p", "ital", " Nord", " :", " nouv", "elles", " /", " Jean"]}, {"tokens_acts_list": [0.0, 0.93, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "Todd", "\u23ce", " of", "", " Wol", "c", "ott", ",", " when", " the", " latter", " was", " sitting", "\u23ce", " j", "n", " the", " Clayton", " will", " case", "."]}]}, {"quantile_name": "Subsample Interval 0", "examples": [{"tokens_acts_list": [0.0, 0.91, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "Office", " and", " after", ".", " ", "335", " ", "", "SECOND", "", " STREET", ",", " Corner", " of", " Union", " and", " Second", ".", " In", " addition", " to"]}, {"tokens_acts_list": [0.0, 0.91, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "X", "P", " on", " this", " machine", "...", " and", "...", " and", "...", " Im", " an", " AO", "L", "er", " ", ":(", "\u23ce", "<", "Z", "ip"]}, {"tokens_acts_list": [0.0, 0.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "Pat", ".", " No", ".", " ", "4", ",", "534", ",", "781", ")", " teaches", " an", " improved", " nutri", "ent", " supplement", " comprising", " a", " partic", "ulate"]}, {"tokens_acts_list": [0.0, 0.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "R", "ain", "C", "T", ">", " E", "a", "gle", "Screen", ":", " perhaps", " you", " have", " subsc", "ribed", " to", " bug", "mail", " for", " some", " package"]}, {"tokens_acts_list": [0.0, 0.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "Pat", ".", " No", ".", " ", "4", ",", "709", ",", "479", " ", "dis", "cl", "oses", " an", " apparatus", " which", " util", "izes", " a", " vert"]}]}, {"quantile_name": "Subsample Interval 1", "examples": [{"tokens_acts_list": [0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "Iron", " A", " Steel", "\u23ce", " Royal", " Dutch", ".", " N", " Y", "_", " ", "196", " ", "108", " ", "i", "Z", "\u23ce", "", " Se", "a"]}, {"tokens_acts_list": [0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "Machine", " Learning", " Engineers", " (", "experience", " with", " Python", " +", "\u23ce", "T", "ensor", "Flow", " required", ").", "\u23ce\u23ce", "Please", " reach", " out", " to", " hiring", "@"]}, {"tokens_acts_list": [0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "Windows", "?", " I", "'m", " confused", ".", "\u23ce", "<", "bj", "sn", "ider", ">", " there", " is", " a", " linux", " kernel", ",", " and", " a", " windows"]}, {"tokens_acts_list": [0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "Springs", ".", "\u23ce", "On", " his", " own", " ver", "anda", ",", "", " Deput", "j", "r", " Sheriff", "\u23ce", " J", ".", " M", ".", "", " Brook"]}, {"tokens_acts_list": [0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "Vice", " President", " and", " Senior", " Operations", " Officer", "\u23ce", "", " Shareholders", " Information", " -", " --------------", "----------------------------------------------------------------", "\u23ce", "Annual", " and", " Other", " Reports", "\u23ce", " The", " Company"]}]}, {"quantile_name": "Subsample Interval 2", "examples": [{"tokens_acts_list": [0.0, 0.7, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "Crown", "", " Vic", " go", ".\"", " \"", "It", " served", " the", " country", " well", ".\"", " \"", "And", " I", " got", " really", " good", " at", " spot", "ting"]}, {"tokens_acts_list": [0.0, 0.7, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "Ad", ".)", " ", "\u23ce\u23ce", "276", ".", " Mit", ".", "", " Fol", "lis", ",", " foll", "us", " =", " super", "bus", ",", " van", "ys", ","]}, {"tokens_acts_list": [0.0, 0.7, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "Card", "inal", "F", "ang", ">", " aqu", "arius", ",", " I", " saw", " a", " python", "-", "key", "ring", " module", " this", " morning", ".", "  ", "I"]}, {"tokens_acts_list": [0.0, 0.7, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "Iowa", " are", " held", " at", " ", "4", "ia", "4", "", "Sc", "per", " bn", ".", "\u23ce", "", "BACON", "", " Th", "ero", " is", " not"]}, {"tokens_acts_list": [0.0, 0.7, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "Large", "", " Mart", " s", "nat", "ched", " Big", " Mike", ",", " he", " was", " able", " to", " lib", "erate", " himself", " but", " they", " still", " tor", "ched"]}]}, {"quantile_name": "Subsample Interval 3", "examples": [{"tokens_acts_list": [0.0, 0.61, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "C", "-", "corp", ".", " --", " we", "'re", " talking", " LO", "L", ".", "\u23ce\u23ce", "Does", " that", " situation", " happen", " very", " often", " yet", "?", ""]}, {"tokens_acts_list": [0.0, 0.61, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "C", ".", "sub", ".", "1", " ", "-", "C", ".", "sub", ".", "5", " ", "alk", "yl", ",", " C", ".", "sub", ".", "1"]}, {"tokens_acts_list": [0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "Mom", "-", "\u23ce", "Terms", " of", " District", " Court", " for", " Webster", "\u23ce", " County", " for", " the", "", " Tear", " ", "1", "911", ".", "\u23ce", "I"]}, {"tokens_acts_list": [0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "II", " also", " read", " it", " for", " you", ".\"", " \"\"", "", "Ra", "je", "ev", ".", ".\"", "\"", " \"'", "You", " used", " to", " say", " that"]}, {"tokens_acts_list": [0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "Sin", " ", "28", ":", " Januar", " ", "181", "S", ".", " K", "U", "ji", "eM", "M", "/", " f", "S", "r", " \u00bb", "\u00f6", "\u00f6"]}]}, {"quantile_name": "Subsample Interval 4", "examples": [{"tokens_acts_list": [0.0, 0.51, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "Japanese", "", " Un", "ex", "am", "ined", " Patent", " Application", " Publication", " No", ".", " ", "2", "008", "-", "76", "105", "A", " dis", "cl", "oses"]}, {"tokens_acts_list": [0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "God", " is", " he", " that", " ju", "\u017f", "t", "ific", "th", ":", " who", " is", " he", " that", " can", " con", "-", " dem", "n", "?", " Rom"]}, {"tokens_acts_list": [0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "N", "-", "terminal", " pept", "ide", " fragment", ",", " the", " pre", "-", "fragment", " (", "signal", " pept", "ide", ")", " is", " cle", "aved", " off", " to"]}, {"tokens_acts_list": [0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "N", "\u00b0", " ", "14", "261", ".", " niss", "ant", " les", " l", "ieux", " lo", "u\u00e9s", " av", "aient", " \u00e9t\u00e9", " sa", "isis", " et", " vend", "us"]}, {"tokens_acts_list": [0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "N", "'", "up", "\\xc3", "\\x8f", "T", "^", ".'", " i", "ii", "iii", " >", "", "rii", "", " Ic", "ii", "it", "-", "ni", " ?"]}]}, {"quantile_name": "Subsample Interval 5", "examples": [{"tokens_acts_list": [0.0, 0.41, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "Thank", " you", " very", " much", ".\"", " \"", "", "Doesn", "'t", " happen", " often", "...", " but", " when", " it", " does", ",\"", " \"", "I", " like", " to"]}, {"tokens_acts_list": [0.0, 0.41, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "Under", " and", " by", " virtue", " of", " the", " power", "\u23ce", " and", " authority", " v", "ested", " m", " the", " unders", "ign", "\u23ce", " ed", " trust", "ee", " by"]}, {"tokens_acts_list": [0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "Un", " terrain", " bien", " sit", "u\u00e9", " sera", " to", "uj", "ours", " une", "", " B", "ONNE", "", " AFF", "AIRE", "", " Voy", "ez", " les", " m"]}, {"tokens_acts_list": [0.0, 0.39, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "Have", " a", " good", " day", ".\"", " \"", "Remember", " to", " drive", " safely", ".\"", " \"", "Sorry", "!\"", " \"", "lt", "'s", " hitting", " the", "--", " -"]}, {"tokens_acts_list": [0.0, 0.39, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "Have", " your", " say", "'", " webpage", ".", "\u23ce", "\u2022", "Collection", " and", " use", " of", " expertise", "\u23ce\u23ce\u23ce\u23ce", " To", " prepare", "", " Regulation", " (", "EU", ")"]}]}, {"quantile_name": "Subsample Interval 6", "examples": [{"tokens_acts_list": [0.0, 0.31, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "Is", ",", "  ", "at", "  ", "an", "  ", "additional", "  ", "expense", "  ", "of", "  ", "near", "  ", "half", "  ", "a", "  ", "million", " "]}, {"tokens_acts_list": [0.0, 0.31, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "Take", " My", " Staff", " and", " Meet", " Me", " for", " That", " Him", " Making", "", " Believes", " of", " the", "", " C", "rip", "pling", " Economic", " Effects", " of"]}, {"tokens_acts_list": [0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "IN", ",", "", " GL", "OX", "IOS", ".", " MA", "G", ".", "", " PA", "TR", ".", "", " VES", "TIG", ".", " PR", "O", " "]}, {"tokens_acts_list": [0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "IN", " IN", ".", " L", "\u23ce", " forming", " my", " customers", " and", " the", " public", " v", "\u23ce", " ", "ener", "ally", ",", " that", " ", "1", " "]}, {"tokens_acts_list": [0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "http", "://", "j", ".", "mp", "/", "j", "uju", "-", "florence", " http", "://", "j", ".", "mp", "/", "j", "uju", "-", "docs", " http"]}]}, {"quantile_name": "Subsample Interval 7", "examples": [{"tokens_acts_list": [0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "IG", ".", " ", "4", ".", "", " Namely", ",", " blue", " light", " ", "105", " ", "(", "F", "IG", ".", " ", "4", ")", " is"]}, {"tokens_acts_list": [0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "IG", ".", " ", "4", ",", " no", " phase", " change", " has", " occurred", " in", " each", " sh", "aded", " bio", " spot", " ", "13", " ", "a", " and"]}, {"tokens_acts_list": [0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 2, "is_repeated_datapoint": false, "tokens": ["<EOT>", "_", "Scale", ":", " {", "x", ":", " ", "1", ",", " y", ":", " ", "1", "}", " m", "_", "", "Offset", ":", " {", "x", ":"]}, {"tokens_acts_list": [0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "Our", " interview", " process", "\u23ce", " is", " a", " first", " call", " to", " go", " through", " programming", " compet", "ence", ",", " as", " well", " as", " a", " follow", " up"]}, {"tokens_acts_list": [0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "Of", "", " Continuous", " Pin", " And", " Post", "", " Terminals", "\",", " which", " was", " presented", " at", " the", "", " Connector", " And", "", " Interconn", "ection", " Technology"]}]}, {"quantile_name": "Subsample Interval 8", "examples": [{"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.06, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["\u23ce\u23ce", "West", "  ", "", "Fir", "le", " ", "\u23ce", "", "W", "esth", "amp", "n", "ett", "  ", "-", " ", "\u23ce\u23ce", "18", " ", "\u23ce\u23ce", "5", " ", "\u23ce\u23ce", "13", " ", "\u23ce\u23ce", "12", " ", "\u23ce\u23ce", "6", " ", "\u23ce\u23ce", "3", " ", "\u23ce\u23ce", "18", " ", "\u23ce\u23ce", "38", " "]}, {"tokens_acts_list": [0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "Hz", "),", " ", "6", ".", "79", " ", "(", "2", "H", ",", " m", "),", " ", "7", ".", "0", "-", "7", ".", "15"]}, {"tokens_acts_list": [0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "varchar", "(", "100", ")", " NULL", ",", "\u23ce\u23ce", "", "Column", "Number", " small", "int", ",", "\u23ce\u23ce", "Amount", " float", "\u23ce\u23ce", " In", " this", " example", ","]}, {"tokens_acts_list": [0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "mainstream", ".", "\u23ce\u23ce", "~~~", "\u23ce", "tim", "ri", "chard", "\u23ce", " Going", " out", " of", " your", " way", " to", " say", " it", "?", "\u23ce\u23ce", "People", " act"]}, {"tokens_acts_list": [0.0, 0.09, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "With", " a", " \"", "K", "\"", " at", " the", " end", ".\"", " \"", "A", " k", "icking", " \"", "K", "\".", " \"", "K", "\",", "", " K"]}]}, {"quantile_name": "Bottom Activations", "examples": [{"tokens_acts_list": [0.0, 0.07, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "assistant", " recording", " secretary", ",", " Mrs", ".", " L", ".", " E", ".", " Smith", ",", " Seattle", ";", " treasurer", ",", " Miss", "", " Sa", "die", ""]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 17, "is_repeated_datapoint": false, "tokens": ["<EOT>", ")", " will", " be", " released", " as", " free", " and", " open", " source", " software", " (", "", "F", "OSS", ").", "<EOT>", "Project", " Summary", "/", "Abstract", "", " Chronic", "", " Ob", "struct", "ive", "", " Pul", "mon", "ary", " Disease", " (", "", "COP", "D", ")", " is"]}, {"tokens_acts_list": [0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "TO", " and", " formed", " on", " the", " pass", "iv", "ation", " film", " ", "110", ",", " a", " di", "electric", " layer", " ", "112", " ", "formed", " on"]}, {"tokens_acts_list": [0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "engl", "is", "r", "lu", "  ", "h", "  ", "", "Stal", "ii", "  ", "i", "M", "-", "n", "  ", "in", "  ", "dem", "-", " "]}, {"tokens_acts_list": [0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "TA", ",", " nu", "eT", "O", "E", " val", "ses", "", " D", "pts", ".", " los", " mismo", "\u00bb.", " I", ")", "ir", "\u00ed", "{", "("]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": [".", " Williams", " would", " bring", " forward", " the", " results", " of", " a", " ", "\u23ce", "practical", " experience", " which", " few", " or", " none", " of", " his", " contempor", "aries", " perhaps", " ", "\u23ce", "could", " equal", ",", " was", " to", " be", " expected", ",", " and", " it", " was", " to", " be", " expected", ",", " moreover", ","]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["t", "ho", " headqu", "ar", "\u23ce", " t", "ere", " of", " the", " railroad", " offices", " in", " this", " city", "\u23ce", " I", " relative", " to", " the", " taking", " of", " every", " pre", "\u23ce", "1", " ", "ca", "ution", " necessary", " In", " the", " railroad", "\u23ce", " r", " p", " yards", " against", " fires", " on", " t", "ho"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.06, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["", "Fir", "le", " ", "\u23ce", "", "W", "esth", "amp", "n", "ett", "  ", "-", " ", "\u23ce\u23ce", "18", " ", "\u23ce\u23ce", "5", " ", "\u23ce\u23ce", "13", " ", "\u23ce\u23ce", "12", " ", "\u23ce\u23ce", "6", " ", "\u23ce\u23ce", "3", " ", "\u23ce\u23ce", "18", " ", "\u23ce\u23ce", "38", " ", "\u23ce\u23ce", "11", " "]}, {"tokens_acts_list": [0.0, 0.28, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 2, "is_repeated_datapoint": false, "tokens": ["<EOT>", "", "Cor", "res", "pond", "ing", "Source", "Object", ":", " {", "file", "ID", ":", " ", "0", "}", "\u23ce", "  ", "m", "_", "P", "ref", "ab"]}, {"tokens_acts_list": [0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "dinner", " already", " as", "ac", "?", "\u23ce", "<", "B", "U", "G", "ab", "undo", ">", " its", " like", " ", "3", "pm", "\u23ce", "<", "as"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["", "Rust", ":", "\u23ce", "```", "\u23ce", "//", " Define", " a", " struct", " for", " the", " union", "\u23ce", " struct", " Un", "ion", "Key", "board", " {", "\u23ce", "    ", "a", ",", " b", ",", " c", ",", " d", ",", " e", ",", " f", ",", " g", ",", " h", ",", " i", ",", " j"]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.06, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": [" ", "\u23ce", "", "W", "esth", "amp", "n", "ett", "  ", "-", " ", "\u23ce\u23ce", "18", " ", "\u23ce\u23ce", "5", " ", "\u23ce\u23ce", "13", " ", "\u23ce\u23ce", "12", " ", "\u23ce\u23ce", "6", " ", "\u23ce\u23ce", "3", " ", "\u23ce\u23ce", "18", " ", "\u23ce\u23ce", "38", " ", "\u23ce\u23ce", "11", " ", "\u23ce\u23ce", "16", " "]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["nothing", "\"", " \"", "There", " is", " no", " thing", " that", " is", " interested", " in", "\"", " \"", "Do", " not", " know", " nothing", " can", " be", " remembered", "\"", " \"", "You", " have", " talked", " about", " the", " announ", "cer", " before", " this", " It", " is", " a", " very", " good", " job", " Help", " me", " to", " look"]}, {"tokens_acts_list": [0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 2, "is_repeated_datapoint": false, "tokens": ["<EOT>", " ", "", "MADRID", ".", "\u2014", "\u00ednd", "ice", " de", " los", " habitantes", " de", " Madrid", " por", " orden", " alfab", "\u00e9tico", " de", " ", "ape", "Man", "uel", ""]}, {"tokens_acts_list": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 20, "is_repeated_datapoint": false, "tokens": ["\"", "Very", " terrible", " you", " have", " studied", "\"", " \"", "Do", " not", " have", "\"", " \"", "I", " still", " thought", " you", " did", " the", " repair", "\"", " \"", "The", " money", "?\"", " \"", "Very", " not", " all", " right", " like", " this", "\"", " \"", "This", " is", " the", " tip", " that", " a", " guest"]}, {"tokens_acts_list": [0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "rescue", " the", " lon", "gev", "ity", "\u23ce", " in", " the", " deletion", ",", " and", " even", " more", " precisely", ",", " only", " in", " ad", "ult", "hood", " ("]}, {"tokens_acts_list": [0.0, 0.06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "recruiting", " fees", " are", " well", " known", " in", " the", " industry", ".", " But", " I", " suppose", " if", "\u23ce", " you", " were", " not", " a", " technical", " manager", " ("]}, {"tokens_acts_list": [0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "devices", ",", " however", ",", " have", " all", " the", " serious", " draw", "backs", " of", " requires", " a", " certain", " effort", " by", " the", " user", " for", " adjust", "ing"]}, {"tokens_acts_list": [0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "devices", " to", " adapt", " in", " other", " ways", ".", " For", " example", ",", " a", " prolong", "ed", " period", " of", " cooling", " may", " be", " sens", "ed", " and"]}, {"tokens_acts_list": [0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "train_token_ind": 1, "is_repeated_datapoint": false, "tokens": ["<EOT>", "devices", " to", " communicate", " with", " each", " other", " adding", " to", " the", " mobility", " of", " users", ".", " A", " computing", " device", ",", " such", " as", " a", " Personal"]}]}], "top_logits": ["", "am", "figure", "photo", "article", "image", "assistant", "border", "ID", "american"], "bottom_logits": ["svetovne", "Over", "angle\u0161ki", "damages", "evenimente", "list\u0103", "\u0933", "TA", "IN", "nonetheless"]}