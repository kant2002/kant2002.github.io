{"index":11748620,"examples_quantiles":[{"quantile_name":"Top Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.45,0.0,0.0,0.0,0.19,0.25,0.38,0.25,0.0,0.54,0.72,1.0,0.0,0.0,0.0,0.79,0.0,0.0,0.0,0.0,0.0,0.39,0.0,0.0,0.0,0.0,0.0,0.0,0.33,0.0,0.56,0.74,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["'s"," the"," opposite"," of"," ugly","?","\u23ce\u23ce","Assistant",":"," The"," opposite"," of"," \"","ugly","\""," is"," \"","beautiful","\""," or"," \"","attractive",".\"","\u23ce\u23ce","Human",":"," ","\u23ce\u23ce","What","'s"," the"," opposite"," of"," \"","big","\"","?","\u23ce\u23ce","Assistant",":"," The"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.19,0.0,0.0,0.78,0.0,0.0,0.0,0.0,0.0,0.0,0.71,0.0,0.0,0.06,0.0,0.38,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.22],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["onym"," of"," a"," word"," is"," a"," word"," opposite"," in"," meaning"," to"," it",".","\u23ce","Input",":"," complex",".","\u23ce","Output",":","\u23ce\u23ce","Assistant",":"," Here","'s"," the"," ant","onym"," for"," \"","complex","\":","\u23ce\u23ce","simple","\u23ce\u23ce"," The"," word"," \"","simple","\""]},{"tokens_acts_list":[0.34,0.0,0.0,0.08,0.0,0.35,0.0,0.0,0.0,0.02,0.0,0.0,0.22,0.0,0.61,0.0,0.21,0.0,0.0,0.27,0.79,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.63,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":[" ","\\xeb","\\xad","\\x90","\uc57c","?","\u23ce\u23ce","Assistant",":"," \"","\ub9cc","\uc871","\"","\uc758"," ","\ubc18","\ub300","\ub9d0","\uc740"," ","\"","\ubd88","\ub9cc","\uc871","\""," ","\ub610","\ub294"," ","\"","\ubd88","\ub9cc","\"","\uc785","\ub2c8","\ub2e4","."," ","\u23ce\u23ce","-"," "]},{"tokens_acts_list":[0.0,0.13,0.44,0.14,0.08,0.0,0.26,0.6,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.62,0.0,0.04,0.0,0.5,0.77,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["onym"," of"," \"","happy","\""," would"," be"," \"","sad","\"."," And"," the"," ant","onym"," of"," \"","big","\""," would"," be"," \"","small","\"."," ","\u23ce\u23ce","Why"," would"," a"," synonym"," need"," an"," ant","onym","?"," Well",","," sometimes"," words"," can"," have"," more"]},{"tokens_acts_list":[0.0,0.17,0.43,0.15,0.1,0.0,0.27,0.59,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.64,0.0,0.03,0.0,0.49,0.76,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["onym"," of"," \"","happy","\""," would"," be"," \"","sad","\"."," And"," the"," ant","onym"," of"," \"","big","\""," would"," be"," \"","small","\"."," ","\u23ce","Why"," would"," a"," synonym"," need"," an"," ant","onym","?"," Well",","," sometimes"," words"," can"," have"," more"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12,0.34,0.74,0.0,0.59,0.33,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","\"","synonym","s","\":"," [\"","leaf","\","," \"","sheet","\","," \"","paper","\"],","\u23ce","\"","ant","ony","ms","\":"," [\"","screen","\","," \"","proj","ector","\"]","\u23ce","},"," {","\u23ce","\"","definition","\":"," \"","A"," written"," or"," printed"," document"," consisting"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.64,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["these"," c","olds","."," Could"," you"," explain"," more"," how"," each"," of"," them"," differs"," to"," help"," me"," better"," remember"," their"," ant","ony","ms","?","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," breakdown"," of"," some"," key"," synonym","s"," for"," ","'","plac","ate","'"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.58,0.0,0.0,0.0,0.0,0.0,0.0,0.63,0.0,0.69,0.0,0.0,0.16,0.3,0.08,0.62,0.0,0.28,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["b","aff","ling"," conf","using"," and"," bew","il","dering"," and"," ant","ony","ms"," for"," in"," compreh","ens","ible"," are"," the"," words"," under","stand","able"," clear"," plain"," and"," intellig","ible","."," So"," that"," was"," incomp","reh","ens","ible"," our"," first"," word","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.45,0.0,0.0,0.0,0.19,0.25,0.38,0.25,0.0,0.54,0.72,1.0,0.0,0.0,0.0,0.79,0.0,0.0,0.0,0.0,0.0,0.39,0.0,0.0,0.0,0.0,0.0,0.0,0.33,0.0,0.56],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["<EOT>","\u23ce\u23ce","Human",":"," What","'s"," the"," opposite"," of"," ugly","?","\u23ce\u23ce","Assistant",":"," The"," opposite"," of"," \"","ugly","\""," is"," \"","beautiful","\""," or"," \"","attractive",".\"","\u23ce\u23ce","Human",":"," ","\u23ce\u23ce","What","'s"," the"," opposite"," of"," \"","big","\""]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.49,0.0,0.04,0.0,0.0,0.67,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["of"," a"," word"," is"," a"," word"," opposite"," in"," meaning"," to"," it",".","\u23ce","Input",":"," cr","ook",".","\u23ce","Output",":","\u23ce\u23ce","Assistant",":"," I"," apolog","ize",","," but"," there"," seems"," to"," be"," a"," mis","under","standing","."," \"","\u2191","Cr"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.67,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.22,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["onym"," of"," a"," word"," is"," a"," word"," opposite"," in"," meaning"," to"," it",".","\u23ce","Input",":"," add",".","\u23ce","Output",":","\u23ce\u23ce","Assistant",":"," For"," the"," adj","ective"," \"","add","\","," I"," apolog","ize",","," but"," there","'s"," a"," mis","under"]},{"tokens_acts_list":[0.0,0.0,0.13,0.0,0.0,0.0,0.17,0.43,0.15,0.1,0.0,0.27,0.59,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.64,0.0,0.03,0.0,0.49,0.76,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.02,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["for"," example",","," the"," ant","onym"," of"," \"","happy","\""," would"," be"," \"","sad","\"."," And"," the"," ant","onym"," of"," \"","big","\""," would"," be"," \"","small","\"."," ","\u23ce","Why"," would"," a"," synonym"," need"," an"," ant","onym","?"," Well",","]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.58,0.0,0.0,0.0,0.0,0.0,0.0,0.63,0.0,0.69,0.0,0.0,0.16,0.3,0.08,0.62,0.0,0.28,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["the"," words"," b","aff","ling"," conf","using"," and"," bew","il","dering"," and"," ant","ony","ms"," for"," in"," compreh","ens","ible"," are"," the"," words"," under","stand","able"," clear"," plain"," and"," intellig","ible","."," So"," that"," was"," incomp","reh","ens","ible"," our"," first"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.21,0.0,0.0,0.59,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.52,0.25,0.0,0.0,0.09,0.0,0.0,0.0,0.81,0.0,0.0,0.67,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["a"," word"," is"," a"," word"," opposite"," in"," meaning"," to"," it",".","\u23ce","Input",":"," syll","ab","ic",".","\u23ce","Output",":","\u23ce\u23ce","Assistant",":"," Here"," are"," some"," potential"," ant","ony","ms"," for"," \"","syll","ab","ic","\":","\u23ce\u23ce","1","."," as"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.47,0.0,0.0,0.0,0.0,0.0,0.0,0.27,0.0,0.0,0.0,0.0,0.0,0.63,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["question","."," The"," word"," that"," is"," an"," ant","onym"," for"," \"","stand"," out"," from","\""," is",":","\u23ce\u23ce","B",")","\u2191"," Distinguish"," from","\u23ce\u23ce","\"","\u2191","Distinguish"," from","\""," means"," to"," be"," not","ice","ably"," different"," or"," distinct"," from"," something"," else"]}]},{"quantile_name":"Subsample Interval 0","examples":[{"tokens_acts_list":[0.0,0.13,0.44,0.14,0.08,0.0,0.26,0.6,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.62,0.0,0.04,0.0,0.5,0.77,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["onym"," of"," \"","happy","\""," would"," be"," \"","sad","\"."," And"," the"," ant","onym"," of"," \"","big","\""," would"," be"," \"","small","\"."," ","\u23ce\u23ce","Why"," would"," a"," synonym"," need"," an"," ant","onym","?"," Well",","," sometimes"," words"," can"," have"," more"]},{"tokens_acts_list":[0.0,0.17,0.43,0.15,0.1,0.0,0.27,0.59,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.64,0.0,0.03,0.0,0.49,0.76,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["onym"," of"," \"","happy","\""," would"," be"," \"","sad","\"."," And"," the"," ant","onym"," of"," \"","big","\""," would"," be"," \"","small","\"."," ","\u23ce","Why"," would"," a"," synonym"," need"," an"," ant","onym","?"," Well",","," sometimes"," words"," can"," have"," more"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12,0.34,0.74,0.0,0.59,0.33,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["\u23ce","\"","synonym","s","\":"," [\"","leaf","\","," \"","sheet","\","," \"","paper","\"],","\u23ce","\"","ant","ony","ms","\":"," [\"","screen","\","," \"","proj","ector","\"]","\u23ce","},"," {","\u23ce","\"","definition","\":"," \"","A"," written"," or"," printed"," document"," consisting"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.64,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"ha-haiku35_resampled":true,"tokens":["these"," c","olds","."," Could"," you"," explain"," more"," how"," each"," of"," them"," differs"," to"," help"," me"," better"," remember"," their"," ant","ony","ms","?","\u23ce\u23ce","Assistant",":"," Here","'s"," a"," breakdown"," of"," some"," key"," synonym","s"," for"," ","'","plac","ate","'"]}]},{"quantile_name":"Subsample Interval 1","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.49,0.0,0.04,0.0,0.0,0.67,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14],"train_token_ind":20,"is_repeated_datapoint":true,"ha-haiku35_resampled":true,"tokens":["of"," a"," word"," is"," a"," word"," opposite"," in"," meaning"," to"," it",".","\u23ce","Input",":"," cr","ook",".","\u23ce","Output",":","\u23ce\u23ce","Assistant",":"," I"," apolog","ize",","," but"," there"," seems"," to"," be"," a"," mis","under","standing","."," \"","\u2191","Cr"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.67,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.22,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"ha-haiku35_resampled":true,"tokens":["onym"," of"," a"," word"," is"," a"," word"," opposite"," in"," meaning"," to"," it",".","\u23ce","Input",":"," add",".","\u23ce","Output",":","\u23ce\u23ce","Assistant",":"," For"," the"," adj","ective"," \"","add","\","," I"," apolog","ize",","," but"," there","'s"," a"," mis","under"]},{"tokens_acts_list":[0.0,0.0,0.13,0.0,0.0,0.0,0.17,0.43,0.15,0.1,0.0,0.27,0.59,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.64,0.0,0.03,0.0,0.49,0.76,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.02,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["for"," example",","," the"," ant","onym"," of"," \"","happy","\""," would"," be"," \"","sad","\"."," And"," the"," ant","onym"," of"," \"","big","\""," would"," be"," \"","small","\"."," ","\u23ce","Why"," would"," a"," synonym"," need"," an"," ant","onym","?"," Well",","]}]},{"quantile_name":"Subsample Interval 2","examples":[{"tokens_acts_list":[0.43,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13,0.0,0.0,0.0,0.17,0.43,0.15,0.1,0.0,0.27,0.59,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.64,0.0,0.03,0.0,0.49,0.76,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["the"," opposite"," of"," another"," word","."," So",","," for"," example",","," the"," ant","onym"," of"," \"","happy","\""," would"," be"," \"","sad","\"."," And"," the"," ant","onym"," of"," \"","big","\""," would"," be"," \"","small","\"."," ","\u23ce","Why"," would"," a"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.42,0.59,0.47,0.0,0.42,0.34,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\"","synonym","s","\":"," [\"","document","\","," \"","paper","\","," \"","write","-","up","\"],","\u23ce","\"","ant","ony","ms","\":"," [\"","screen","\","," \"","proj","ector","\"]","\u23ce","},"," {","\u23ce","\"","definition","\":"," \"","A"," page"," in"," a"," book"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.58,0.0,0.0,0.0,0.0,0.0,0.0,0.63,0.0,0.69,0.0,0.0,0.16,0.3,0.08,0.62,0.0,0.28,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["understand"," and"," synonym","s"," for"," impossible"," are"," the"," words"," b","aff","ling"," conf","using"," and"," bew","il","dering"," and"," ant","ony","ms"," for"," in"," compreh","ens","ible"," are"," the"," words"," under","stand","able"," clear"," plain"," and"," intellig","ible","."," So"," that"]},{"tokens_acts_list":[0.38,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13,0.0,0.0,0.0,0.17,0.43,0.12,0.09,0.0,0.25,0.57,0.0,0.0,0.0,0.04,0.0,0.0,0.0,0.61,0.0,0.02,0.0,0.49,0.74,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["the"," opposite"," of"," another"," word","."," So",","," for"," example",","," the"," ant","onym"," of"," \"","happy","\""," would"," be"," \"","sad","\"."," And"," the"," ant","onym"," of"," \"","big","\""," would"," be"," \"","small","\"."," ","\u23ce\u23ce","Why"," would"," a"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.25,0.53,0.0,0.16,0.0,0.0,0.0,0.09,0.0,0.5,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["\u23ce","\u2191","Identify"," the"," word"," that"," is"," ant","onym"," of"," the"," given"," word",".","\u23ce","Input",":"," Word",":"," \"","\u2191","Funny","\"","\u23ce","\u2191","Ant","onym",":"," \"","\u2191","Serious","\"","\u23ce","Output",":","\u23ce\u23ce","Assistant",":"," You","'re"," correct"]}]},{"quantile_name":"Subsample Interval 3","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.43,0.0,0.0,0.0,0.21,0.23,0.41,0.24,0.0,0.48,0.67,0.92,0.0,0.0,0.0,0.75,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":[";","<EOT>","\u23ce\u23ce","Human",":"," What","'s"," the"," opposite"," of"," ugly","?","\u23ce\u23ce","Assistant",":"," The"," opposite"," of"," \"","ugly","\""," is"," \"","beautiful","\""," or"," \"","attractive",".\""," These"," words"," describe"," something"," that"," is"," ple","asing"," to"," look"," at"," or"]},{"tokens_acts_list":[0.0,0.0,0.13,0.44,0.14,0.08,0.0,0.26,0.6,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.62,0.0,0.04,0.0,0.5,0.77,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ant","onym"," of"," \"","happy","\""," would"," be"," \"","sad","\"."," And"," the"," ant","onym"," of"," \"","big","\""," would"," be"," \"","small","\"."," ","\u23ce\u23ce","Why"," would"," a"," synonym"," need"," an"," ant","onym","?"," Well",","," sometimes"," words"," can"," have"]},{"tokens_acts_list":[0.0,0.0,0.17,0.43,0.15,0.1,0.0,0.27,0.59,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.64,0.0,0.03,0.0,0.49,0.76,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ant","onym"," of"," \"","happy","\""," would"," be"," \"","sad","\"."," And"," the"," ant","onym"," of"," \"","big","\""," would"," be"," \"","small","\"."," ","\u23ce","Why"," would"," a"," synonym"," need"," an"," ant","onym","?"," Well",","," sometimes"," words"," can"," have"]}]},{"quantile_name":"Subsample Interval 4","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.42,0.59,0.47,0.0,0.42,0.34,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","\"","synonym","s","\":"," [\"","document","\","," \"","paper","\","," \"","write","-","up","\"],","\u23ce","\"","ant","ony","ms","\":"," [\"","screen","\","," \"","proj","ector","\"]","\u23ce","},"," {","\u23ce","\"","definition","\":"," \"","A"," page"," in"," a"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.44,0.0,0.0,0.0,0.23,0.24,0.41,0.24,0.0,0.47,0.71,0.9,0.0,0.0,0.0,0.76,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":[")"," AS","C",";","<EOT>","\u23ce\u23ce","Human",":"," What","'s"," the"," opposite"," of"," ugly","?","\u23ce\u23ce","Assistant",":"," The"," opposite"," of"," \"","ugly","\""," is"," \"","beautiful","\""," or"," \"","attractive",".\""," These"," words"," describe"," something"," that"," is"," ple","asing"," to"]},{"tokens_acts_list":[0.0,0.0,0.0,0.08,0.47,0.0,0.0,0.56,0.39,0.0,0.0,0.0,0.34,0.0,0.41,0.0,0.0,0.0,0.19,0.0,0.43,0.58,0.0,0.0,0.15,0.0,0.36,0.0,0.0,0.0,0.0,0.0,0.0,0.17,0.0,0.0,0.0,0.0,0.0,0.0,0.43],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["stand"," out"," from","\""," ?","\u23ce","A",")","\u2191"," Blend"," in"," with","\u23ce"," B",")","\u2191"," Distinguish"," from","\u23ce"," C",")","\u2191"," Emerge"," from","\u23ce"," D",")","\u2191"," Separate"," from","\u23ce\u23ce"," Assistant",":"," Let","'s"," think"," through"," this"," step","-","by"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.21,0.3,0.18,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16,0.21,0.41,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["close"," the"," assessment"," record"," to"," unauthorized"," individuals","\u23ce"," C",")"," To"," discuss"," the"," assessment"," record"," with"," the"," candidate","\u23ce"," D",")"," To"," submit"," the"," assessment"," record"," to"," the"," person"," responsible"," for"," keeping"," the"," documents","\u23ce\u23ce","\u2191"," Correct"," answer",":"," A",")"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.29,0.43,0.41,0.0,0.14,0.17,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","\"","synonym","s","\":"," [\"","spread","\","," \"","section","\","," \"","division","\"],","\u23ce","\"","ant","ony","ms","\":"," [\"","screen","\","," \"","proj","ector","\"]","\u23ce","},"," {","\u23ce","\"","definition","\":"," \"","A"," single"," screen"," of"," a"," computer"]}]},{"quantile_name":"Subsample Interval 5","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.39,0.0,0.0,0.0,0.19,0.29,0.0,0.0,0.0,0.32,0.38,0.34,0.0,0.0,0.0,0.22,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.48,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["\uc758"," ","\ubc18","\ub300","\ub9d0","\uc774"," ","\\xeb","\\xad","\\x90","\uc57c","?","\u23ce\u23ce","Assistant",":"," ","\uc2e0","\\xeb","\\xa2","\\xb0","\uc758"," ","\ubc18","\ub300","\ub9d0","\uc740"," ","\"","\ubd88","\uc2e0","\""," ","\ub610","\ub294"," ","\"","\uc758","\uc2ec","\"","\uc785","\ub2c8"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.33,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[".","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","Given"," an"," adj","ective",","," generate"," its"," ant","onym","."," An"," ant","onym"," of"," a"," word"," is"," a"," word"," opposite"," in"," meaning"," to"," it",".","\u23ce","Input",":"," truth","ful",".","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.33,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["if"," you"," provide"," me"," with"," the"," definition"," or"," context"," of"," the"," word",","," I"," can"," try"," to"," suggest"," some"," possible"," ant","ony","ms",".","<EOT>","\u23ce\u23ce","Human",":"," In"," linux"," command"," line"," how"," to"," un","zip"," all"," zip"," files"," in"," a"]},{"tokens_acts_list":[0.0,0.0,0.0,0.06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.33,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.22,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["age"," |"," old"," |"," new"," |"," ","\u23ce","|"," altitude"," |"," low"," |"," high"," |","\u23ce","|"," area"," |"," small"," |"," vast"," |"," ","\u23ce","|"," brightness"," |"," dark"," |"," light"," |"," ","\u23ce","|"," clarity"," |"," obsc","ure"," |"," clear"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12,0.34,0.74,0.0,0.59,0.33,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["s","\":"," [\"","leaf","\","," \"","sheet","\","," \"","paper","\"],","\u23ce","\"","ant","ony","ms","\":"," [\"","screen","\","," \"","proj","ector","\"]","\u23ce","},"," {","\u23ce","\"","definition","\":"," \"","A"," written"," or"," printed"," document"," consisting"," of"," one"," or"]}]},{"quantile_name":"Subsample Interval 6","examples":[{"tokens_acts_list":[0.0,0.38,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.13,0.0,0.0,0.0,0.17,0.43,0.12,0.09,0.0,0.25,0.57,0.0,0.0,0.0,0.04,0.0,0.0,0.0,0.61,0.0,0.02,0.0,0.49,0.74,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["means"," the"," opposite"," of"," another"," word","."," So",","," for"," example",","," the"," ant","onym"," of"," \"","happy","\""," would"," be"," \"","sad","\"."," And"," the"," ant","onym"," of"," \"","big","\""," would"," be"," \"","small","\"."," ","\u23ce\u23ce","Why"," would"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.25,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["iu","  ","","\u00a9","cg","ci","lf","a","\\xc3","\\x87","","  ","P","On"," ","\u23ce","","aft","uc","l",")","  ","bag","  ","eine","  ","ge","ip","il","fe","  ","\u2191","J","f","roft","  ","in","  ","f","id",")"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.25,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["terr","estre",",","\u21ea"," PL","AT",".","\u2191"," R","sp","."," ","546"," ","a",";"," p","."," o","pp","."," i","va","ipt","og",",","\u2191"," Til","\u00e9m",".","168"," ","b","11"," ","2"," ","qui"," cons","iste"," en"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.44,0.0,0.0,0.0,0.23,0.24,0.41,0.24,0.0,0.47,0.71,0.9,0.0,0.0,0.0,0.76,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["AS","C",";","<EOT>","\u23ce\u23ce","Human",":"," What","'s"," the"," opposite"," of"," ugly","?","\u23ce\u23ce","Assistant",":"," The"," opposite"," of"," \"","ugly","\""," is"," \"","beautiful","\""," or"," \"","attractive",".\""," These"," words"," describe"," something"," that"," is"," ple","asing"," to"," look"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.14,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.25,0.0,0.0,0.23,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["can"," vary"," from"," person"," to"," person",","," so"," there"," isn","'t"," a"," single"," word"," or"," phrase"," that"," can"," be"," considered"," the"," opposite"," of"," \"","ugly","\""," univers","ally",".","<EOT>","\u23ce\u23ce","Human",":"," Given"," the"," document"," below",","," you"," have"," to"]}]},{"quantile_name":"Subsample Interval 7","examples":[{"tokens_acts_list":[0.08,0.0,0.13,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.06,0.0,0.17,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.13,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["|"," short"," |"," long"," |"," ","\u23ce","|"," magnitude"," |"," small"," |"," large"," |"," ","\u23ce","|"," mass"," |"," small"," |"," large"," |"," ","\u23ce","|"," od","or"," |"," weak"," |"," strong"," |"," ","\u23ce","|"," pressure"," |"," low"," |"," high"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.21,0.0,0.0,0.57,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.54,0.28,0.0,0.0,0.09,0.0,0.0,0.0,0.8,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":["ant","onym"," of"," a"," word"," is"," a"," word"," opposite"," in"," meaning"," to"," it",".","\u23ce","Input",":"," syll","ab","ic",".","\u23ce","Output",":","\u23ce\u23ce","Assistant",":"," Here"," are"," some"," potential"," ant","ony","ms"," for"," \"","syll","ab","ic","\":","\u23ce\u23ce"]},{"tokens_acts_list":[0.07,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.0,0.0,0.0,0.0,0.18,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":[")","PC","l","4","F"," (","B",")","B","F","3"," ","(","C",")","CO","2"," ","(","D",")","Si","(","CH","3",")","4",".","\u23ce","Output",":","\u23ce\u23ce","Assistant",":"," Let","'s"," solve"," this"," step"," by"," step"]},{"tokens_acts_list":[0.0,0.19,0.0,0.0,0.19,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.25,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.17,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.18,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.25,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["near"," |"," far"," |"," ","\u23ce","|"," electric"," conduct","ivity"," |"," low"," |"," high"," |","\u23ce","|"," flexibility"," |"," rigid"," |"," flexible"," |","\u23ce","|"," gran","ul","arity"," |"," fine"," |"," co","arse"," |"," ","\u23ce","|"," hard","ness"," |"," soft"]},{"tokens_acts_list":[0.0,0.0,0.0,0.27,0.0,0.0,0.0,0.0,0.0,0.0,0.06,0.0,0.17,0.0,0.0,0.0,0.0,0.0,0.13,0.0,0.17,0.0,0.0,0.0,0.0,0.0,0.11,0.0,0.22,0.0,0.0,0.0,0.0,0.0,0.16,0.0,0.22,0.0,0.0,0.14,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["complexity"," |"," simple"," |"," complex"," |"," ","\u23ce","|"," cost"," |"," cheap"," |"," expensive"," |","\u23ce","|"," density"," |"," sparse"," |"," dense"," |","\u23ce","|"," depth"," |"," shallow"," |"," deep"," |","\u23ce","|"," distance"," |"," near"," |"," far"," |"," ","\u23ce"]}]},{"quantile_name":"Subsample Interval 8","examples":[{"tokens_acts_list":[0.0,0.0,0.04,0.0,0.16,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09,0.0,0.0,0.0,0.0,0.0,0.0,0.08,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.07,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.35,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["od","or"," |"," weak"," |"," strong"," |"," ","\u23ce","|"," pressure"," |"," low"," |"," high"," |"," ","\u23ce","|"," resistance"," |"," low"," |"," high"," |"," ","\u23ce","|"," shape"," |"," round"," |"," sharp"," |","\u23ce","|"," shape"," |"," flat"," |"," sp"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["op","ul","aire","^"," \u00e2",".","\u2191"," Imp","op","ula"," r","it\u00e9",","," af","."," V","."," d\u00e9","pl","aire","^"," p","<","M"," -","p","/","e","."," \u2022","\u25a0","\u2022","\u25a0","\u25a0",""," ","\u25a0","\u2022"," '"," *","'"]},{"tokens_acts_list":[0.0,0.08,0.0,0.14,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08,0.0,0.16,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.11,0.0,0.08,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["texture"," |"," smooth"," |"," rough"," |"," ","\u23ce","|"," thermal"," conduct","ivity"," |"," low"," |"," high"," |","\u23ce","|"," thickness"," |"," thin"," |"," thick"," |"," ","\u23ce","|"," volume"," |"," small"," |"," large"," |","\u23ce","|"," weight"," |"," light"," |"," heavy"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["from"," the"," input"," question",".","\u2191"," Avoid"," repe","ating"," the"," same"," style"," or"," phrase"," in"," generating"," your"," modified"," question"," e",".","g","."," this"," task"," can"," be"," always"," solved"," using"," a"," simple"," neg","ation"," i",".","e","."," by"," adding"," not"]}]},{"quantile_name":"Bottom Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[" ","184"," ","b",","," u","."," due","-"," ","\u23ce","t","\u00e4n","te","rog",","," Tim","."," ","29"," ","b",";"," d","mi"," tad","\u0131","ng"," o","lov"," zon","m","ides"," ","\u23ce","nov","lu","ov",",","\u2191"," Le"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.22,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["your"," question","."," Here"," is"," a"," list"," of"," attributes"," and"," associated"," cont","rast","ive"," words"," that"," may"," help"," write"," cont","rast","ive"," trigger"," words","."," Note"," that"," cont","rast","ive"," trigger"," words"," help"," in"," fl","ipping"," the"," label",".","\u23ce","|"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["characteristics",".","\u23ce\u23ce","The"," options"," are",":","\u23ce","a","."," Quality"," -"," Quality"," characteristics"," can"," be"," measured"," ","\u23ce","c",".","\u2191"," Structural"," -","\u2191"," Structural"," characteristics"," refer"," to"," the"," organization","/","layout"," of"," the"," architecture"," and"," can"," be"," measured","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["personality"," traits","\u23ce"," B",":","\u2191"," Lon","gev","ity","\u23ce"," C",":","\u2191"," Alz","heimer","'s"," dem","entia","\u23ce"," D",":"," Environment"," never"," plays"," a"," bigger"," role","\u23ce"," Please"," eliminate"," two"," incorrect"," options"," first",","," then"," think"," it"," step"," by"," step"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.16,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["is"," considered"," positive",","," and"," would"," generally"," be"," considered"," a"," pr","ais","eful"," attribute","."," The"," opposite"," of"," Grade"," A"," would"," be"," to"," describe"," a"," person"," neg","atively",","," e",".","g","."," har","sh","ly",","," dis","resp","ect","fully"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.08,0.13,0.0,0.01,0.0,0.0,0.0,0.06,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["an","ab","."," ","7",","," ","1",","," ","27",","," ","ubi"," op","pos",".."," ","\u23ce","tk"," \u00e9","vo","yj","ua",".","\u2191"," ","Ise","eus"," p","."," ","46",","," ","26",";","\u2191"," And","oc","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.11,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[")","it"," makes"," little"," distinction"," between"," the"," way"," the"," world"," is"," and"," the"," way"," it"," ought"," to"," be"," (","C",")","its"," knowledge"," is"," accumulated"," from"," many"," different"," research"," contexts"," (","D",")","it"," is"," subj","ective"," and"," bi","ased","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["n","f","\u00bb","n","de","*"," ","\u23ce","re",",","  ","V","."," C","\u00bb","n"," f","end","ere"," .","\u2191"," Difer","din","\u00e0","re","."," ","\u23ce\u23ce","\u2191","Se","emp","if","ei","are"," .","\u2191"," P","if","et","are","  "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["A",":"," The"," machine"," itself","\u23ce"," B",":"," A"," human"," chess"," master"," hiding"," inside"," a"," mechanical"," ill","usion",".","\u23ce","C",":"," A"," group"," of"," chess"," enthusi","asts","\u23ce"," D",":"," An"," Austrian"," empress","\u23ce","\u2191"," Correct"," answer",":"," B","\u23ce\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["is"," bar","king",".","\u23ce\u23ce","Assistant",":"," ","\u23ce","The"," dog"," is"," not"," bar","king",".","\u23ce","<","|","stop","|",">","<EOT>","\u23ce","What"," city"," has"," the"," lowest"," crime"," rate"," in"," the"," US","?","\u23ce","1"," ","\u23ce\u23ce","Assistant",":"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.04,0.13,0.0,0.0,0.0,0.0,0.0,0.35,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["Which"," of"," the"," following"," algorithms"," is"," used"," for"," supervised"," learning","?","\u23ce","a",")"," K","-","means"," clustering","\u23ce"," b",")","\u2191"," Gradient"," descent","\u23ce"," c",")"," Principal"," component"," analysis","\u23ce"," d",")"," Random"," forest","\u23ce","2","."," Which"," of"," the"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["class"," diagram","\u23ce"," Select"," one",":","\u23ce","a",".","\u2191"," Tak","eover","\u23ce"," b","."," One","-","way"," association","\u23ce"," c",".","\u2191"," Inheritance","\u23ce"," d","."," Two","-","way"," association","\u23ce\u23ce"," Assistant",":"," The"," correct"," answer"," is",":"," a","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["nt","\u00ef","or",","," ","\u23ce","m","\u00ef","t","\u00ef","\u00f4","r",".","  ","\u2191","V","oy",".","  ","\u2191","Bonus",".","  ","[","|","  ","Plus","  ","brave",".","  ","\u2191","M","\u00eb","l","\u00ef","\u00f4","r","\u00ef","b","\u00fc"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[" ","\u23ce\u23ce","\u2191","S","th","md","iri",".","  ","\u2191","Cent","rar","le","  ","di","  ","\u2191","Chi","ud","ere","  ","."," ","\u23ce","t","A","gr","ire","  ","un","  ","fer","rag","lie","  ","di","  ","fi","efe",",","  "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["one"," meaning","."," And"," when"," two"," words"," have"," the"," same"," meaning",","," they","'re"," called"," synonym","s","."," But"," sometimes",","," you"," might"," want"," to"," use"," a"," word"," with"," a"," specific"," meaning",","," and"," not"," the"," general"," meaning"," that"," the"," synonym"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","B",":"," the"," fact"," of"," deliber","ation"," and"," our"," sense"," that"," some"," actions"," are"," up"," to"," us","\u23ce"," C",":"," the"," fact"," of"," deliber","ation"," and"," ind","eter","min","ism","\u23ce"," D",":"," scientific"," determin","ism","\u23ce"," Please"," eliminate"," two"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.07,0.05,0.17,0.25,0.0,0.27,0.0,0.01,0.08,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"ha-haiku35_resampled":true,"tokens":[":","\u23ce","a","."," Quality","\u23ce"," b","."," Abstract","\u23ce"," c",".","\u2191"," Structural","\u23ce"," d",".","\u2191"," Operational","\u23ce"," e","."," Process","\u23ce","\u21ea"," ONLY"," SELECT"," THE","\u21ea"," CORRECT"," ONE"," WITHOUT","\u21ea"," EXPLAINING","\u23ce\u23ce"," Assistant",":"," b","."," Abstract","\u23ce\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.11,0.0,0.07,0.0,0.0,0.0,0.0,0.0,0.0,0.09,0.0,0.14,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.14,0.0,0.21,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.27,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["|"," weight"," |"," light"," |"," heavy"," |"," ","\u23ce","|"," width"," |"," narrow"," |"," wide"," |"," ","\u23ce","|"," location"," |"," in"," |"," out"," |","\u23ce","|"," location"," |"," up"," |"," down"," |","\u23ce","|"," location"," |"," above"," |"," below"," |"]}]}],"top_logits":["ordinary","consistent","condens","Static","legitimate","weak","familiar","vulnerable","reasonable","passive"],"bottom_logits":["\u0434\u0430\u043d\u0438","session","imens","\u0444\u0435\u0431","image","ession","eship","ithm","ickness"]}