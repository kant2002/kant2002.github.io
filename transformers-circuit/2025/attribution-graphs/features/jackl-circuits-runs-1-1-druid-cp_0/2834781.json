{"index":2834781,"examples_quantiles":[{"quantile_name":"Top Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.83,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["feedback"," information"," generated"," by"," a"," feedback"," generator"," and","/","or"," to"," analyze"," data"," stored"," in"," a"," data"," store","."," The"," analyses"," performed"," by"," the"," monitoring"," application"," can"," be"," employed"," to"," generate"," information"," including",","," but"," not"," limited"," to",","," productivity"," reports"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.99,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["millions"," of"," customers",".","\u23ce","Many"," conventional"," systems"," attempt"," to"," conserv","e"," processing"," resources"," by"," collecting"," survey"," responses"," having"," an"," analysis","-","friendly"," format","."," For"," example",","," many"," electronic"," surveys"," include"," questions"," that"," sol","icit"," rankings",","," ranges"," of"," numbers"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.98,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.82,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ams"," which"," can"," then"," be"," detected","."," A"," detected"," beam"," generated"," by"," a"," foreign"," particle"," is"," analyzed"," by"," using"," an"," analysis"," technique"," such"," as"," laser"," photo"," lumin","escence"," or"," a"," secondary"," X","-","ray"," analysis"," (","X","M","R",").","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.97,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["has"," performed"," its"," comput","ations",","," quant","ifications",","," ap","pr","ais","als",","," assess","ments",","," calib","rations",","," analyses",","," predictions",","," and"," the"," like",","," in"," perfect"," isolation"," from"," other"," applications"," that"," can"," be"," performing"," identical",","," and"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.97,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["."," The"," invention"," also"," satisf","ies"," the"," need"," for"," an"," inspection"," tool"," that"," provides"," both"," qual","itative"," and"," quant","itative"," analysis"," of"," the"," disk","'s"," surface","."," The"," inspection"," apparatus"," is"," highly"," accurate"," and"," provides"," a"," simple",","," cost","-","effective"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.78,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.97,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["medical"," developments"," have"," resulted"," in"," new"," processes"," for"," analyzing"," and"," controlling"," the"," state"," of"," patients","."," Among"," these"," processes"," are"," analyses"," which"," are"," performed"," after"," injection"," of"," a"," contrast"," agent",".","\u2191"," Contrast"," agents"," may"," be"," used",","," for"," example",","]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.97,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["plastic"," fi","bers"," or"," glass"," fi","bers"," and"," used"," for"," the"," filt","ration"," in"," households",","," technical"," applications"," and"," for"," analyses","."," Paper","-","type"," non","w","oven"," materials"," are"," composite"," materials"," consisting"," of"," fib","rous"," material"," containing"," cel","lu","lose"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.97,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["instance",","," the"," detection"," of"," dangerous"," compounds"," in"," air",","," water",","," and"," earth"," often"," requires"," extensive"," chemical"," tests"," and"," analyses",".","\u2191"," Certain"," car","ci","no","gens"," are"," excluded"," by"," f","iat"," at"," any"," level"," in"," drinking"," waters",","," yet"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.97,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["gel"," based"," techniques"," are"," oper","at","ionally"," difficult"," to"," implement"," and"," require"," highly"," skilled"," personnel","."," In"," addition",","," the"," analyses"," are"," lengthy"," and"," require"," a"," great"," deal"," of"," set"," up"," time","."," A"," d","ena","turing"," cap","ill","ary"," gel"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.96,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["using"," a"," smart"," antenna"," for"," performing"," beam"," forming"," according"," to"," a"," structure"," of"," the"," multiple"," an","ten","nas","."," Various"," analyses"," have"," been"," performed",","," for"," example"," a"," post","-","FF","T"," in"," which"," FF","T"," is"," performed"," for"," the"," respective"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.96,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["apparatus"," suitable"," for"," use"," in"," a"," medical"," image"," diagnosis"," apparatus",","," a"," non","-","destruct","ive"," examination"," apparatus",","," an"," analysis"," apparatus"," using"," radiation",","," and"," the"," like",".","\u23ce","2","."," Description"," of"," the"," Related"," Art","\u23ce","\u2191"," Imaging"," methods"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.96,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["blood"," collecting"," apparatus"," that"," coll","ects"," blood"," as"," one"," example","."," The"," blood"," collecting"," apparatus"," is"," used"," for"," quant","itative"," analyses"," in"," nuclear"," medicine"," diagnosis"," (","e",".","g",".,"," P","ET"," (","\u2191","Posit","ron","\u2191"," Emission","\u2191"," Tom","ography"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.96,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["like",","," high"," degree"," of"," integration"," thereof"," has"," been"," increasingly"," developed","."," With"," the"," development",","," time"," required"," for"," an"," analysis"," of"," the"," logic"," circuits"," has"," been"," prolong","ed","."," In"," view"," of"," the"," fact",","," it"," has"," been"," demanded"," to"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.96,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[".","\u23ce","\u2191","Typical","\u21ea"," PR","ML"," read"," channels"," have"," ML"," det","ectors"," that"," determine"," the"," data"," based"," on"," an"," analysis"," of"," samples"," taken"," from"," an"," analog"," w","ave","form"," read"," from"," a"," disk",","," for"," example",","," rather"," than"," just"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.96,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["the"," data"," to"," be"," processed"," are"," protected"," against"," attack","ers"," attempting"," to"," obtain"," the"," security","-","relevant"," data"," by"," an"," analysis"," of"," the"," circuit","."," Due"," to"," S","PA","/","D","PA"," attacks"," (","S","PA","/","D","PA","=","simple"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.96,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["of"," the"," Related"," Art","\u23ce"," The"," field"," of"," Operations"," Research"," (","OR",")"," is"," concerned"," with"," the"," mathematical"," or"," scientific"," analyses"," of"," processes"," and"," provides"," tools"," that"," may"," be"," used"," in"," making"," decisions","."," For"," example",","," OR"," provides"," guidance"," on"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.96,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["reflects"," the"," current"," operation"," states"," (","over","lo","aded",","," idle",")"," of"," the"," components"," of"," the"," network","."," Through"," analysis"," on"," such"," data",","," the"," overall"," operation"," conditions"," of"," the"," network"," are"," g","ras","ped",","," and"," the"," network"," configuration"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.96,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["activity",","," can"," reduce"," the"," time"," necessary"," to"," detect"," viable"," test"," micro","organ","isms","."," In"," some"," implementations",","," an"," analysis"," of"," the"," fluor","escence"," intensity"," due"," to"," a"," fluor","escent"," product"," of"," an"," enzyme"," reaction"," serves"," to"," determine"," whether"," the"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.96,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["propag","ating"," velocity"," on"," c","racks"," produced"," on"," metal"," materials",","," and"," this"," propag","ating"," velocity"," is"," applied"," to"," the"," analysis"," for"," prog","ressing"," c","racks"," on"," metal"," materials","."," However",","," this"," invent","ive"," idea"," cannot"," solve"," the"," above","-"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.96,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["applications","."," The"," challenge"," of"," handling"," big"," data"," include"," capture",","," c","uration",","," storage",","," search",","," sharing",","," analysis"," and"," visualization","."," The"," trend"," to"," larger"," data"," sets"," is"," due"," to"," the"," prolif","eration"," of"," data"," capture"," devices"," and"]}]},{"quantile_name":"Subsample Interval 0","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["interru","ption"," caused"," by"," various"," exceptions"," occurring"," during"," the"," execution"," of"," an"," instruction"," is"," generally"," not"," so"," frequent"," and"," requires"," analysis"," of"," the"," causes"," and"," count","erm","eas","ures"," by"," software"," (","interrupt"," handler",","," etc",".)"," as"," well"," as"," interru"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.84,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[","," register"," R","2"," ","is"," not"," a"," free"," register"," at"," ","55",".","\u23ce","This"," type"," of"," search"," requires"," analysis"," of"," the"," entire"," program"," and"," sometimes"," even"," with"," an"," entire"," code"," analysis",","," the"," system"," cannot"," make"," complete"," determ","inations"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.82,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["the"," structured"," documents"," are"," analyzed"," by"," software"," referred"," to"," as"," a"," browser"," and"," the"," browser"," performs"," display"," in"," accordance"," with"," analysis"," results"," whereby"," the"," web"," pages"," can"," be"," displayed",".","\u23ce","Furthermore",","," when"," such"," a"," web"," page"," is"," to"," be"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["to"," maintain"," the"," constant"," temperature","."," These"," latter"," techniques"," suffer"," from"," two"," confl","icting"," problems",".","\u23ce","First",","," the"," analysis"," method"," implemented"," in"," Phillips"," requires"," that"," steady"," state"," thermal"," conditions"," be"," achieved"," prior"," to"," taking"," readings","."," This"," in"," turn"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.84,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["based"," upon"," harm","onic"," content"," of"," a"," voltage"," signal"," and"," electromagnetic"," interference"," (","EM","I",")"," data",".","\u23ce","Manual"," analysis"," by"," a"," human"," expert"," with"," an"," oscill","osc","ope"," can"," analyze"," the"," w","ave","form"," shape","."," However"," few"," people"]}]},{"quantile_name":"Subsample Interval 1","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["can","'t"," d","/","l"," to"," excel"," spread","sheet"," and"," you"," have"," to"," use"," their","\u23ce"," website"," and"," software"," to"," analyze"," and"," plot"," the"," date","."," The"," company"," side"," steps"," the","\u23ce"," issue"," because"," they"," are"," hoping"," to"," monet","ize"," the"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["is"," gas","ified"," and"," lib","erated","."," The"," cle","anl","iness"," of"," the"," inside"," of"," the"," bomb"," is"," determined"," by"," analyzing"," the"," lib","erated"," gases"," and"," vap","ours"," and"," measuring"," the"," concentration"," of"," the"," acet","ylene"," gas","."," The"," standard"," of"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[" ","\u23ce\u23ce\u23ce","\u00bb","?",":","\u2022"," ","\u23ce\u23ce\u23ce","i"," ","\u23ce\u23ce\u23ce","V","    ","",".,","    ","/"," ","\u23ce\u23ce\u23ce","\u21ea","ANALYSIS","."," ","\u23ce\u23ce\u23ce","Introduction"," y"," ","\u23ce\u23ce","\u21ea","PART","  ","I","."," ","\u23ce\u23ce","THE","  ","\u21ea","CREATION","  ","AND"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["fragments"," connected"," to"," a"," reporter"," gene"," (","usually"," chlor","amp","he","nic","ol"," acet","yl"," transfer","ase",")."," Then"," deletion"," analysis"," can"," gross","ly"," dem","arc","ate"," the"," important"," regions",".","\u2191"," Elect","rop","hor","etic"," mobility"," shift"," ass","ays"," with"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["a"," structured"," plastic"," carrier"," plate",","," an"," intermediate"," plate"," and"," a"," cover"," plate","."," In"," this"," case",","," the"," required"," analysis"," components"," or"," pum","ps",","," heat"," sources"," and"," sensors"," are"," integrated","."," The"," different"," plastic"," components"," are"," produced"," separately"," on"]}]},{"quantile_name":"Subsample Interval 2","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ne","eding"," different"," types"," of"," caps",".","\u23ce","\u2191","Conventional"," tube","-","in","-","rack"," detection"," typically"," util","izes"," image"," analysis"," tools"," on"," ","2","-","dimensional"," images"," acquired"," by"," one"," camera"," or"," a"," plurality"," of"," cameras"," in"," order"," to"," determine"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u2191","Gz",","," identifying"," sites"," of"," phosph","or","yl","ation"," in"," the"," a"," sub","unit"," of","\u2191"," Gz",","," and"," analyzing"," the"," promot","er"," region"," of"," the"," gene"," encoding","\u2191"," G","z","al","pha",".","\u2191"," Substantial"," progress"," was"," made"," towards"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["highly"," specific"," for"," the"," L","H"," alpha","/","beta"," d","imer",","," and"," the"," immun","op","rec","ip","it","ates"," analyzed"," on"," sodium"," dod","ec","yl"," sulf","ate","-","poly","ac","ry","lam","ide"," gel"," elect","rop","hor","esis"," for"," radio"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["transmitted"," un","imp","aired","."," These"," properties"," can"," be"," used"," for"," the"," production"," of"," optical"," filters",","," polar","izers",","," analyz","ers"," etc",".","\u23ce","\u2191","Chol","est","eric"," liquid"," cryst","als"," for"," the"," above"," applications"," can"," prefer","ably"," consist"," of"]},{"tokens_acts_list":[0.23,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["aly","...","](","https","://","en",".","wikipedia",".","org","/","wiki","/","Institute","_","for","_","\u2191","Propaganda","_","Analysis",")","\u23ce\u23ce","------","\u23ce","zoom","6","628","\u23ce","\u2191","Meditation"," should"," be"," mandatory"," class",".","\u2191"," B","enf","icial"," to"]}]},{"quantile_name":"Subsample Interval 3","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.61,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["require"," the"," scan"," of"," a"," wide"," range"," of"," wavel","eng","ths","."," However",","," LE","Ds"," are"," suited"," to"," optical"," analyz","ers"," which"," c","asts"," a"," specific"," wavel","ength"," of"," light"," into"," or"," onto"," a"," sample",","," as"," in"," the"," case"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.61,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.61,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["tools"," now"," available"," for"," the"," study"," of"," complex"," mix","tures"," of"," biological"," origin"," are"," G","C","-","MS","-","COM"," analytical"," systems",";"," LC","-","MS","-","COM"," analytical"," systems"," are"," under"," development","."," Computer"," science"," developments",","," combined"," with"," a"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["de"," consegu","ir","las",","," solic","ita"," la","\u2191"," C","\u00e1m","ara"," de"," v","inar","oz"," que"," las"," ent","idades"," an\u00e1l","o","gas"," env","\u00ed","en"," lo"," antes"," pos","ible"," un"," of","icio"," protest","ando"," contra"," los"," at","rop","el","los"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["of"," ","\u23ce","angels"," and"," spirits",","," and"," is"," so"," cons","oc","iated"," ","\u23ce\u23ce\u23ce\u23ce","\u21ea","ALPHAB","ETICAL"," AND","\u21ea"," ANALYTICAL"," INDEX","."," ","\u23ce\u23ce\u23ce\u23ce","with"," t","horn",","," that"," were"," he"," to"," be"," pl","uc","ked"," ","\u23ce","as","under"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["","ayer",","," y"," que"," puede"," con","tar"," con"," nu","estra"," l","eal"," cooper","aci\u00f3n"," en"," fi","ende"," teor","\u00edas"," an\u00e1l","o","gas",","," y"," sin"," embargo"," de"," cu","anto"," la","\u2191"," \u00cdnd","ole"," de"," n","uest","ro"," peri","\u00f3d","ico"]}]},{"quantile_name":"Subsample Interval 4","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce\u23ce","*"," Work"," on"," special"," projects"," to"," drive"," growth"," at","\u2191"," Sho","gun","\u23ce\u23ce","*"," Write"," queries"," and"," work"," with"," analytics"," tools"," to"," understand"," the"," business","\u23ce\u23ce","*"," Build"," out"," and"," maintain"," internal"," software"," (","admin",","," affiliate"," tracking"," system",","]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["mod","ulus"," character","ization"," was"," derived"," in","-","s","itu"," from"," random"," impul","ses"," by"," means"," of"," an"," FF","T"," analyzer","."," However",","," these"," devices"," do"," not"," describe"," an"," inter","chang","eable"," probe"," tip"," that"," may"," be"," utilized"," to"," determine"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["with"," a"," second"," part",","," for"," instance"," hooks",","," string","-","like"," elements"," with"," an"," over","-","sized"," head",","," anal","ogue"," elements",","," etc",".","\u23ce","For"," evident",","," cost","-","based"," reasons",","," particularly"," important"," when"," single","-","use"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["step"," is"," to"," develop"," a"," system"," based"," on"," the"," principles"," of"," the","\u2191"," Tan","dem","\u2191"," Differential","\u2191"," Mobility","\u2191"," Analyzer"," to"," measure"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["par","tic","les"," are"," accessible"," to"," an"," applied"," liquid"," sample",","," and"," ad","sor","ption","/","imm","obil","ization"," of"," anal","ytes"," contained"," in"," the"," liquid"," sample"," can"," take"," place",".","\u23ce","Furthermore",","," the"," method"," of"," the"," present"," invention"," for"]}]},{"quantile_name":"Subsample Interval 5","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[".\"","\u23ce\u23ce","\"","When"," my"," friends"," need"," new"," b","ads"," there","'s"," only"," one"," word"," I"," tell"," them",","," Bad","Anal","y","ze",".\"","\u23ce\u23ce","[","http","://","t","iff","zh","ang",".","com","/","startup","/","index",".","html","?"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["adjust","  ","it","  ","to","  ","its"," ","\u23ce","satisfaction",".","  ","I","  ","first","  ","observed","  ","the","  ","anal","  ","segment","  ","used","  ","while"," ","\u23ce","the","  ","cat","erp","il","lar","  ","was","  ","feeding",",","  "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["?\""," \"","\u2191","Oral"," is"," when"," I"," put"," something"," in"," your"," mouth",".\""," \"","\u2191","Sw","ine",".\""," \"","\u2191"," Anal"," is"," when"," I","'m"," serving"," you"," from"," behind",".\""," \""," That","'s"," fun","?\""," \"","A"," few"," like"," it",".\""]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["is"," linked"," to"," the"," side","-","chain"," of"," the"," amino"," acid","."," in"," some"," embod","iments",","," the"," pept","ide"," anal","ogue"," may"," be"," modified"," via"," glyc","os","yl","ation"," or"," with"," a"," glyc","an",","," at"," the"," N","-","terminal"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[".","\u2191"," Moy","ens"," \u00e9v","ac","u","ans"," (","v","om","it","ifs",","," pur","gat","ifs",")."," \u2014","\u2191"," Anal","."," Dans"," ","\u23ce","le"," typh","us",","," comme"," dans"," les"," f","i\u00e8","v","res"," typ","ho","\u00ef","des",","]}]},{"quantile_name":"Subsample Interval 6","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.32,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["you"," want"," to"," see"," published"," papers","/","links"," about"," it","?","\u23ce","Google"," doesn","'t"," publicly"," document"," the"," \"","Google"," Analytics"," Internal"," API","\".","\u23ce\u23ce","~~~","\u23ce","h","ah","aint","ernet","\u23ce"," Please"," provide"," some"," evidence"," of"," this"," claim"," as"," it"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.31,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["collected"," \"","Google","\""," gets"," it",","," processes"," it"," and"," then"," push","es"," the","\u23ce"," refined"," data"," to"," \"","Google"," Analytics","\"."," The"," raw"," data"," is"," used"," in"," the"," \"","internal"," API","\"","\u23ce","(","Which"," is"," different"," than"," the"," public"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["with"," ","14"," ","\u23ce","sp","iny"," rays",","," the"," second"," with"," ","20"," ","soft"," rays",","," ","\u23ce","anal"," fin"," ","12",","," tail"," for","ked"," yellow"," with"," brown"," ","\u23ce","spots","."," ","\u23ce\u23ce","\u2191","Sp","."," "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["public"," APIs"," that"," you"," are"," familiar"," with",")."," Both","\u23ce"," your"," source"," and"," O","Ps"," are"," referring"," to"," \"","Google"," Analytics","\""," (","which"," is"," the"," public","\u23ce"," part"," that"," web","m","asters"," see",").","\u23ce\u23ce","~~~","\u23ce","h","ah","aint"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.29,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","rc","hm","ura","\u23ce"," You"," are"," asking"," for"," a"," publicly"," available"," document"," showing"," the"," architecture"," of","\u23ce"," Google"," +"," Analytics"," +","\u2191"," Dou","ble","click"," +","\u2191"," Ad","words","..."," I"," can","'t"," get"," that",","," but"," other"," than","\u23ce"]}]},{"quantile_name":"Subsample Interval 7","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.64,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u2191","Ros","ario",","," \"","Los"," nuev","os"," planes"," de"," ","igu","ald","ad"," en"," la"," empresa",":"," un"," an\u00e1l","isis"," de"," las","\u23ce"," prim","eras"," experi","encias","\",","\u2191"," Revista"," de","\u2191"," Der","echo"," Social",","," n\u00fa","m","."," "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.63,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["estrat","eg","ias"," pedag","\u00f3g","icas"," y"," desarrollo"," de","\u23ce"," instrument","os",";"," c",".","\u2191"," Organ","izaci\u00f3n"," y"," an\u00e1l","isis"," de"," la","\u23ce\u23ce"," informaci\u00f3n"," en"," matrices"," y"," triang","ul","aci\u00f3n"," de"," los","\u23ce"," datos"," hall","ados"," y"," de"," las"," fu"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.56,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":12,"is_repeated_datapoint":false,"tokens":["<EOT>","cu","ad","ro"," ","5"," ","\u2013","\u2191"," Resultado"," del"," an\u00e1l","isis"," por"," f","R","X"," de"," las"," estat","u","illas"," de"," fac","tura"," tard","\u00eda"," de"," las"," col","ecc","iones"," del"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.61,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ni"," en"," cu","anto"," a"," los"," rit","mos"," de"," sus"," din","\u00e1m","icas","."," La","\u23ce"," valid","ez"," del"," an\u00e1l","isis"," te","\u00f3","rico"," real","izado"," a"," partir"," de"," la","\u2191"," Econom","\u00eda"," pol\u00edtica"," que"," quer","emos","\u23ce"," de"," nuevo"," r"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.61,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["nes",","," M",".,"," G",".","\u23ce"," ","cir","cul","aci\u00f3n"," internacional"," de"," los"," conoc","imientos"," a"," partir"," del"," an\u00e1l","isis"," de"," las"," r","edes"," de"," cooper","aci\u00f3n"," nacional","\u23ce"," e"," internacional"," establec","idas",".","\u23ce","Se"," identific","an"," el"," tipo"]}]},{"quantile_name":"Subsample Interval 8","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["Research",")","\u23ce\u23ce","[","https","://","discret","e","anal","ys","is","jour","nal",".","com","](","https","://","discret","e","anal","ys","is","jour","nal",".","com",")","\u23ce","(","arx","iv"," overlay"," journal"," started"," by"," Fields"," Medal"," winner"," Tim","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u00b0"," Z","."," ","49","527"," ","(","26",")"," ","\u23ce","\u2191","Trad","uit"," de"," :"," \"","\u2191","M","anal","ive","\"."," D",".","L","."," ","81","-","27","286",".","\u2191"," Br","."," :"," ","40"," ","F"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.47,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.45,0.08,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.42,0.05],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[".","\u2191","Linq",".","\u2191","Expressions",";","\u23ce","using"," System",".","\u2191","Reflection",";","\u23ce","using"," Microsoft",".","Code","Anal","ysis",";","\u23ce","using"," Microsoft",".","Code","Anal","ysis",".","C","Sh","arp",";","\u23ce","using"," Microsoft",".","Code","Anal","ysis"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["aded"," (","Berkeley","\u2191"," D","ros","oph","ila","\u2191"," Genome"," Project",")"," and"," subjected"," to"," G","EN","S","CA","N","anal","ysis"," (","\u2191","Bur","ge"," and","\u2191"," Kar","lin",","," ","1","997",")"," to"," predict"," the"," int","ron","-"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["tat","ione"," qu","am"," hab","oit"," in","\u2191"," Valent","inae"," ","\u23ce\u23ce","academ","iae"," sac","ello"," G",".","\u2191"," May","anal","ns","."," G","a","sM","sp","alis"," (","Valentine",")."," ","\u23ce","It","^","C",".","\u2191"," B","revi","arium"]}]},{"quantile_name":"Bottom Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ou"," utilis","\u00e9e"," pour"," des"," us","ages"," connex","es",","," ne"," pour","ra"," nas"," \u00eatre"," celle"," de"," la"," .","c","anal","isation"," urb","aine",","," ou"," s","'","il"," n","'","y"," a"," pas"," dans"," la"," commune"," de"," service"," public"," des"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","Further",","," according"," to"," the"," literature",","," there"," is"," described"," invention"," success","ively"," measuring"," absolute"," positions"," of"," respective"," objects"," detected"," by"," radar"," beam"," to"," thereby"," determine"," whether"," the"," respective"," objects"," are"," the"," same"," obstacle",".","\u23ce","When"," the"," fixed"," scanning"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["."," For"," example",","," not"," only"," are"," C","CD"," cameras"," used"," in"," interfer","ometry",","," but"," in"," microsc","opy",","," astronomy"," and"," geology","."," The"," present"," invention"," can"," fill"," nearly"," any"," electronic"," imaging"," and"," processing"," requirement",".","\u23ce","An"," object"," of"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[","," .","gamma","."," rays"," em","itted"," from"," radio","active"," materials"," depos","ited"," inside"," a"," body"," to"," be"," examined"," are"," detected",","," and"," an"," image"," of"," a"," distribution"," of"," the"," radio","active"," materials"," inside"," the"," body"," is"," obtained"," on"," a"," basis"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["intensity",","," and"," time"," to"," digital"," signals",".","\u23ce","For"," example",","," image"," sensors"," are"," devices"," that"," capture"," images"," as"," detected"," optical"," signals",","," and"," generate"," corresponding"," digital"," data"," using"," inher","ent"," properties"," in"," semiconductor"," materials","."," The"," image"," sensors"," include"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u2191","","Ger",".,","\u2191"," Thr","ii","nen","app","arat","."," The"," lac","ry","mal"," g","lands",","," the"," c","anal","ic","uli"," lac","rim","ales",","," the"," lac","ry","mal"," s","ac",","," and"," the"," na","sal"," ","duct","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["iments",","," the"," communications"," system"," sel","ects"," a"," mode"," for"," the"," communications"," equipment"," using"," the"," system",","," and"," speech"," is"," processed"," according"," to"," that"," mode",".","\u23ce","One"," embod","iment"," of"," a"," speech"," compression"," system"," includes"," a"," full","-","rate"," codec"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["."," ","6",",","\u2191"," Bl","umen",","," ","'","i","ras","ic"," ","2"," ","3",",","\u2191"," C","anal","stra","fte"," ","2","l",",","\u2191"," W","ohn","ungen"," f\u00fcr"," reich",".","\u2191"," Le","ute",".","\u2191"," Bart",".,"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u00e9"," el"," obt","enu"," l","'","autor","isation"," de"," faire"," \u00e9t","ud","ier"," \u00e0"," leurs"," f","rai","\u00bb"," la"," c","anal","isation"," de"," la","\u2191"," Canc","be",","," qui"," fait"," partie"," du"," canal"," pro","je","t\u00e9",";"," ils"," consid","\u00e8","rent"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["to"," also"," as"," a"," \"","coding"," method","\")"," having"," an"," ability"," (","error"," correction"," ability",")"," of"," corr","ecting"," a"," detected"," error"," of"," data"," in"," order"," to"," ensure"," predetermined"," transmission"," quality","."," However",","," in"," the"," F","EC"," method",","," when"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u2191","M\u00e9t","aster","num"," \u00e0"," p","eine"," conv","ex","e"," sur"," son"," mil","ieu",","," tr\u00e8s","-","fin","ement"," c","anal","icul","\u00e9"," sur"," sa"," ligne"," m\u00e9d","iane",","," \u00e0"," l","obe"," terminal"," so","uvent"," d","'","un"," r","oux"," de"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ized"," coal"," is"," transported"," up","wards"," using"," air"," as"," the"," transport"," medium","."," The"," pul","ver","ized"," coal"," passes"," through"," classifier"," v","anes"," within"," the"," pul","ver","izer","."," These"," classifier"," v","anes"," may"," vary"," in"," structure",","," but"," are"," intended"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u21ea","FACTS"," is"," hardly"," developed","."," Examples"," of"," the"," simulator"," for"," the","\u21ea"," FACTS"," include",","," for"," example",","," an"," offline"," simulator"," which"," is"," not"," connected"," to"," an"," external"," system"," such"," as"," a","\u21ea"," SC","ADA"," system","."," In"," this"," simulator"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["documents"," are"," provided"," that"," are"," labeled"," by"," topic",","," and"," this"," set"," of"," labeled"," documents"," is"," used"," to"," train"," a"," classifier","."," This"," approach"," can"," create"," a"," very"," accurate"," classifier"," for"," the"," pre","-","defined"," categories"," (","i",".","e","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["en","eses","  ","ese"," canal"," us","amos"," para"," latin","o","ame","rica","\u23ce","<","\u2191","Nau","dy",">"," para"," c","anal","izar"," todas"," las"," c","osas","\u23ce","<","E","du","ard","o","R",">"," cuando"," es","?","\u23ce","<","E","du"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["van"," AFC"," naar","\u2191"," Bl","oem","end","aal"," M","3","848"," ","E","\u2191"," Jacob","son",",","\u2191"," Ju","li","anal","aan"," ","152",",","\u2191"," Ov","erv","een",","," van","\u2191"," H","aar","lem"," naar","\u2191"," Bl","oem","end","aal"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","Se"," trouve"," une"," m\u00e9","lo","die"," que"," nous"," recomm","and","ons"," aux"," am","ateurs"," qui"," fu","ient"," la"," b","anal","it\u00e9","."," ","\u23ce","La","\u2191"," M","ire"," du"," d\u00e9s","ert","eur",","," romance"," dramat","ique",","," est"," destin","\u00e9e"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["10"," ","inches"," long","."," Fi","'","","uct","ification"," un","-"," known","."," ","6","."," F","."," c","anal","ic","uld","t","us",",","\u2191"," L","inn",","," (","chann","elled","\u2191"," Fuc","us",");"," fr","ond"," linear"," chann"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.74,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.53,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["series"," of"," ever"," improving"," image"," analysis"," tools"," developed"," by"," collabor","ators"," throughout"," the"," world","."," The"," output"," of"," the"," anal","ytic"," tools"," allows"," us"," to"," compare"," the"," anatomy"," and"," phys","iology"," of"," br","ains"," between"," groups"," or"," within"," an"," individual"," over"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["heat"," without"," duc","ting",".","\u2191"," Prefer","ably",","," the"," fan"," responds"," to"," the"," formation"," of"," hot"," spots",","," as"," detected"," by"," a"," heat"," sensor",","," by"," conv","ecting"," the"," heat"," away"," from"," the"," hot"," spots"," toward"," other"," components"," within"," the"]}]}],"top_logits":["of","and","on","in",",","tool","tools","by","conc","at"],"bottom_logits":["Zobacz","\u00e5rs","Extern\u00ed","Noter","Godine","genannt","Familie","Referencer","\u0421\u043e\u0431\u044b\u0442\u0438\u044f","\u00c5r"]}