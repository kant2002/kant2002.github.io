{"index":26659811,"examples_quantiles":[{"quantile_name":"Top Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["the"," greatest"," achieve","\u23ce"," ","ment"," of"," Its"," kind"," In"," history",".","\u23ce","If"," it"," may"," be"," said"," of"," Robert"," Morris"," that","\u23ce","\"","without"," hl","r","n"," Washington","'s"," sword"," would","\u23ce"," have"," ru","sted"," in"," its"," sh","eath",",\""]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.99,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["like","\u21ea"," THAT","?","\u23ce","Now"," to","\u2191"," Mention",":"," Why"," Do"," You"," Write","\u23ce"," That"," Way","?"," By"," Richard"," Burton",".","\u23ce","Indianapolis",":"," The","\u2191"," Bob","bs","-","\u2191","Mer","rill"," Co",".","\u23ce","GO","D"," IN"," THE","\u21ea"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.98,0.0,0.0,0.0,0.98,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["working"," in"," the"," out","sk","irts"," of"," the"," German"," empire"," in"," ","1","875",".\""," \"","His"," name"," was"," Robert"," Koch",".\""," \"","Robert"," Koch"," was"," a"," country"," doctor"," in"," a"," small"," town"," that"," was"," filled"," with"," sheep"," farmers"," and"," had"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.98,0.0,0.0,0.0,0.98,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["sk","irts"," of"," the"," German"," empire"," in"," ","1","875",".\""," \"","His"," name"," was"," Robert"," Koch",".\""," \"","Robert"," Koch"," was"," a"," country"," doctor"," in"," a"," small"," town"," that"," was"," filled"," with"," sheep"," farmers"," and"," had"," an"," outbreak"," of"," ant"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.37,0.0,0.0,0.0,0.0,0.0,0.98,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ings"," entitled","\u2191"," Portraits"," of"," Distinguished"," Living"," Characters"," of"," Scotland",","," including"," Sir"," Walter"," Scott",","," Lord"," Jeffrey",","," Robert"," Burns"," and"," Professor"," Wilson",".","\u21ea"," NI","CIAS"," (","d","."," ","414"," ","B",".","C",".),"," a"," soldier"," and"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.97,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ancestors"," were"," t","be","\u23ce"," pat","rons"," and"," friends"," of"," the"," gr","tat","\u23ce","\u2191"," Scot","ch"," poet",","," Robert"," Burns",","," declining","\u23ce"," subscribe"," to"," t","be"," Burns"," monument","\u23ce","\u2191"," Kil","m","arn","ock"," because"," be"," dis","li","ked"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.97,0.16,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["opolit","ical"," analyst"," and"," historian",".\""," \"","Among"," his"," scholarly"," works"," are"," the"," unauthorized"," bi","ographies"," of","\""," \"","George"," Herbert"," Walker"," Bush"," and"," Barack"," Hussein"," Obama",".\""," \"","Since"," Bush"," the"," Elder"," made"," his"," speech"," at"," the"," United"," Nations"," back"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.97,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","Van","\u2191"," Dy","ck",",","\u2191"," St","ac","hel","berg",","," Stanford",",","\u23ce","!"," General"," Arthur",","," Robert"," Burns",","," and"," our"," famous","\u23ce"," La","\u2191"," Fl","or"," de"," la","\u2191"," Isab","ela"," Manila","\u2191"," Cig","ars"," A","H"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.97,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["isingly",","," bug","b","ear",".","com"," is"," taken",".","\u2191"," Surprisingly",","," it"," features"," Paul"," Graham",",","\u23ce","Robert"," Morris",","," Trevor","\u2191"," Black","well",","," Jessica","\u2191"," Living","ston"," and"," John"," McCarthy"," (","\u23ce","<","http","://","www","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.96,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["arium"," un","\u23ce"," til"," the"," dispat","ches"," from"," New"," York"," told","\u23ce"," of"," the"," arrest"," of"," a"," Rev","."," Robert"," Morris","\u23ce","\u2191"," Ke","mp"," in"," the"," home"," of"," Mr",".","\u2191"," Det","sch","el","."," for","\u23ce"," m","erly"," of"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.96,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["harmon","ium",".","\u21ea"," MISS","\u21ea"," MICH","ELL"," (","\u2191","Organ","ist"," Wesley"," Church","),","\u2191"," Pup","il"," of"," George"," Herbert","."," R",".","A",".","M",".,"," R",".","H","."," Hart",","," M",".","A",".,"," and"," Professor"," Carl"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.95,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ert",",","\u2191"," Ros","sl","n","l"," et"," le","\u2191"," S","tn","b","at",",","\u2191"," Ver","di",","," Richard"," Wagner",",","\u2191"," Tam","ber","\u2022","ll","ok",","," E",".","\u2191"," Ro","ssl",",","\u2191"," Cap","oul",",","\u2191"," M"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.94,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ture"," for"," and"," in"," the"," name"," of",","," and"," as"," the"," act"," and"," deed"," of"," them",","," the"," said"," Robert"," Morris"," and"," Mary",","," his"," wife",","," in"," order"," that"," the"," same"," may"," be"," recorded"," as"," such"," in"," due"," form"," of"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.94,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["As","\u2191"," Rus","kin"," turned"," from"," the"," criticism"," of"," works"," of"," art"," to"," the"," criticism"," of"," society",","," so"," William"," Morris"," turned"," from"," the"," making"," of"," works"," of"," art"," to"," the"," effort"," to"," remake"," society","."," Mr",".","\u2191"," Mack","ail"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.94,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["printStackTrace","();","\u23ce","\u21b9\u21b9\u21b9\u21b9","}","\u23ce","\u21b9\u21b9\u21b9","}","\u23ce","\u21b9\u21b9","}","\u23ce","\u21b9","}","\u23ce","}","<EOT>","the"," said"," Robert"," Morris"," and"," his"," he","irs",","," all"," and"," singular"," the"," said"," land"," and"," premises"," hereby"," granted"," or"," mentioned"," so"," to"," be"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.94,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","man"," who"," fav","ored"," free"," trade"," relations","\u23ce"," with"," the"," United"," States",","," was"," defeated"," |","\u23ce","by"," William"," Morris"," by"," ","800"," ","majority",".","\u23ce","\u2191","Recip","roc","ity","\u2191"," Candidate","\u2191"," Loses",".","\u23ce","I","\u23ce"," A"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.93,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["but"," now"," ","yon"," tell"," me"," these","\u23ce"," are"," supplied"," by"," the"," great"," literature"," of","\u23ce"," music","."," In"," Richard"," Wagner","'s"," music","\u23ce"," there"," is"," everything"," adapt","able"," to"," any","\u23ce"," drama"," ever"," conceived","."," And"," I"," can"," im","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.92,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ol"," i","C",".","\u2191"," Fran","ck",")","\u2191"," T","ann","h","\u00e2","user",","," ou","ver","ture"," (","Richard"," Wagner",")"," la","\u2191"," D","amn","ation"," de","\u2191"," F","aust"," i","\u2191"," Ber","li","oz",")","\u2191"," Trois"," ","airs"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.92,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["find"," out"," what"," was"," killing"," the"," local"," sheep",".\""," \"","So"," Koch"," started"," a"," series"," of"," experiments",".\""," \"","Robert"," Koch"," believed"," the"," g","erm"," theory"," of"," disease",","," that"," disease"," could"," spread"," by"," means"," of"," micro","bes",","," these"," things"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.92,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["e","."," This"," is"," the","\u23ce"," cheap","est"," book"," ever"," published",".","\u23ce","Complete","\u2191"," Po","etical"," Works"," of"," Robert"," Burns",",","\u23ce","with","\u2191"," Memoir"," and","\u2191"," Gloss","ary",","," illustrated",";"," paper",",","\u23ce","50"," ","cents",".","\u23ce"]}]},{"quantile_name":"Subsample Interval 0","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["."," attend"," a"," big"," fish"," try"," to"," bo","\u23ce"," cotton"," up"," In"," his"," honor",".","\u23ce","\u2191","T","ho"," Robert"," Burns"," was"," looked"," for"," last","\u23ce"," night",","," She"," l","jas"," a"," large"," lot"," of"," -","groc","eries"," for","\u23ce","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["a"," premi\u00e8re"," trouve"," sa"," r\u00e9","f","utation"," sous"," la"," pl","ume"," d","'","un"," art","iste",","," ","\u23ce","Richard"," Wagner",","," et"," d","'","un"," observ","ateur",",","\u2191"," Had","don","."," D","'","apr\u00e8s"," ","\u23ce","Wagner"," (","3"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["v","t",".,"," who"," arrived"," here","\u23ce"," New"," Year","'s"," day"," to"," become"," the"," u","rine","\u23ce"," or"," Lieutenant"," Robert"," Burns"," rar","qu","-","\u23ce","har","son",","," U","."," S","."," N","."," The"," wedding"," details","\u23ce"," were"," impro","mp"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["any"," of"," them",","," shall"," and"," will"," warrant"," and"," forever"," defend"," and"," further",","," that"," he",","," the"," said"," Robert"," Morris",","," and"," his"," he","irs",","," shall",","," and"," will",","," at"," any"," time"," or"," times"," h","ere","after"," at"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["mass","\u00e9",","," de","\u2191"," F\u00e9l","ic","ien"," David",","," et"," m\u00eame"," de"," ce"," g\u00e9","nie"," incomp","ris",","," Richard"," Wagner","."," A"," on","ze"," he","ures",","," le"," th","\u00e9"," fut"," ser","vi"," ;"," il"," \u00e9tait"," fait"," avec"," cette"," perf"]}]},{"quantile_name":"Subsample Interval 1","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.81,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["."," Their"," primary"," inventor"," was","\u2191"," Nikol","ai"," Tesla",","," an"," innov","ator"," working"," at"," the"," same"," time"," as"," Thomas"," Edison","."," Since"," their"," inception",","," AC"," in","duction"," motors"," have"," found"," their"," way"," into"," thousands"," of"," products",","," from"," consumer"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.81,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["he"," cons","orted"," and"," worked"," with"," other","\u2191"," Soci","alists",","," his"," desires"," and"," ","22"," ","\u21ea","WILLIAM","\u21ea"," MORRIS"," hopes",","," and"," therefore"," his"," methods",","," were"," differ","-"," ","ent"," from"," the","irs","."," They"," were",","," many"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[","," famous"," night"," ow","ls"," include"," President"," Obama",","," Charles"," Darwin",","," Adolf"," Hitler",",","\u23ce","Winston"," Churchill",","," James"," Joyce",","," Marcel","\u2191"," Pro","ust",","," Keith"," Richards"," and"," Elvis","\u23ce","\u2191"," Pre","sley","."," And"," finally",","," you"," definitely"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[","," and"," the"," world"," has"," la","mented"," the"," per","verse"," waste"," of"," natural"," powers"," ","18"," ","\u21ea","WILLIAM","\u21ea"," MORRIS"," which"," their"," rebellion"," caused","."," Indeed",","," in"," the"," case"," of"," Morris"," it"," has"," seemed"," to"," many"," that"," he"," quar"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["rors"," to"," dul","ness","."," You"," must"," persu","ade"," them"," that"," peace"," means"," a"," ","24"," ","\u21ea","WILLIAM","\u21ea"," MORRIS"," fuller"," and"," more"," gl","orious"," life"," than"," war",","," if"," you"," would"," make"," them"," desire"," it"," pass","ion","ately","."]}]},{"quantile_name":"Subsample Interval 2","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.71,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[".","NS","\u21ea"," LES","BAL","X","ANS"," La","\u2191"," Gr","\u00e8","ce"," et"," l","'","\u2191","Ent","ente"," Lord"," Robert"," Cecil"," a"," d\u00e9c","la","r\u00e9"," je","udi"," \u00e0"," la","\u2191"," Cham","bre"," des"," communes"," que"," les","-","gouvern","ements"," al","li"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.71,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["published"," ","9"," ","Mar","."," ","2","006",","," both"," by"," Patrick"," J",".","\u2191"," Mall","oy",","," Michael"," Cohen",","," and","\u2191"," Al","ain"," J","."," Cohen",","," address"," network"," analysis"," tasks"," that"," rely"," on"," determining"," a"," sequential"," dependency"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["people"," with"," no"," emotion"," are"," rare",",","\u23ce","but"," some"," of"," it"," was"," arguably"," confirmed"," later"," through"," interviews"," with"," John"," Wayne","\u23ce","\u2191"," G","acy"," who"," demonstrated"," much"," of"," the"," same"," behaviors"," but"," had"," learned"," to"," compens","ate","\u23ce"," and"," fake"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["to"," make","\u23ce"," massive"," urban"," projects"," happen","\u23ce\u23ce"," By"," God",","," I"," get"," it"," now","!"," Google"," have"," hired"," Robert"," Moses","!","\u23ce\u23ce","------","\u23ce","\u2191","Tep","ix","\u23ce"," If"," cars"," (","parking",")"," were"," banned"," inside"," most"," areas"," of"," the"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["covering",","," of"," no"," value"," to"," any"," city"," except"," myself",".","\u2191"," Reward"," if"," returned"," to"," my"," office","."," Robert"," Elliott","."," ","27"," ","south"," Royal"," street","."," FOR","\u21ea"," RENT","?","Six"," room"," house",","," city"," water","."," Washington"]}]},{"quantile_name":"Subsample Interval 3","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.61,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ance"," imaging","."," The"," ","'","\u2191","H","emo","-","Neural"," hypothesis",",'","recently"," proposed"," by"," sponsor"," Dr","."," Christopher"," Moore",","," pred","icts"," that"," hyper","emia"," may"," also"," mod","ulate"," neural"," exc","it","ability",".","\u2191"," H","emo","-","Neural"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.61,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["Inc","."," (\"","Philip"," Morris","\"),"," acc","ounted"," for"," ","26","%"," of"," the"," Company","'s"," sales","."," If"," Philip"," Morris"," were"," to"," stop"," purchasing"," li","co","rice"," extract"," from"," the"," Company",","," it"," would"," have"," a"," significant"," adverse"," effect"," on"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["Apple"," has"," a"," ","99","%"," approval"," rate"," for"," h","1","b"," apps","\u23ce\u23ce"," Then"," why"," do"," you"," think"," Tim"," Cook"," personally"," signed"," this"," letter","?","\u23ce\u23ce","[","https","://","s","3",".","amazonaws",".","com","/","b","rt",".","org"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["was"," completed",".","\u23ce","The"," United"," States"," transmitted"," an","\u23ce"," identical"," protest"," inform","ally"," to"," Gen","\u23ce"," ","eral"," Francisco"," Franco",","," commanding","\u23ce"," the"," Spanish"," rebel"," forces",","," but"," no","\u23ce"," word"," has"," been"," received"," concern","\u23ce"," ing"," the"," reaction"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ounded"," smart",".\""," \""," Yeah",".\""," \"","Do"," you"," think","\u2191"," St","one","wall"," Jackson"," is"," any"," relation"," to"," Michael"," Jackson","?\""," \"","And",","," we","'re"," back",".\""," \"","Just"," do"," your"," part",".\""," \""," So"," you"," all"," set"," for"]}]},{"quantile_name":"Subsample Interval 4","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.51,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["original",","," the"," one"," from"," the"," comics",".\""," \"","He"," might"," as"," well"," have"," told"," you"," his"," name"," was"," Bruce"," Wayne",".\""," \"","Well",","," I","'m"," sorry",","," but"," that","'s","..."," that","'s"," the"," only"," name"," he"," ever"," gave"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.51,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ediat","amente"," los"," prepar","at","ivos"," para"," la"," mar","cha"," sobre"," Madrid",","," para"," un","irse"," con"," el"," General"," Francisco"," Franco"," y"," sus"," trop","as",","," que"," marc","han"," del"," Sur"," sobre"," la"," capital",","," y"," con"," las"," colum","nas"," del"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["the"," guest"," actually","\u23ce"," said"," that"," he"," thought"," the"," inventor"," of"," Twitter"," would"," go"," down"," in"," history","\u23ce"," alongside"," Alexander"," Graham"," Bell"," and"," Samuel","\u2191"," Morse",".","\u23ce\u23ce","It","'s"," m","ild","ly"," innovative",","," but"," let","'s"," not"," go"," nuts"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["people"," lives",".","\u23ce\u23ce","------","\u23ce","roh","it","89","\u23ce","A"," life"," well"," lived",".","\u23ce\u23ce","R","IP"," Sir"," Christopher"," Lee","\u23ce\u23ce","------","\u23ce","r","qu","an","tz","\u23ce"," Same"," day"," as","\u2191"," Orn","ette"," Coleman","."," Amazing",".","\u23ce\u23ce","------"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["by"," Richard","\u2191"," Fe","yn","man","\u23ce\u23ce","------","\u23ce","dh","y","as","ama","\u23ce","\"","Money"," Ball","\""," by"," Michael"," Lewis"," is"," great","."," It","'s"," about"," evalu","ating"," talent"," in"," new","\u23ce"," ways"," to"," exploit"," market"," ineff","ici","encies"," in"]}]},{"quantile_name":"Subsample Interval 5","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["the"," movie"," _","\u2191","J","inn","ah","_","\u23ce\u23ce","------","\u23ce","st","ox","\u23ce"," He"," was"," a"," cousin"," of"," Ian"," Fleming"," and"," one"," of"," the"," inspir","ations"," for"," James"," Bond",".","\u23ce","What"," a"," remarkable"," man","!","\u23ce\u23ce","------","\u23ce","ach"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["m","0","n","key","_",">"," https","://","www",".","google",".","com","/","maps","/","place","/","Bruce","+","Wayne","'s","+","\u2191","Residence","/@","42",".","7","623","411",",-","83",".","283","347",",","17",".","75","z"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["there"," aren","'t"," any"," real"," wiz","ards",".\""," \"","Although",","," there"," is"," one",".\""," \"","Thomas","\u2191"," Al","va"," Edison",".\""," \"","The","\u2191"," Wizard"," of","\u2191"," Men","lo"," Park",".\""," \"","A"," truly"," great"," man",".\""," \""," Did"," he"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u2191","Brown","low"," has"," been"," here"," and"," gave"," the","\u2191"," Abol","ition","ists"," particular"," thunder",";"," then"," Mr","."," John"," Mitchell"," followed"," him",","," and"," sal","ted"," down"," the"," English"," Government","."," From"," the"," spec","iman"," of"," the"," antip","athy"," of"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["f","-","rc","rom","bie",",","  ","D",".","  ","D","."," \u2014"," The","  ","Rev",".","  ","James","  ","Madison","  ","to"," ","\u23ce","Dr",".","  ","Smith"," \u2014"," A","  ","\u2191","Daughter","  ","Born","  ","to","  ","Dr","."]}]},{"quantile_name":"Subsample Interval 6","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["a"," piece"," of"," j","unk","!\""," \"","There","'s"," nothing"," there","!\""," \"","You","'re"," brave"," ..."," talking"," about"," Bruce"," Lee","!\""," \"","What"," do"," you"," think",","," the"," basis"," you","'re"," an"," id","iot","!\""," \"","Mr","."," Liu"," offered"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["/"," group",").","\u2191"," Patients"," will"," be"," reviewed"," at"," six","-"," ^"," ...","j","n","th"," intervals"," at"," the"," Robert"," Wood"," Johnson"," Medical"," School"," clinical"," facility"," and"," additional"," telephone"," contacts"," with"," patients"," will"," be"," made"," every"," two"," months"," to"," optimize"," compliance"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ards",","," Oscar",",","\u23ce","i","lo","berts",","," James",".","\u23ce","\u2191","I","to","berts",","," Peter",".","\u23ce","Kay",","," Benjamin",".","\u23ce","\u2191","Seek","ins",","," John"," H",".","\u23ce","snow",","," James"," o",".","\u23ce","\u2191","Sp"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["nic","ker","bo","cker","'"," s","  ","History","  ","of","  ","New","  ","York",".","  ","By","  ","Washington","  ","Irving",".","  ","\u2191","Ill","us","\\xc2","\\xac",""," ","\u23ce","t","rations","  ","by","  ","E",".","  ","W","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["el","  ","\u2191","Lill","ian","    ","Cambridge","  ","Springs"," ","\u23ce\u23ce","\u2191","K","lin","gin","smith",",","  ","John","  ","Glenn","   ","\u2191","Con","ne","aut","  ","Lake"," ","\u23ce\u23ce","\u2191","Kr","amer",",","   ","\u2191","Nor","bert","  ","\u2191","C"]}]},{"quantile_name":"Subsample Interval 7","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["announcement"," to"," make",".\""," \"","Hey",","," what"," celebrity"," do"," you"," look"," like","?\""," \"","\u2191","","Uh",","," Bruce"," Willis",".\""," \"","\u2191","Mm","m",","," no",".\""," \"","\u2191","Hum","pty","\u2191"," Dum","pty","!\""," \"","\u2191","O","oh"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["you"," play"," with"," fire","...\""," \"","When"," you","'re"," casting"," around"," for"," fresh","...\""," \"","\u21ea","ANDREW",":\""," \"","Andrew"," Neil"," here","...\""," \"","\u2191","Shit","!\""," \"","God",","," his"," bloody"," chair","'s"," barely"," cold","!\""," \"","The"," king"," is"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u2014","\u2191"," Paul","ine"," Guy",".","Publications","."," \u2014","\u2191"," Lib","ourg",","," Alexandre",","," <","66",","," rue"," Saint"," Denis",","," a"," Paris"," et","\u2191"," Nev","eu",","," Louise",","," ","71",","," G","ran","de","R","ue",".","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["."," (","English"," version",")"," Question"," for"," written"," answer"," E","-","010","710","/","12"," ","to"," the"," Council"," David"," Martin"," (","S","&","D",")"," (","23"," ","November"," ","2","012",")"," Subject",":"," EU","-","Japan"," Free"," Trade"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["hand",".\""," \""," What"," is"," it",","," anyway","?\""," \""," \"","\u2191","Claud","ius"," the"," God","\""," by"," Robert","\u2191"," Graves",".\""," \"","A"," fine"," historical"," reconstruction"," of"," the"," life"," of","\u2191"," Claud","ius",","," the"," Roman"," emperor",","," thought"," of"]}]},{"quantile_name":"Subsample Interval 8","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.52,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["so","\""," by"," Louis","\u2191"," M","alle",".\""," \"","Ok",".","Fritz"," Lang",","," Jules","\u2191"," Das","sin",","," Nicholas"," Ray","...\""," \"","Search"," on"," the"," top",".\""," \"\"","\u2191","F","anny"," and"," Alexander","\","," \"","\u2191","Fell","ini"," Eight"," and"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ien"," door"," zijn"," st","rek","king"," in"," l","ijn","rech","te"," str","ijd"," is"," met"," de"," ge","est"," van"," Karl"," Marx","."," Is"," het"," eer","bi","edi","ging"," van"," het"," g","enie",","," wann","eer"," een"," d","erg","elijk"," dog","mat"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","acknowledged","  ","the","  ","auth","orship","  ","in","  ","a","  ","letter","  ","to","  ","Sir","  ","Walter","  ","Scott",",","  ","and","  ","sent","  ","him","  ","two","  ","con"," ","\u23ce","tin","u","ations","  ","of","  ","the"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[",","\u2191"," N","eb",".,"," in"," response"," to"," a"," telegram"," that"," his"," father"," is"," seriously"," ill","."," \u2014","Judge"," Peter"," Jonas"," assessed"," a"," fine"," of"," $","10"," ","each"," of"," eleven"," gaming"," cases"," in"," the"," county"," court"," this"," morning",","," in"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u21ea","HARVEY","\u2014","Nov","."," ","20",","," at"," the"," American"," mission",","," Cairo"," Egypt",","," the"," Rev","."," William"," Harvey",","," D",".","D",".,"," brother"," of"," Andrew",","," James",","," Elizabeth"]}]},{"quantile_name":"Bottom Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["he"," (","92",",","\u2191"," Gh",".-","\u2191","El","ys","\u00e9es","),"," Good"," by"," Mr",".","\u2191"," Chips","."," Lord"," Byron"," :"," L","'","\u2191","Autre"," (","C",".","\u2191"," Gran","it",").","\u2191"," Lu","tet","ia"," :","\u2191"," Circ","onst"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["Spencer",","," who"," is"," attending"," Northwestern"," university"," at"," Chicago",","," spent"," Saturday"," and"," Sunday","."," Sunday"," at"," home","."," Edward"," Jensen"," or"," ","5"," ","\u2191","E","ighty"," Avenue"," will"," leave"," tomorrow"," morning"," for","\u2191"," Mon","r","ovia",","," Cal",".,"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["for"," EM","C","\u2191"," Compliance",","," IEEE"," No","."," PC","5","595",","," and"," Howard"," W","."," Johnson"," and"," Martin"," Graham"," (","1","993",")"," High","-","Speed"," Digital"," Design",":"," A","\u2191"," Handbook"," of"," Black"," Magic",","," PT","R","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["DA","Y","\u2191"," Kem","per","\u2191"," Bo","cock"," ","320"," ","S",".,"," E","."," C",".,"," To"," Charles"," Henry"," Webb"," ","160"," ","\u21ea","SHEL","LEY",","," WITH"," A","\u21ea"," COPY"," OK","\u2191"," Harri","et"," Monroe"," ","313"," ","\u21ea"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["."," ll","yman"," of","\u23ce","\u2191"," Ful","ton",",","\u2191"," ","Ky",".","\u23ce","Mr","."," and"," Mrs","."," Robert"," Arnold",",","\u23ce","daughter",","," Christie"," May",","," and"," Mrs",".","\u23ce","Arnold","'s"," sister",","," Miss"," Ruth","\u2191"," Os","ehe"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","is"," the"," guest"," of"," her"," sister",","," Miss","\u23ce","\u2191"," Min","nie","\u2191"," Sm","y","the",".","\u23ce","Thomas"," Berry"," and"," son"," Fred",",","\u23ce","of"," Logan",","," were"," the"," guests"," of","\u23ce"," Mr","."," and"," Mrs","."," Walt","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["w","ily"," as"," a"," w","eas","el"," and"," as"," craf","ty","\u23ce","\u25a0","as"," a"," fox",","," Thomas"," E","."," Watson"," has",","," for","\u23ce"," the"," present"," at"," least",","," clev","erly"," ou","tw","itted","\u23ce"," Uncle"," Sam",".","\u23ce","The"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["icide","-","resistant","\u2191"," Crops","\","," Chapter"," ","4"," ","pp","."," ","53","-","83","."," ed","."," Stephen"," Duke",","," Lewis","\u2191"," Pub",","," C","RC"," Press","\u2191"," B","oca","\u2191"," R","aton",",","\u2191"," F","la","."," "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.52,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["so","\""," by"," Louis","\u2191"," M","alle",".\""," \"","Ok",".","Fritz"," Lang",","," Jules","\u2191"," Das","sin",","," Nicholas"," Ray","...\""," \"","Search"," on"," the"," top",".\""," \"\"","\u2191","F","anny"," and"," Alexander","\","," \"","\u2191","Fell","ini"," Eight"," and"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["ien"," door"," zijn"," st","rek","king"," in"," l","ijn","rech","te"," str","ijd"," is"," met"," de"," ge","est"," van"," Karl"," Marx","."," Is"," het"," eer","bi","edi","ging"," van"," het"," g","enie",","," wann","eer"," een"," d","erg","elijk"," dog","mat"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["\u23ce","acknowledged","  ","the","  ","auth","orship","  ","in","  ","a","  ","letter","  ","to","  ","Sir","  ","Walter","  ","Scott",",","  ","and","  ","sent","  ","him","  ","two","  ","con"," ","\u23ce","tin","u","ations","  ","of","  ","the"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":[",","\u2191"," N","eb",".,"," in"," response"," to"," a"," telegram"," that"," his"," father"," is"," seriously"," ill","."," \u2014","Judge"," Peter"," Jonas"," assessed"," a"," fine"," of"," $","10"," ","each"," of"," eleven"," gaming"," cases"," in"," the"," county"," court"," this"," morning",","," in"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["\u21ea","HARVEY","\u2014","Nov","."," ","20",","," at"," the"," American"," mission",","," Cairo"," Egypt",","," the"," Rev","."," William"," Harvey",","," D",".","D",".,"," brother"," of"," Andrew",","," James",","," Elizabeth"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["esen"," glasses",",","\u23ce","Mrs"," Bird","\u2191"," W","aller",";"," one","-","fourth"," barrel","\u23ce"," il","our","."," Miss"," Helen"," Martin",";"," box"," cig","ars",".","\u23ce","I","\u2191"," Hast","ings",",","\u2191"," Bl","ades",";"," box"," ro","ap","."," Mary"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u2191","L","au","tr","ec","\u23ce","233"," ","Portrait"," de"," Georges"," Henri"," Manuel","\u23ce","\u2191"," Port","ret"," van"," Georges"," Henri"," Manuel","\u23ce","\u2191"," Temp","era"," op"," kar","ton",","," ","83"," ","x"," ","50"," ","cm",";"," gem",".:"," a"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["sted","elijk"," Museum",","," Amsterdam","\u23ce\u23ce\u23ce\u23ce","\u2191"," Toulouse","\u2191"," L","au","tr","ec","\u23ce","233"," ","Portrait"," de"," Georges"," Henri"," Manuel","\u23ce","\u2191"," Port","ret"," van"," Georges"," Henri"," Manuel","\u23ce","\u2191"," Temp","era"," op"," kar","ton",","," ","83"," ","x"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["a","et","UT",".","\u23ce","'","\u21ea"," PLEA"," FOR","\u21ea"," OVER","WOR","KED","\u21ea"," SCHOOL","\u21ea"," CHILDREN","'.","\u23ce","William"," Penn",":"," \"","\u2191","B","less"," my"," soul",","," children",","," still"," stu","dj","ing","!"," You"," ought"," to"," tie"," out"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["6"," ","el"," Sr",".","\u2191"," G","\u00f3m","ez","\u2191"," Jord","ana"," que"," tamp","oco"," i","sor","\u00ed"," que"," Karl"," Marx"," describ","\u00eda"," el"," proceso"," do"," (","s","ra"," ","\\xc3","\\x8d","Q","O","t","ible"," precis","ar"," cal","ando"," pod"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["k","ling","enberg","."," Eduard"," Ludwig","\u2191"," Lind","holm",","," Johann"," Leopold"," Julius","  ","\u2191","Kl","au",","," Julius"," Robert"," Hermann",",","\u2191"," Maxim","ilian"," Arthur","\u2191"," Ga","ede",","," Carl","\u2191"," Pl","ath",",","\u2191"," Ad","olph"," Friedrich"," Julius","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","two"," chief"," just","ices",",","\u2191"," Se","ym","our"," of"," Connecticut"," ami","\u23ce"," Rhodes"," of"," California",","," Judge"," David"," Davis"," and","\u23ce"," Judge","\u2191"," Shep","ley",","," in"," which"," cases",","," as"," far"," as"," their"," of","\u23ce"," f","ice"," indicates"]}]}],"top_logits":["Jr","'s","(","Sr","(@","of","III","jr","and",","],"bottom_logits":["Zobacz","iba","Duitse","namen","\u00c5r","INCREMENT","Franse","Intent","begriff","ymes"]}