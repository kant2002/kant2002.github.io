{"index":11417417,"examples_quantiles":[{"quantile_name":"Top Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["la","zy","power",":"," so",","," since"," a"," search"," on"," demo",".","j","uj","uch","arms",".","com"," for"," the"," software"," package"," we"," were"," discussing"," before"," doesn","'t"," appear"," to"," exist","..."," the"," most"," I"," was"," able"," to"," do"," was"," add"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.97,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["reg","lo","."," Los"," que"," no"," da","dera"," original","idad","."," ^","yan"," qued","ado"," conv","enc","idos"," de"," la"," historia"," que"," ll","evo"," he","H","abl","ando","\u2191"," H","oy","os"," del"," est","ilo",","," partic","ular","mente"," del"," de"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.92,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ez","at","cha",">"," long","b","on","gl","ung","fish",":"," someone"," that"," i"," know"," had"," the"," same"," issue"," with"," online"," banking","."," the"," site"," wo","uld","nt"," allow"," for"," anything"," other"," than"," the"," official"," java"," to"," be"," used",".","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.89,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["sF","a","lf","hood","  ","ft","ill","  ","extends","  ","its","  ","f","elf","  ","to","  ","the","  ","div","iding","  ","or","  ","fe","pe","rating","  ","the"," ","\u23ce","p",".","  ","III",".","  ","Light","  ","from","  "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.86,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ex","o",">","\u2191"," Hm","m",","," he"," left",","," but"," I"," think"," the"," clean","est"," solution"," would"," probably"," be"," fast","-","export"," |"," fast","-","import","-","filter"," |"," fast","-","import"," ?","\u23ce","<","r","ozz","in",">"," Or"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.84,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ult"," for","\u23ce"," the"," company",".","\"\"","\u23ce\u23ce","Since"," then",","," the"," E","FF"," spoke"," up"," loud","ly"," against"," the"," right"," to"," be"," forgotten"," (","Google","\u23ce"," Spain"," v","\u21ea"," A","EP","D"," and"," Mario","\u2191"," Cost","eja"," Gonz\u00e1lez","),"," even"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.84,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["the"," F","UD",","," ok","?"," VS"," has"," nothing"," to"," do"," with"," dum","bing"," down"," learning"," any"," language"," or"," any"," process",".","\u23ce","*"," For","ge","A","us"," ub","un","tu","ized"," and"," x","ub","un","tu","ized"," his"," kub","untu"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.84,0.59,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["of","  ","men","  ","under","  ","heaven","  ","(","considering"," ","\u23ce","the","  ","not","or","iety","  ","of","  ","matter","  ","of","  ","fact","  ","to","  ","the","  ","contrary",")","  ","but","  ","can","  ","with","  ","less"," "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.84,0.78,0.42,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["device"," and"," a"," memory"," device"," which"," allow"," substantially"," avoiding"," or"," at"," least"," reducing"," the"," above","-","mentioned"," problems"," of"," the"," state"," of"," the"," art",".","<EOT>","1","."," Field"," of"," the","\u2191"," Invention","\u23ce"," The"," present"," invention"," relates"," to"," a"," head"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.82,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["reported"," on"," the"," Q","A"," page"," in"," any"," of"," gust","y",","," fe","is","ty",","," or"," sid"," with"," that"," source"," (","and"," I"," don","'t"," have"," an","\u2191"," ","Etch"," ch","root",").","  ","The"," file"," looks"," clean",","," and"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.81,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["The","High","Child",":"," -","t"," ","1","222"," ","worked"," for"," me",","," I"," have"," yet"," to"," try"," the"," startup"," trick","\u23ce","<","hol","ot","one",">"," The","High","Child",":"," I","'ll"," try"," in"," a"," bit"," and"," report"," back"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["."," Germany",").","\u23ce","There"," is"," no"," indication"," in"," the"," above"," publication"," that"," any"," of"," the"," above"," products"," use"," a"," critical"," weight"," ratio"," of"," palm","itic"," to"," st","ear","ic"," acids",".","<EOT>","1","."," Field"," of"," the","\u2191"," Invention","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["a"," pl","anner"," some","\u23ce"," constraints",","," and"," comp","iling"," the"," result"," sounds"," much"," easier"," that"," this"," person","'s","\u23ce"," day"," job",".","\u2191"," Embedded"," Java"," already"," does"," some"," of"," this"," while"," some"," systems"," did"," it"," in","\u23ce"," hardware"," to"," avoid"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.79,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["is"," of"," a"," similar"," mind","set","\u23ce"," that"," there"," is"," just"," not"," enough"," evidence"," to"," support"," a"," lot"," of"," the"," mind","/","gut","\u23ce"," theories","."," Same"," goes"," for"," the"," metal"," tox","icity"," and"," vaccination"," links","."," She","'s"," not","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.78,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ment"," they"," may"," know"," of"," a"," user"," gu","ilde","\u23ce","<","_","cur","ux","z","_",">"," i"," doubt"," a"," clean"," install"," would"," be"," broken"," :",")","\u23ce","<","man","v","eru",">","\u2191"," D","elv","ien",":"," you"," are"," on"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.78,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["gave"," up"," on"," the"," idea"," because"," I"," decided"," it"," was"," illegal",".","\u23ce\u23ce","Now"," in"," this"," thread"," people"," are"," saying"," multiple"," such"," platforms"," exist","."," Did"," I"," make","\u23ce"," a"," mistake"," or"," get"," something"," wrong","?"," Or"," are"," they"," applying"," some"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.78,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ub","ility"," of"," the"," same"," is"," under"," ","30"," ","\u03bc","g","/","ml",".","\u23ce","\u2191","Accordingly",","," the"," intensive"," and"," thor","ough"," research"," on"," ox","a","zo","li","din","one"," derivatives",","," conducted"," by"," the"," present"," inventors"," a","iming"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.84,0.78,0.42,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["and"," a"," memory"," device"," which"," allow"," substantially"," avoiding"," or"," at"," least"," reducing"," the"," above","-","mentioned"," problems"," of"," the"," state"," of"," the"," art",".","<EOT>","1","."," Field"," of"," the","\u2191"," Invention","\u23ce"," The"," present"," invention"," relates"," to"," a"," head","rest"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.78,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["default"," PHP"," modules"," without"," re","-","comp","iling","."," So"," due"," to"," the"," problems"," with"," re","-","comp","iling"," and"," time"," time"," required"," to"," re","pack","age"," it"," I"," am"," just"," not"," going"," to"," worry"," bout"," it",".","\u23ce","<","anz"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.78,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["david","_",">"," d","rb","obb","."," just"," trying"," to"," make"," life"," a"," little"," easier","."," try"," goog","ling"," \"","index"," of","\""," flash","(","whatever",").","so","\u23ce","<","fl","acc","id",">"," i"," can"," only"," help"," if"," you"," listen"]}]},{"quantile_name":"Subsample Interval 0","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.97,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["reg","lo","."," Los"," que"," no"," da","dera"," original","idad","."," ^","yan"," qued","ado"," conv","enc","idos"," de"," la"," historia"," que"," ll","evo"," he","H","abl","ando","\u2191"," H","oy","os"," del"," est","ilo",","," partic","ular","mente"," del"," de"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.92,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["ez","at","cha",">"," long","b","on","gl","ung","fish",":"," someone"," that"," i"," know"," had"," the"," same"," issue"," with"," online"," banking","."," the"," site"," wo","uld","nt"," allow"," for"," anything"," other"," than"," the"," official"," java"," to"," be"," used",".","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.89,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["sF","a","lf","hood","  ","ft","ill","  ","extends","  ","its","  ","f","elf","  ","to","  ","the","  ","div","iding","  ","or","  ","fe","pe","rating","  ","the"," ","\u23ce","p",".","  ","III",".","  ","Light","  ","from","  "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.86,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["ex","o",">","\u2191"," Hm","m",","," he"," left",","," but"," I"," think"," the"," clean","est"," solution"," would"," probably"," be"," fast","-","export"," |"," fast","-","import","-","filter"," |"," fast","-","import"," ?","\u23ce","<","r","ozz","in",">"," Or"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.84,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["ult"," for","\u23ce"," the"," company",".","\"\"","\u23ce\u23ce","Since"," then",","," the"," E","FF"," spoke"," up"," loud","ly"," against"," the"," right"," to"," be"," forgotten"," (","Google","\u23ce"," Spain"," v","\u21ea"," A","EP","D"," and"," Mario","\u2191"," Cost","eja"," Gonz\u00e1lez","),"," even"]}]},{"quantile_name":"Subsample Interval 1","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.81,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["The","High","Child",":"," -","t"," ","1","222"," ","worked"," for"," me",","," I"," have"," yet"," to"," try"," the"," startup"," trick","\u23ce","<","hol","ot","one",">"," The","High","Child",":"," I","'ll"," try"," in"," a"," bit"," and"," report"," back"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["."," Germany",").","\u23ce","There"," is"," no"," indication"," in"," the"," above"," publication"," that"," any"," of"," the"," above"," products"," use"," a"," critical"," weight"," ratio"," of"," palm","itic"," to"," st","ear","ic"," acids",".","<EOT>","1","."," Field"," of"," the","\u2191"," Invention","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["a"," pl","anner"," some","\u23ce"," constraints",","," and"," comp","iling"," the"," result"," sounds"," much"," easier"," that"," this"," person","'s","\u23ce"," day"," job",".","\u2191"," Embedded"," Java"," already"," does"," some"," of"," this"," while"," some"," systems"," did"," it"," in","\u23ce"," hardware"," to"," avoid"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.79,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["is"," of"," a"," similar"," mind","set","\u23ce"," that"," there"," is"," just"," not"," enough"," evidence"," to"," support"," a"," lot"," of"," the"," mind","/","gut","\u23ce"," theories","."," Same"," goes"," for"," the"," metal"," tox","icity"," and"," vaccination"," links","."," She","'s"," not","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.78,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["ment"," they"," may"," know"," of"," a"," user"," gu","ilde","\u23ce","<","_","cur","ux","z","_",">"," i"," doubt"," a"," clean"," install"," would"," be"," broken"," :",")","\u23ce","<","man","v","eru",">","\u2191"," D","elv","ien",":"," you"," are"," on"]}]},{"quantile_name":"Subsample Interval 2","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.71,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["re","ats",".","\u2191"," Start","ups"," considering"," sexy"," new"," cheap"," T","LD","'s"," may"," want"," to"," rec","ons","ider","\u23ce"," cost"," /"," benefit"," metrics"," of"," such"," path",".","\u23ce\u23ce","On"," Thursday",","," April"," ","23"," ","at"," ","9",":","10"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.71,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.41,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["Un","ivers","al","S","uper","Box",">"," @","John","_","","athan",","," You","'ve"," been"," told"," multiple"," times"," what"," information"," we"," need"," to"," help"," you","."," And"," no",","," no"," one"," has"," said"," anything"," about"," issues"," on"," the"," nex","us"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["cs","lle"," du"," ci","-","dev","ant"," ","\u23ce","a","\u2191"," P","au","chard"," qui",","," au"," lieu"," de"," d","roit",","," n","'","a"," app","ris"," ","\u23ce","e"," que"," l","'","alphabet",","," pour"," ar","river"," \u00e0"," t","aire"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.45,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["to"," the"," target"," is"," not"," always"," possible",".","\u23ce","In"," another"," aspect"," of"," the"," state"," of"," the"," art"," technology"," of"," high"," sensitivity"," infra","red"," imaging"," systems",","," it"," should"," be"," noted"," that"," the"," two","-","dimensional"," array"," of"," C","CD"," elements"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["take"," a"," look"," and"," that"," might"," make"," things"," go"," faster","."," if"," not",","," need"," m","vo","'s"," input"," on"," relative"," priority"," for"," me"," to"," look"," at"," this","\u23ce","<","j","d","st","rand",">"," if"," we"," can","'t"," come"," up"]}]},{"quantile_name":"Subsample Interval 3","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.52,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["This"," is"," good"," news"," for"," diab","etics",","," although"," I"," find"," it"," pretty"," dep","ressing"," that","\u23ce"," something"," like"," a"," quarter"," of"," the"," US"," population"," needs"," something"," like"," this"," in"," the","\u23ce"," first"," place",".","\u23ce\u23ce","~~~","\u23ce","templ","ae","dh"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["'t"," really"," matter"," -"," again"," the"," point"," is"," that"," anybody","\u23ce"," reading","\u2191"," Object","iv","ism"," qual","ifies"," as"," a"," subject",".","\u23ce\u23ce","By"," the"," way",",","\u2191"," Rand"," does"," discuss"," epis","tem","ology"," and"," metaph","ys","ics"," in"," some"," detail"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["------","\u23ce","zen","zen","\u23ce"," There"," are"," a"," lot"," of"," claims"," about"," why"," F","#"," is"," better"," and"," not"," much"," data"," to"," back","\u23ce"," them"," up","."," I","'d"," really"," like"," to"," learn"," F","#",","," but"," are"," there"," proven"," benefits"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ory","  ","opinions","  ","and","  ","utter","ances","  ","on","  ","the","  ","value","  ","of"," ","\u23ce","the","  ","two","  ","phases","  ","of","  ","its","  ","development","  ","as","  ","an","  ","educational","  ","discipline",";","  ","and","  "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["int","rod","ij","ced",","," or"," in"," vit","ro","."," In"," considering"," the"," phenomena"," of"," im","-"," mun","ity"," against"," organized"," bodies"," of"," all"," kinds",","," phenomena"," which"," may"," be"," app","ro","-"," pri","ately"," considered"," under"," the"," general"," head","-"]}]},{"quantile_name":"Subsample Interval 4","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","say"," that"," any"," more","."," So"," that"," when"," they"," were"," forced"," ","\u23ce","to"," believe"," that"," there"," is"," a"," principle"," of"," broth","erly"," love"," ","\u23ce","and"," kind","ness"," through"," the","\u2191"," Chur","di"," of"," Christ",","," beginning"," ","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["nt"," feel"," like"," going"," into"," the"," specif","ics"," of"," windows"," memory","\u23ce"," management"," when"," i"," and"," others"," gave"," you"," the"," resources"," to"," learn"," more",","," so"," i"," listed","\u23ce"," one"," common"," case"," that"," everyone"," can"," appreciate","."," As"," per"," your"," original"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["in","  ","\u2191","Hints","  ","on","  ","a"," Horse","-"," ","\u23ce","race",",\""," apply","  ","to","  ","the","  ","profound","  ","and","  ","calculating"," ","\u23ce","t","hin","kers","  ","on","  ","the","  ","tu","rf",",","  ","when","  "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[","," pra","hab","ita"," deliber","at","ione"," m","atura"," ","\u23ce","ded","arat"," ,"," dec","ern","it"," &"," de"," f","ini"," t"," ,"," h","uj","uf","m","odi","\u2191"," Doc","\u00ec","ri","nam"," erro"," \u2022"," ","\u23ce","ne","am"," e","/"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["licensed"," and"," that"," they","'re","\u23ce"," extract","ing"," every"," penny"," that"," they"," can","."," They"," simply"," don","'t"," need"," the"," majority"," of","\u23ce"," the"," partners"," in"," order"," to"," do"," this",".","\u23ce\u23ce","Microsoft",","," of"," course",","," is"," after"," the"," large"]}]},{"quantile_name":"Subsample Interval 5","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.41,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["2"," ","months",",","\u23ce","started"," getting"," lots"," of"," spam"," on"," it","."," I"," don","'t"," recall"," that"," company"," getting"," data","\u23ce"," bre","ached",","," so"," totally"," unclear"," how"," that"," happened",".","\u23ce\u23ce","~~~","\u23ce","","ala","nh","\u23ce"," I","'ve"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.41,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["by"," various"," writers"," and"," some"," results"," have"," been"," achieved","/"," Three"," myths"," in"," particular"," bear"," upon"," the"," problem"," of"," this"," book",","," (","1",")"," that"," of"," the"," prim","eval"," monster"," of"," darkness"," and"," disorder",","," (","2",")"," that"," of"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["other"," good"," ideas"," have"," been"," taken","."," There"," is","\u23ce"," nothing"," wrong"," with"," this",","," but"," the"," cost"," with"," this"," fast"," p","aced"," approach"," is"," that","\u23ce"," the"," oldest"," and"," most"," complex"," industries"," like"," agriculture"," are"," going"," to"," put"," you","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["av","ons"," \u00e9t\u00e9"," fort","ement"," press","\u00e9s"," \u00e0"," ne"," pas"," fer","mer"," le"," volume"," sans"," v","enir"," \u00e0"," nouveau"," au"," sang"," de","\u2191"," J","\u00e9s","us","-","Christ"," qui"," pu","rif","ie"," de"," tout"," p\u00e9","ch\u00e9","."," Et"," si"," tous"," ne"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","The"," present"," invention"," can"," be"," applied"," us","efully"," to"," all"," containers"," where"," it"," is"," to"," be"," avoided"," that"," a"," medium",","," in"," particular"," the"," liquid"," contents",","," le","aks"," into"," the"," wall"," elements"," of"," the"," container","."," In"]}]},{"quantile_name":"Subsample Interval 6","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.41,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["2"," ","months",",","\u23ce","started"," getting"," lots"," of"," spam"," on"," it","."," I"," don","'t"," recall"," that"," company"," getting"," data","\u23ce"," bre","ached",","," so"," totally"," unclear"," how"," that"," happened",".","\u23ce\u23ce","~~~","\u23ce","","ala","nh","\u23ce"," I","'ve"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.41,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["by"," various"," writers"," and"," some"," results"," have"," been"," achieved","/"," Three"," myths"," in"," particular"," bear"," upon"," the"," problem"," of"," this"," book",","," (","1",")"," that"," of"," the"," prim","eval"," monster"," of"," darkness"," and"," disorder",","," (","2",")"," that"," of"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["other"," good"," ideas"," have"," been"," taken","."," There"," is","\u23ce"," nothing"," wrong"," with"," this",","," but"," the"," cost"," with"," this"," fast"," p","aced"," approach"," is"," that","\u23ce"," the"," oldest"," and"," most"," complex"," industries"," like"," agriculture"," are"," going"," to"," put"," you","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["av","ons"," \u00e9t\u00e9"," fort","ement"," press","\u00e9s"," \u00e0"," ne"," pas"," fer","mer"," le"," volume"," sans"," v","enir"," \u00e0"," nouveau"," au"," sang"," de","\u2191"," J","\u00e9s","us","-","Christ"," qui"," pu","rif","ie"," de"," tout"," p\u00e9","ch\u00e9","."," Et"," si"," tous"," ne"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["\u23ce","The"," present"," invention"," can"," be"," applied"," us","efully"," to"," all"," containers"," where"," it"," is"," to"," be"," avoided"," that"," a"," medium",","," in"," particular"," the"," liquid"," contents",","," le","aks"," into"," the"," wall"," elements"," of"," the"," container","."," In"]}]},{"quantile_name":"Subsample Interval 7","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.41,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["2"," ","months",",","\u23ce","started"," getting"," lots"," of"," spam"," on"," it","."," I"," don","'t"," recall"," that"," company"," getting"," data","\u23ce"," bre","ached",","," so"," totally"," unclear"," how"," that"," happened",".","\u23ce\u23ce","~~~","\u23ce","","ala","nh","\u23ce"," I","'ve"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.41,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["by"," various"," writers"," and"," some"," results"," have"," been"," achieved","/"," Three"," myths"," in"," particular"," bear"," upon"," the"," problem"," of"," this"," book",","," (","1",")"," that"," of"," the"," prim","eval"," monster"," of"," darkness"," and"," disorder",","," (","2",")"," that"," of"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["other"," good"," ideas"," have"," been"," taken","."," There"," is","\u23ce"," nothing"," wrong"," with"," this",","," but"," the"," cost"," with"," this"," fast"," p","aced"," approach"," is"," that","\u23ce"," the"," oldest"," and"," most"," complex"," industries"," like"," agriculture"," are"," going"," to"," put"," you","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["av","ons"," \u00e9t\u00e9"," fort","ement"," press","\u00e9s"," \u00e0"," ne"," pas"," fer","mer"," le"," volume"," sans"," v","enir"," \u00e0"," nouveau"," au"," sang"," de","\u2191"," J","\u00e9s","us","-","Christ"," qui"," pu","rif","ie"," de"," tout"," p\u00e9","ch\u00e9","."," Et"," si"," tous"," ne"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["\u23ce","The"," present"," invention"," can"," be"," applied"," us","efully"," to"," all"," containers"," where"," it"," is"," to"," be"," avoided"," that"," a"," medium",","," in"," particular"," the"," liquid"," contents",","," le","aks"," into"," the"," wall"," elements"," of"," the"," container","."," In"]}]},{"quantile_name":"Subsample Interval 8","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.41,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["2"," ","months",",","\u23ce","started"," getting"," lots"," of"," spam"," on"," it","."," I"," don","'t"," recall"," that"," company"," getting"," data","\u23ce"," bre","ached",","," so"," totally"," unclear"," how"," that"," happened",".","\u23ce\u23ce","~~~","\u23ce","","ala","nh","\u23ce"," I","'ve"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.41,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["by"," various"," writers"," and"," some"," results"," have"," been"," achieved","/"," Three"," myths"," in"," particular"," bear"," upon"," the"," problem"," of"," this"," book",","," (","1",")"," that"," of"," the"," prim","eval"," monster"," of"," darkness"," and"," disorder",","," (","2",")"," that"," of"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["other"," good"," ideas"," have"," been"," taken","."," There"," is","\u23ce"," nothing"," wrong"," with"," this",","," but"," the"," cost"," with"," this"," fast"," p","aced"," approach"," is"," that","\u23ce"," the"," oldest"," and"," most"," complex"," industries"," like"," agriculture"," are"," going"," to"," put"," you","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["av","ons"," \u00e9t\u00e9"," fort","ement"," press","\u00e9s"," \u00e0"," ne"," pas"," fer","mer"," le"," volume"," sans"," v","enir"," \u00e0"," nouveau"," au"," sang"," de","\u2191"," J","\u00e9s","us","-","Christ"," qui"," pu","rif","ie"," de"," tout"," p\u00e9","ch\u00e9","."," Et"," si"," tous"," ne"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["\u23ce","The"," present"," invention"," can"," be"," applied"," us","efully"," to"," all"," containers"," where"," it"," is"," to"," be"," avoided"," that"," a"," medium",","," in"," particular"," the"," liquid"," contents",","," le","aks"," into"," the"," wall"," elements"," of"," the"," container","."," In"]}]},{"quantile_name":"Bottom Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.41,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["oys"," me",".","\u23ce","<","y","oss","ar","ian","uk",">"," I"," cant"," work"," with"," des","kt","ops"," without"," a"," task"," bar","...","\u23ce","<","M","oon","Unit","`",">"," wish"," dolph","in"," acted"," the"," same"," way"," as"," c","aja",","]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.41,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["son"," and"," my"," daughter"," both"," taught"," english"," in","\u2191"," Taipei"," a"," few"," y","rs"," back"," ,","and"," my"," son","'s"," wide"," is"," from"," there","\u23ce","<","T","OM","_","ot","ak","ux",">"," in"," my"," notebook",","," the"," system"," settings"," became"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.41,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["  ","dans","  ","l","'","\u2191","Apo","tre"," ","\u23ce","ce","  ","qu","'","il","  ","trouve","  ","de","  ","favorable","  ","\u00e0","  ","ses","  ","id","\u00e9es","  ","et","  ","pas","\u23ce"," se","-","t","-","il","  ","sous","  "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.41,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ien",","," so","gar","\u2191"," M\u00e4nn","er"," der","\u2191"," Wissensch","aft"," haben"," die"," Be"," haupt","ung"," von"," der","\u2191"," Mehr","heit"," bew","ohn","ter","\u2191"," Wel","ten"," best","rit","ien"," und"," be","stre","iten"," ins","onder","heit"," im","mer"," wieder"," die"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.41,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[">"," bd","de","bian",","," don","'t"," worry"," :",")","\u23ce","<","d","oko",">"," iw","j",":"," added"," a"," background"," section",","," left"," in"," your"," review"," comment","."," please"," remove"," it"," if"," you"," think"," it","'s"," under","stand","able","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.41,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["able"," to"," me","\u23ce","<","jab","ra",">"," anyone"," know"," if"," there"," is"," any"," sites"," that"," can"," help"," someone"," with"," warning"," from"," l","int","ian","\u23ce","<","charles",">"," y","ay"," nig","ger"," masters"," of"," the"," universe","!!","!!!","\u23ce","<"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.41,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ulating"," materials"," used"," in"," mult","ich","ip"," cube"," structure",".","\u23ce","The"," afor","ement","ioned"," needs"," are"," satisfied"," by"," the"," device"," and"," process"," of"," the"," present"," invention"," which"," is"," directed"," to"," the"," manufacture"," of"," a"," high"," density"," semiconductor"," structure"," having"," reduced"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.41,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","<","ricardo","_",">"," e"," tem"," mais"," pa","rit","\\xc3","\\x83","","\u00a7","\u2191","","\u00c3","\u00b5","es"," se"," liga","\u23ce","<","x","d","o","ctor",">"," tal","v","ez"," na"," sua"," pr","\\xc3","\\x83","","\u00b3","x","ima"," inst"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.41,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["2"," ","months",",","\u23ce","started"," getting"," lots"," of"," spam"," on"," it","."," I"," don","'t"," recall"," that"," company"," getting"," data","\u23ce"," bre","ached",","," so"," totally"," unclear"," how"," that"," happened",".","\u23ce\u23ce","~~~","\u23ce","","ala","nh","\u23ce"," I","'ve"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.41,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["by"," various"," writers"," and"," some"," results"," have"," been"," achieved","/"," Three"," myths"," in"," particular"," bear"," upon"," the"," problem"," of"," this"," book",","," (","1",")"," that"," of"," the"," prim","eval"," monster"," of"," darkness"," and"," disorder",","," (","2",")"," that"," of"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["other"," good"," ideas"," have"," been"," taken","."," There"," is","\u23ce"," nothing"," wrong"," with"," this",","," but"," the"," cost"," with"," this"," fast"," p","aced"," approach"," is"," that","\u23ce"," the"," oldest"," and"," most"," complex"," industries"," like"," agriculture"," are"," going"," to"," put"," you","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["av","ons"," \u00e9t\u00e9"," fort","ement"," press","\u00e9s"," \u00e0"," ne"," pas"," fer","mer"," le"," volume"," sans"," v","enir"," \u00e0"," nouveau"," au"," sang"," de","\u2191"," J","\u00e9s","us","-","Christ"," qui"," pu","rif","ie"," de"," tout"," p\u00e9","ch\u00e9","."," Et"," si"," tous"," ne"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":true,"tokens":["\u23ce","The"," present"," invention"," can"," be"," applied"," us","efully"," to"," all"," containers"," where"," it"," is"," to"," be"," avoided"," that"," a"," medium",","," in"," particular"," the"," liquid"," contents",","," le","aks"," into"," the"," wall"," elements"," of"," the"," container","."," In"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["my"," client","'s"," guilt",".\""," \"","Mr",".","\u2191"," Flor","rick"," was"," sentenced"," for"," ","DP","'","ing"," cases"," in"," trade"," for"," sexual"," fav","ors",","," so"," sex"," is"," entirely"," relevant"," to"," his"," guilt",","," Your"," Honor",".\""," \"","The"," fact"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["role"," in"," sl","owing"," down"," a"," system","\u23ce","<","\u2191","Ki","los",">"," maybe"," the"," nvidia","-","good","ie"," no"," happy","\u23ce","<","\u2191","Ki","los",">"," i"," forget"," what"," it"," was","\u23ce","<","\u2191","Ki","los",">"," current","\u23ce","<"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["I"," don","'t"," get"," is"," why"," he"," compl","ains"," about"," that"," gem"," when"," he"," obviously","\u23ce"," found"," an"," alternative"," that"," better"," suited"," his"," needs","."," Why"," for"," heaven","'s"," sake"," not","\u23ce"," just"," use"," this"," but"," raise"," an"," issue","."," If"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["  ","was","  ","reached","  ","to","  ","carry","  ","permanently","  ","a","  ","part"," ","\u23ce","of","  ","the","  ","public","  ","debt","  ","in","  ","the","  ","form","  ","of","  ","Treasury","  ","savings","  ","certificates","  ","it"," ","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["sont","  ","l\u00e0","  ","par","  ","m","alh","eur","  ","les","  ","liv","res","  ","les","  ","plus","  ","prom","pts"," ","\u23ce","\u00e0","  ","sort","ir","  ","de","  ","France","  ","et","  ","\u00e0","  ","\u00e9m","ig","rer","  ","\u00e0"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["solution"," and"," production"," of"," suspended"," matter"," appear"," as"," the"," continuous"," processing"," proceeds",".","\u2191"," He","ret","of","ore",","," no"," essential"," res","olutions"," have"," been"," found",".","<EOT>","1","."," Field"," of"," the","\u2191"," Invention","\u23ce"," The"," present"," invention"," relates"," to"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["de","  ","dem","ander","  ","\u00e0"," V",".","  ","M",".","  ","ny","  ","le"," ","\u23ce","min","ift","c","rc","  ","du","  ","fer"," ,"," ny","  ","l","'"," v","f","age","  ","du","  ","f","eu"," ,"," ny","  "]}]}],"top_logits":["RePEc","blica","INSEE","uset","\u0442\u0435\u043c\u043f\u0435\u0440\u0430\u0442","Geboren","\u00e8g","Bibliografia","naio","\u00edtic"],"bottom_logits":["\u23ce","and","in","instead","so","\u2191","as",""," ","vs"]}