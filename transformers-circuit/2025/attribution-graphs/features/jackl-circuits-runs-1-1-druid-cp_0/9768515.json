{"index":9768515,"examples_quantiles":[{"quantile_name":"Top Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["as"," of"," February"," ","3",","," ","2","022",","," between","\u2191"," Her","tz"," Global"," Holdings",","," Inc",".,"," and"," Stephen"," M",".","\u2191"," Sc","her","r"," (","incorporated"," by"," reference"," to","\u2191"," Exhibit"," ","10",".","7"," ","to"," the"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.99,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["is"," to"," draw"," cart","oons"," ",":-","p","\u23ce\u23ce","[","https","://","en",".","wikipedia",".","org","/","wiki","/","Stephen","_","\u2191","Hill","enburg","#","Early","_","ca","ree","...","](","https","://","en",".","wikipedia",".","org","/","wiki"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.98,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["wolf","ram",".","com","/","language","/","ref","/","Image","Ident","ify",".","html",")","\u23ce\u23ce","A"," blog","post"," from"," Stephen","\u2191"," Wolf","ram"," about"," this"," site"," and"," related"," topics",":","\u23ce","[","http","://","blog",".","steph","enw","olf","ram"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.98,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["sharing"," Agreement"," dated"," as"," of"," April"," ","22",","," ","2","022"," ","between"," The","\u2191"," Her","tz"," Corporation"," and"," Stephen"," M",".","\u2191"," Sc","her","r"," (","incorporated"," by"," reference"," to","\u2191"," Exhibit"," ","10",".","21"," ","to"," the"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.98,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["then"," put"," them"," together",":"," life","\u23ce"," can"," be"," similar",".)"," We"," sang"," mostly"," \"","parl","or"," music","\","," like"," Stephen"," Foster"," mel","odies","\u23ce"," from"," an"," old"," book",","," etc"," etc",".,"," and"," kid","ded"," each"," other"," and"," had"," po"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.98,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["may"," be"," employed","."," Further",","," the"," ex","tr","usion"," coating"," given","\u21ea"," LIQUID","\u21ea"," FILM","\u21ea"," COATING"," authored"," by"," Stephen"," F",".","\u2191"," K","ist","ler"," and","\u2191"," Pet","ert"," M",".","\u2191"," Schwe","izer"," (","\u21ea","CHAPMAN"," and","\u21ea"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.65,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.97,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["donna"," D","."," and"," Donald"," M","."," Lambert"," Laboratory"," of","\u2191"," M","yel","oma","\u2191"," Genetics"," and"," the"," Nancy"," and"," Stephen"," Grand"," Laboratory"," for","\u2191"," M","yel","oma","\u2191"," Prote","om","ics"," at"," the","\u2191"," M","yel","oma"," Institute"," for"," Research"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.97,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.93,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["~~~","\u23ce","adam","d","o","upe","\u23ce"," I"," didn","'t"," realize"," this"," until"," I"," read"," \"","On"," Writing","\""," by"," Stephen"," King","\u23ce","(<","http","://","www",".","amazon",".","com","/","Writing","-","Stephen","-","King","/","dp","/","0"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.97,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["b","annon"," gone",","," I"," think"," the"," only"," immigration"," hard","-","li","ners"," left"," in"," Trump","'s","\u23ce"," circle"," are"," Stephen"," Miller"," and"," his"," chief"," of"," staff"," John"," Kelly"," who"," could"," very","\u23ce"," easily"," quit"," before"," IC","E"," has"," a"," chance"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.97,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ology",",","\u2191"," Diabetes"," &","\u2191"," Metabolism"," ","\u23ce","in"," the"," Department"," of"," Medicine"," at"," the"," University"," of"," Florida"," and"," Stephen"," A","."," Harrison",","," MD",","," Director",","," ","\u23ce","Summit"," Clinical"," Research"," and"," company"," management",","," the"," Company"," announced"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.97,0.0,0.0,0.0,0.0,0.0,0.68,0.0,0.0,0.0,0.0,0.57,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["See"," infra","ctions"," Name"," '"," [","un","read","able","]"," '"," Last",",","\u2191"," Jer","old"," McC","ur","dy",","," Stephen","\u2191"," Sc","hen","ker",","," Marc","\u2191"," Hamm","ock",","," Bruce","\u2191"," G","ee",",","\u2191"," Shir","ley","\u2191"," Lap"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.97,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["lip","inski"," (","Washington"," Hospital"," Center",").","\u2191"," Collaboration"," between","\u2191"," A","rai"," Lab",",","\u21ea"," AM","RI"," Core"," and"," Stephen","\u2191"," Ep","stein",".","\u2191"," Imaging"," labeled"," li","pos","omes"," in"," inf","arc","ted"," mouse"," my","oc","ard","ium","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.97,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["possibility"," OF","\u21ea"," SUCH","\u21ea"," DAMAGE",".","\u23ce"," ","*/","\u23ce\u23ce\u23ce","/**","\u23ce"," ","*","\u23ce"," ","*"," @","author"," Stephen","\u2191"," Daw","son","-","\u2191","H","agger","ty","\u23ce"," ","*"," @","version"," $","\u2191","Revision",":"," ","1","."]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.97,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.94,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[")"," it"," works",".","\u23ce\u23ce","[","1","]","\u23ce","[","http","://","en",".","wikipedia",".","org","/","wiki","/","Stephen","_","\u2191","El","op","](","http","://","en",".","wikipedia",".","org","/","wiki","/","Stephen","_","\u2191","El","op"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.97,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce\u23ce","------","\u23ce","\u2191","L","uc","\u23ce"," Not"," my"," cup"," of"," tea"," but"," it"," seems"," nic","ely"," done",","," and"," Stephen","\u2191"," F","ry"," likes"," it"," so","\u23ce"," there","'s"," that",":","\u23ce","[","https","://","twitter",".","com","/","steph"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.97,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["practically","\u23ce"," in","ext","ric","able"," from"," general"," life"," stuff",".","\u23ce\u23ce","You"," can","'t"," learn"," how"," to"," write"," like"," Stephen"," King"," by"," ","aping"," his"," methods","."," You","'re","\u23ce"," just"," never"," going"," to"," get"," there",","," doesn","'t"," matter"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.97,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ause",")\""," \"","Hello",".\""," \"","Hello"," and"," welcome"," to"," Q","I",",\""," \"","The"," quiz"," that"," rh","ymes"," with"," Stephen","\u2191"," F","ry",".\""," \"","Now"," let","'s"," meet"," the"," members"," of"," our"," happy"," band",",\""," \"","Al"," Davies",","]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.97,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["the","\u2191"," Preparations",","," de","duced"," from"," Original","\u2191"," Experiments",".","\u2191"," Translated"," from"," the"," Second"," German"," Edition",","," by"," Stephen","\u2191"," Dar","by","."," ","18","mo","."," cloth",","," ","6","s","."," M","R",".","\u21ea"," YEAR","SLEY"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.97,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["'t"," we",","," sir","?\""," \"","A"," California"," University"," ","7",":","00","am"," Pacific"," Standard"," Time","\""," \"","Professor"," Stephen","\u2191"," Mal","ley","'s"," Office","\""," \"","\u2191","Y","ep","?\""," \""," Morning",".\""," \""," Morning",".\""," \"","You"," want"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.97,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce\u23ce","Between"," the"," World"," and"," Me"," by"," Ta","-","\u2191","Neh","isi","\u2191"," Co","ates","\u23ce\u23ce"," Band"," of"," Brothers"," by"," Stephen"," E",".","\u2191"," Amb","rose","\u23ce\u23ce"," The","\u2191"," Mill","ionaire","\u2191"," Fast","l","ane"," by"," M","J"," De","Mar","co"]}]},{"quantile_name":"Subsample Interval 0","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["Point",".\""," \"[","\u2191","Ch","uck","les","]","\""," \"","\u2191","Excellent",".\""," \"","That","'s"," a"," draft"," letter"," from"," Kevin"," to"," the"," Arizona"," reg","ulator",".\""," \"","I"," know"," he"," likes"," the"," personal"," touch",".\""," \""," He","'s"," gonna"," love"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u2191","M","um","!\""," \"","\u21ea","RADIO",":"," ","'","One"," of"," the"," chief"," witnesses"," in"," the"," trial"," of"," Dr"," Stephen"," Ward"," ","'","has"," admitted"," p","erj","uring"," herself",".","'\""," \"","Today",","," President"," Kennedy"," and"," his"," wife","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9,0.0,0.0,0.45,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\"","Sometimes"," they"," see"," more"," like"," the"," brothers"," than"," cous","ins",".\""," \"","You"," had"," a"," head"," start",".\""," \"","Stephen",","," take"," Conrad"," inside"," and"," wash"," up"," for"," lunch",".\""," \"","And",","," you"," young"," lady",","," you"," better"," scr"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["card","\u23ce"," under"," head"," of"," \"","New",","," To","-","Day",",\""," in","\u23ce"," ou"," advertising"," columns",","," Dr","."," Stephen","\u23ce"," s","ou"," has"," returned"," f","*","om"," his"," recent"," visit","\u23ce"," to"," ","\u03af","\u03af","'.","","\u03bf"," East"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["and"," gains"," acceptance"," (","as"," the"," presenter"," mentioned","\u23ce"," many"," regulatory"," hur","dles",")"," and"," fast","!","\u23ce\u23ce","<EOT>","\u23ce\u23ce","Kevin"," Rose",":","\u2191"," Di","gg"," Failed"," Because"," ","'","Social"," Media","\u2191"," Grew"," Up","'"," -"," horn","ok","pl","ease"]}]},{"quantile_name":"Subsample Interval 1","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.81,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.51,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["in"," English"," custom",".\""," \"","I"," can"," say"," no"," more",","," except"," I"," love"," you",".\""," \"","Ever"," faithful",","," Jonathan",".\""," \"[","\u21ea","THUNDER","\u21ea"," RUM","BLING","]","\""," \"","\u21ea","JONATHAN",":\""," \"","The"," letters"," I"," have"," written"," have"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.81,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["'s"," gas"," operated",","," you"," see",".\""," \"","Gas"," operated","?\""," \"","It"," shoots"," real"," fast",".\""," \"","Well",","," Michael",","," can"," you"," shoot","?\""," \"","\u2191","Prof","iciently",","," Father",".\""," \"","Well","..."," as"," many"," times"," as"," you"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["once"," again","\""," \"","\u2191","Brid","get"," k","eller",".\""," \"","And"," deb","uting"," at"," number"," seven",","," Mr","."," Brian"," Davis",".\""," \"","You"," probably"," all"," think"," he"," just"," got"," lucky",","," being"," my"," son",","," and"," caught"," a"," good"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.26,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["the"," man"," who"," bum","ped"," into"," the"," tree"," was"," a"," rare"," occasion"," when"," it"," got"," through",".\""," \"","I"," remember"," Richard"," Curtis"," and","\u2191"," Ro","wan"," coming"," in"," very"," early"," on"," and"," saying",",\""," \"\"","We","'ve"," got"," this"," great",","]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["votes"," that"," we"," need",".\""," \"","Just"," leaves"," me"," and"," leg","range"," and","...\""," \"","That","'s"," him",".\""," \"","Steven"," turner",".\""," \"","He"," just"," canceled",".\""," \"","I","'m"," so","..."," what"," came"," up","?\""," \"","And"," so"," it"]}]},{"quantile_name":"Subsample Interval 2","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["editor"," of"," The"," Business"," Week",","," New"," York",","," advoc","ated"," international"," stabil","ization"," of"," mon","ev",";","39"," ","Albert"," H",".","\u2191"," Wig","gin",",","\u2191"," Gh","air","man"," of"," the"," Chase"," National"," Bank",","," New"," York",","," recommended"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.64,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u2191","D","illy","  ","Samuel",",","  ","S",".","  ","\u2191","Nor","wood","."," ","\u23ce\u23ce","\u2191","D","ink","  ","Andrew",",","  ","vin","egar","  ","manufacturer",",","  ","res",".","  ","135","  ","Indiana","  ","ave","."," ","\u23ce","\u2191"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.33,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u2191","Ker","sten"," b","rock"," u"," .","\u2191"," Adel","heid","\u2191"," Bart","els"," ,","\u2191"," Louis","en","str"," ."," \u2014"," Gustav"," ,"," S"," ."," v"," .","\u2191"," ","Ack","erer"," Wilhelm","\u2191"," Eigen"," u"," .","\u2191"," Kar","oline"," Fu"," -"," ,"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[".\""," \"","My"," guest"," tonight"," once"," sang"," about"," a"," semi"," by"," the"," sea",","," and"," f","unn","ily"," enough",","," James"," May"," once"," had"," a"," similar"," experience"," but"," when"," a"," d","read","n","ought"," sailed"," by",".\""," \"","\u2191","Anyway",","]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7,0.0,0.0,0.69,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["fast",".\""," \"(","\u21ea","CAMERAS","\u21ea"," CLICKING",")\""," \"","I"," will"," now"," field"," questions",".\""," \"","\u21ea","WOMAN",":\""," \"","Gustav","!\""," \"","Gustav",","," my"," question"," is",","," can"," you"," ride"," fast","?\""," \"","\u2191","Fuck"," you","!\""," \"","Next"]}]},{"quantile_name":"Subsample Interval 3","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.56,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["year"," ago",".\""," \"","Wait",","," I"," thought"," you"," broke"," up"," today",".\""," \"","I"," thought"," you"," came"," here"," from"," Jimmy","\u2191"," P","esto","'s",".\""," \"","No",","," no",","," no",".\""," \"","He","'s"," at"," Jimmy","\u2191"," P","esto"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["uts",".\""," \"","She","'s"," all"," fl","uff",".\""," \"","There","'s"," nothing"," in"," there",".\""," \"","\u2191","Wow",","," Louise",","," you"," seem"," a"," little"," worked"," up"," about"," this",".\""," \"","Do"," I","?\""," \"","I"," agree"," with"," Louise",".\""]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.55,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["he","rit","\u23ce"," age",","," but"," because"," the"," experiment","\u23ce"," required"," twins",","," and"," their"," mother","\u23ce"," was"," willing"," for"," Johnny"," tod"," Jimmy","\u23ce"," to"," participate"," In"," It",".","\u23ce","\u2191","Taken"," to","\u2191"," Clinic"," Daily",".","\u23ce","The"," experiment"," was"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["hand","ker","ch","ief","...\""," \"","..","then"," she"," knocked"," on"," the"," door"," of"," a"," hand","some"," young"," prince"," named"," George",","," who"," gave"," her"," all"," his"," massive"," collection"," of"," Christmas"," presents",","," and"," she"," lived"," happ","ily"," ever"," after",".\""]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.61,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6,0.0,0.0,0.0,0.0,0.68,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["  ","\u2191","H","ez","ek","iah","  ","\u2191","Vs","her",",","  ","Edward"," ","\u23ce","\u2191","Raw","son",",","  ","John","  ","Hull",",","  ","Peter","  ","\u2191","Ol","li","uer",",","  ","\u2191","Jo","su","ah","  ","\u2191","Scott","ow"]}]},{"quantile_name":"Subsample Interval 4","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["."," ","\u23ce\u23ce","\u2191","Ver","bre","itung",":","\u2191"," Trans","sil","van","ien"," (","\u2191","S","ill",".,"," H","."," Otto","),","\u2191"," Ung","arn"," (","C","."," Koch",",","\u2191"," D","ol",".),","\u2191"," Ti","rol"," (","\u2191","A","uss"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.46,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["'re"," going"," to"," be"," finding"," another"," ride",".\""," \"","We"," weren","'t"," at"," any"," retreat",".\""," \"","No",".\""," \"","Andy"," and"," Jake"," are"," actually"," beer"," p","ong"," players"," from"," the"," University"," of","\u2191"," ","Ith","aca",","," and"," technically"," I"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.48,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u2191","Conn",","," at"," ","36","th"," and","\u2191"," Ever","ett","\u2191"," St","x","."," N",".","W",".","\u23ce","Henry"," ","8",".","\u2191"," L","uff","berry",","," D",".","D",".,"," Pastor","\u23ce"," Henry"," W",".","\u2191"," Sn","yder"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u23ce","F",".","\u21ea"," DI","VERS",","," President","."," J","."," D",".","\u21ea"," BELL",","," Secretary",".","\u23ce","\u21ea","JOSEPH","\u21ea"," CAR","PER","."," Vice","\u2191"," P","res","."," J","."," T",".","\u21ea"," CARL","TON",".","\u2191"," Treasurer","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.44,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.34,0.0,0.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["U","l","aa"," Hit"," Policy",".","\u23ce","Sun"," Juan"," de","\u2191"," Tor","to"," Rico","."," General","\u23ce"," Guy"," V","."," Henry",","," Military"," Governor"," of","\u23ce"," the"," Island"," of"," I","'","or","to","\u2191"," Hi","oo",","," in"," a"," series"," of"]}]},{"quantile_name":"Subsample Interval 5","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["the"," studio","?\""," \"","\u2191","Cud","dle"," up"," a"," little"," bit",".\""," \"","There"," you"," go",".\""," \"","Hey",","," Guy",".\""," \"","How"," are"," you"," tonight","?\""," \"","I"," need"," a"," little"," help"," from"," you",".\""," \"","Take"," these"," Johnson"]},{"tokens_acts_list":[0.37,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.76,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.77,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["alan","-","\u23ce","kay",".","wik","ia",".","com","/","wiki","/","Alan","_","Kay","_","Wiki","](","http","://","alan","-","\u23ce","kay",".","wik","ia",".","com","/","wiki","/","Alan","_","Kay","_","Wiki",")","\u23ce\u23ce","------","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u2191","M","enz","rum",".\""," \"","There","'s"," got"," to"," be"," something"," here"," some"," cl","ue",","," somewhere",".\""," \""," Ryan",".\""," \""," What"," are"," you"," doing"," here","?\""," \""," How"," did"," you"," get"," in","?\""," \""," The"," door"," was"," open"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["like"," a"," conform","ist","\""," \"","But"," wasn","'t"," conform","ism"," the"," cause"," of"," the"," third"," reich"," anyway","?\""," \"","Dylan"," Dog"," nightmare"," investigation","\""," \"","Oh"," it","'s"," you"," I"," thought"," you"," weren","'t"," coming"," anymore","\""," \"","This"," way"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.64,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["'"," break","f","asts"," was"," attended"," by","\u23ce"," Prince","\u2191"," ","Loo"," is"," Napoleon"," Bonaparte",","," after","\u23ce"," ","wards"," Napoleon"," III",";"," Dr",".","\u2191"," What","ley",","," the","\u23ce"," protestant"," archbishop"," of"," Dublin",";"," Lord","\u23ce"," William","\u2191"," P"]}]},{"quantile_name":"Subsample Interval 6","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["1","888",","," ","189","o",".","\u2191"," Ner","eis"," ir","ro","rat","\u00e2",""," DE","\u21ea"," SAINT","-","\u21ea","JOSEPH",","," i","898",","," ","1","906",";","\u21ea"," F","AU","VEL",","," ","1","900",";"," MAC"," ~","\u21ea"]},{"tokens_acts_list":[0.28,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.29,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["fernando",">"," jud","gen",":"," sorry"," but",".."," is"," that"," a"," game"," o"," what","??","\u23ce","<","e","ross",">"," fernando"," is"," it",":","  ","/","etc","/","X","11","/","x","org",".","conf","\u23ce","<","fernando",">"," e","ross"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.37,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.17,0.0,0.0,0.0,0.0,0.51,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[".","\u2191"," Frost"," of"," Lawrence",",","\u23ce","for"," ","utter"," deser","tion",".","\u2191"," Custody"," of"," their","\u23ce"," children",","," Rex"," Lloyd",",","\u2191"," Il","elle"," Felix",",","\u2191"," Cy","\u23ce"," ","ril",").","\u2191"," Al","gie","\u2191"," The","resa",","]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["cr","rs"," and","\u2191"," Mun","r","ocs"," were"," there","."," There"," were"," two","\u2191"," U","rides"," \u2014"," Mr","*","."," Augustus","\u2191"," Pl","c","ann","ton"," (","a"," run","away"," match",","," she"," being"," a"," rich"," bd","re","aa"," from"," I"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.31,0.0,0.0,0.3,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["It"," would"," be"," awful",","," wouldn","'t"," it","?\""," \"","It"," would"," be"," a"," crime",".\""," \"","Carlo","!\""," \"","Carlo","!\""," \"","What"," was"," that","?\""," \"","What"," is"," it","?\""," \"","Oh",","," something"," silly",".\""," \"","You"," wouldn"]}]},{"quantile_name":"Subsample Interval 7","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.32,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.26,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.52,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["in"," ST",".\""," \"","Louis","..",".,"," ","\\xe2\\x99","\\xaa","\""," \"","\\xe2\\x99","\\xaa",""," Meet"," me"," in"," St","."," Louis",","," Louis"," ","\\xe2\\x99","\\xaa","\""," \""," I","'ll"," be"," out"," in"," a"," minute",","," Agnes",".\""," \""," All"," right"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["  ","vas","ie","  ","spl","end","eur",",","  ","etc","."," ","\u23ce\u23ce","22",".","  ","\u2191","Que","  ","\u2191","Dante","  ","cr","\u00fbt","  ","ou","  ","non","  ","saint","  ","\u2191","Den","ys","  ","l","'","\u2191","\u00c2","r\u00e9","op"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["of"," Under"," Secretary"," of"," State","^","or"," rather"," Chief","\u2191"," Clerk"," of"," the"," State"," Department","\u2191"," Wash","-"," ","ington"," Irving"," was"," very"," intimate"," in"," the"," li","im","ily"," of"," Mr",","," Mc","T","j","ane","."," and"," sent"," messages"," to"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["http","://","www",".","c","bc",".","ca","/","spark","/","2","011","/","11","/","full","-","interview","-","luis","-","von","-","","ahn","-","on","-","du","ol","ingo","/","\u23ce\u23ce","======","\u23ce","an","tics","\u23ce"," Technical"," note"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.23,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["."," Only"," one"," night"," on"," the"," road",","," Denver"," to"," St","."," Louis",","," if"," you"," take"," the"," St","."," Louis"," Special",","," the","\u2191"," Bur","ling","tons","'"," new"," and"," spl","end","id","ly"," equipped"," Denver","-","St","."," Louis"]}]},{"quantile_name":"Subsample Interval 8","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["har"," vi"," ikke"," hav","t"," f\u00f8r",".","\u2191"," K","anske"," det"," ikke","\u23ce"," havde"," h","\u00e6n","dt",","," hvis"," ikke"," Anders"," og","\u2191"," K","jer","sti","\u23ce","\u2191"," Bj","\u00f8","rh","olt",","," af"," de"," ","\u00e6ld","ste","\u2191"," Sett","ler"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.12,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["yr","inth","ine"," membrane"," five"," years"," ago",".\""," \"","All"," right",","," then"," it","'s"," not"," you",","," it","'s"," Dana",".\""," \"","Dana"," would"," never"," will","ingly"," go"," along"," with"," something"," like"," this",".\""," \"","\u2191","Will","ingly","?\""," \""]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\"","\u2191","Processor","\u2191"," Implemented"," Communications"," Interface"," Having"," External"," Clock","\u2191"," Actu","ated","\u2191"," Dis","abling"," Control","\""," to","\u2191"," Daniel","s",","," dis","cl","oses"," an"," automated"," m","ailing"," system"," having"," a"," peripheral"," controller"," interface"," establishing"," communication"," links"," with"," peripheral"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["be","  ","de","po","fe","df","rom","  ","a","U","F","un","f","tion"," ","\u23ce","in","  ","this","  ","Kirk",".","  ","\u2191","L","ikt","w"," ","ik",",","\u2191","Patron","ages",",","  ","as","  ","er","oding","  ","the"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ych","."," w","em"," zap","rze","czy","\u0107",","," \u017ce"," w","\u2191"," Niem","cz","ech"," stos","un","ki"," nie","\u2191"," J\u00f3z","ef","\u2191"," P\u0142","on","ka"," ","2"," ","K",":"," X","."," ","2"," ","K",";"," Na"," list","\u0119"]}]},{"quantile_name":"Bottom Activations","examples":[{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["la","\u2191"," R","eine"," de"," tous"," les"," e","spr","its"," bi","enh","eur","eux",","," l","'","incomp","a","rable"," Marie",","," ne"," peuvent"," ab","order"," pr\u00e8s"," du"," tr","\u00f4ne"," de","\u2191"," D","ieu"," si","\u2191"," J","\u00e9s","us"," ne"," les"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["zes",","," mar","bre",","," m","eu","bles",","," etc",".,","\u2191"," Succ","\u00e8s"," :"," ","9",","," r","."," Lafayette","."," Paris","."," Le"," prix"," des","\u21ea"," PET","ITES","\u21ea"," ANN","ONCES"," est"," de"," ","1"," ","fr","."," "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.06,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":[">"," Yes",","," was"," looking"," for"," that"," or"," something"," about"," Services","...","\u23ce","<","lucas","_",">"," hello","\u23ce","<","lucas","_",">"," i"," have"," a"," huge"," problem","!"," http","://","paste",".","ubuntu",".","com","/","293","460","/","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["un"," bos","que"," pr\u00f3","ximo"," al","\u2191"," ","R\u00f3d","ano"," que"," h","oy"," se"," ll","ama"," bos","que"," de"," San"," Gil",".","\u2191"," All","\u00ed"," se"," aliment","aba"," con"," la"," le","che"," de"," una"," c","ier","va"," que",","," pers","egu"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["merge","-","upstream"," should"," work","\u23ce","<","rodrig","o","_",">"," but"," there"," might"," be"," a"," better"," way","\u23ce","<","rodrig","o","_",">"," s","eb","128",","," p","itti",","," di","dr","ocks"," ","^^","\u23ce","<","n","ess","ita"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["."," J","."," ","LE","\u21ea"," BOU","CHER",".","\u21ea"," SOUS"," ","LE","\u21ea"," REGNE"," D","\\xc3","\\x89","","\u21ea"," PIERRE","\u21ea"," LA","VAL"," ","\u23ce","Un"," train"," de","\u2191"," T","oj","ag","eurs"," d","\u00e9r","aille"," pr\u00e8s"," de","\u2191"," Bay"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["called"," in"," make","files"," as"," well","\u23ce","<","s","eb","128",">"," check"," maybe"," what"," they"," did"," there","\u23ce","<","rodrig","o","_",">"," ah",","," wait",":","\u23ce","<","rodrig","o","_",">"," Rest","Ext","ras","-","@","API","_"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["short"," window"," whilst"," discovery"," is"," in"," progress","\u23ce","<","dimit","ern",">"," v","oid","space",","," sounds"," good","\u23ce","<","dimit","ern",">"," v","oid","space",","," and"," we"," do"," have"," another"," test"," to"," check"," that"," client"," lo","gins"," are"," blocked"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.83,0.0,0.0,0.0,0.0,0.67,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.68,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["five"," men"," named"," In","\u23ce"," the"," w","rit",","," Patrick","\u2191"," D","olan",","," William"," War","\u23ce"," ","ner",","," Cameron"," Miller",",","\u2191"," Ur","iah","\u2191"," Boll","ng","ham","\u23ce"," and"," Edward"," McK","ay"," are"," rest","rained"," from","\u23ce"," matching"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["b","rest"," =","\u2191"," K","iew","\u2191"," Cre","fe","lder","\u2191"," D","ux"," ="," B","d","ub","A"," B","\u2191"," Elis","ab"," ."," =","\u2191"," West","b"," ."," (","\u2191"," Ho","md"," ","ger"," ."," )"," ","48"," ","."," "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["little"," dynam","ite"," for"," you"," black"," fel","las"," to"," play"," with",".\""," \"[","laugh","s","]","\""," \"","Now",","," Floyd",","," you"," got"," that"," rifle"," up"," on"," the"," wagon",","," don","'t"," ya","?\""," \"","Yeah",","," right",".\""," \""]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["av","unt","ain","iento"," de"," ."," M"," \u00e9"," x"," i"," c"," o"," \u2014"," El"," se\u00f1","or"," reg","idor"," D","."," Salvador","\u2191"," Conde",","," com","ision","ado"," pa","."," ra"," presid","ir"," en"," el"," teatro"," de"," N"," u"," e"," v"," v"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["f\u00f6","rm","ig"," gef","h","we","if","tz"," der","\u2191"," ","Mund"," w","ohl"," ge","bild","et",","," der","\u2191"," Bart"," ft","arf"," ","\u23ce","und"," lock","ig",";"," die","\u2191"," Au","gen"," gro","\u00df",","," von"," f","h","war","zer"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["o","_",":"," Yeah",","," http","://","pa","ste","bin",".","ubuntu",".","com","/","260","360","/","\u23ce","<","rodrig","o","_",">"," stat","ik",":"," does"," that"," give"," you"," some"," cl","ue"," if"," there","'s"," some"," certificate"," missing"," on"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\\xc3","\\x99","\\xc3","\\x9b","\\xc3","\\x99","","\u23ce","<","sa","eid",">"," ever","pl","ays",":"," ping","...","\u23ce","<","ahmad",">","\u2191"," ","\u00da","\u00a9","","\\xc3","\\x99","\\xc3","\\x9a","","\u00a9","\u23ce","<","ahmad",">","\u2191"," ","\u00da","\u00a9"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["thr","illing"," drama"," and","\u23ce"," a"," sen","sn","ti","oun","l"," dual"," character","ization"," by","\u23ce"," the"," extremely"," talented","\u2191"," Bert","\u2191"," L","yt","ell"," make","\u23ce","\"","Brother",".",",\""," the"," Him"," adaptation"," of"," the","\u23ce",".","en","ant","ionn"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.14,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.07,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["ada",","," ha","br","\u00e1"," ten","ido"," bast","ante"," m\u00e9","rito"," por"," su"," am","abil","idad",";"," su"," h","ija"," Mar\u00eda","\u2191"," Joaqu","ina",","," cas","ada"," con"," D","."," Francisco","\u2191"," Dur","ango",","," a","gra","ci","ada",","," mod"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["\u2191","Cl","un","l","n","j","."," Try","\u23ce"," ","tha","\u2191"," San","itary"," C","U","an","ara",".","\u23ce","Earl","\u2191"," Ull","om"," i","\u23ce"," B","1"," ","East"," Adams",".","\u23ce","\u2191","Ph","ona"," Main"," ","11",".","\u23ce"]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.65,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":20,"is_repeated_datapoint":false,"tokens":["902","."," ","1",",","0","$$","fr","."," ","\u23ce","tre"," av","1","\u00ee","?","el"," (","Joseph","-","Marie","-","\u2191","Ed","ou","ard","),"," second"," mat","P","en","si"," n",";"," ","28"," ","ans"," ","3"," "]},{"tokens_acts_list":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"train_token_ind":9,"is_repeated_datapoint":false,"tokens":["<EOT>","police"," station"," to"," give"," her"," statement",".\""," \"","Uncle"," Dragon",","," you"," are"," here","?\""," \"","Mann",","," you"," are"," too"," im","pert","inent",".\""," \"","The"," coffee"," is"]}]}],"top_logits":["\u2191","'s","W","Smith","J","G","R","O","Taylor","Williams"],"bottom_logits":["taiwanese","aste","massachus","obob","club","hem","ibib","the","forge"]}